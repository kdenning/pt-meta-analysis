---
title: 'Perspective Taking Meta-Analysis: Prelims Version'
author: "Kathryn Denning"
date: "1/9/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
library(rio)
library(here)
library(tidyverse)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

# importing data
prelims_data <- import("prelims_data.xlsx") 
```

# Convert effect sizes and CI around it

## Convert F to d

```{r}
f_to_d <- prelims_data %>% 
  select(effect_size_num, F_score, n_pt, n_comparison) %>% 
  filter(F_score != "NA")

f_to_d_output <- fes(f = F_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = f_to_d) #function drops 95CI when producing table for some reason

f_to_d_output %<>% 
  select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  select(-var.d,-id)

prelims_data_fconverted <- left_join(prelims_data, f_to_d_output, by = "effect_size_num")
```

## Convert t to d

```{r}

t_to_d <- prelims_data_fconverted %>% 
  select(effect_size_num, t_score, n_pt, n_comparison) %>% 
  filter(t_score!= "NA")

t_to_d_output <- tes(t = t_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = t_to_d)

t_to_d_output %<>% 
  select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  select(-var.d,-id)


prelims_data_ftconverted <- left_join(prelims_data_fconverted, t_to_d_output, by = "effect_size_num")
```

## Convert means/sds to d

```{r}
# Getting dataset of values that do not have 
mean_sd_data <- prelims_data_ftconverted %>% 
  select(effect_size_num, mean_pt, mean_comparison, sd_pt, 
         sd_comparison, n_pt, n_comparison, cohens_d, d.x, d.y) %>% 
  filter(!is.na(mean_pt) & !is.na(mean_comparison) & !is.na(sd_pt) 
         & !is.na(sd_comparison) & is.na(cohens_d))

msd_to_d <- escalc(measure = "SMD", m1i = mean_pt, m2i = mean_comparison, sd1i = sd_pt, sd2i = sd_comparison, n1i = n_pt, n2i = n_comparison, data = mean_sd_data, vtype = "UB") #since we will have to unbias other cohen's d, told escalc to calculate an unbiased SMD

#yi in the output is SMD, vi is sampling variance

msd_to_d %<>% 
  select(effect_size_num, yi, vi)

converted_data <- left_join(prelims_data_ftconverted, msd_to_d, by = "effect_size_num")

```

## Convert regression coefficients to d

We need SD of the DV - which most studies did not provide - in order to convert this. We will need to add these studies (k = 21) to our "Contact Authors" sheet if they are not already on there for some reason.

# Other data prep

```{r}

meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  select(-c(name, value)) %>% 
  pivot_longer(c(cohens_d_var, var_d.x, var_d.y, vi)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within)) %>% 
  na.omit()
```

# Descriptive information about the studies that made it into the meta-analysis

```{r}
# Number of papers

meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  select(authors) %>% 
  unique() %>% 
  count()

# Number of studies

meta_data %>% 
  select(study_num_total) %>% 
  unique() %>% 
  count()

# Number of unique samples due to some studies having multiple targets

meta_data %>% 
  select(sample_number_total) %>% 
  unique() %>% 
  count()

# Unique effect sizes total

meta_data %>% 
  select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per outcome category

meta_data %>% 
  select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Do we need to collapse perspective taking conditions?

### Stereotyping outcome

```{r}
# Number of effect sizes per comparison we coded 

k_per_comparison <- meta_data %>% 
  select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.
* Before adding in moderators, we will add the PT unspecified comparisons to the analysis (collapsed across Imagine-self and Imagine-other) to see if they impact the results.

### Merging/Overlap outcome

```{r}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor. We will still be able to conduct a second analysis where we had unspecified analyses in to see if they impact the results.

### Interpersonal feelings outcome

```{r}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. We will also add in unspecified at the end to see if it impacts the results.

# Overall Meta-analysis

```{r}
#collapsing across conditions for data-prep

meta_data %>% 

```