---
title: 'Perspective Taking Meta-Analysis: Prelims Version'
author: "Kathryn Denning"
date: "1/11/2021"
output: 
  html_document:
    code_folding: "show"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---


```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

# importing data
prelims_data <- import("prelims_data.xlsx") 
```

# Convert effect sizes

## Convert F to d

```{r convert F to d}
f_to_d <- prelims_data %>% 
  filter(F_score != "NA")

f_to_d_output <- fes(f = F_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = f_to_d) #function drops 95CI when producing table for some reason

f_to_d_output %<>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  dplyr::select(-c(var.d,id)) %>% 
  dplyr::select(effect_size_num, d, var_d) 

prelims_data_fconverted <- left_join(prelims_data, f_to_d_output, by = "effect_size_num")
```

## Convert t to d

```{r convert t to d}

t_to_d <- prelims_data_fconverted %>% 
  dplyr::select(effect_size_num, t_score, n_pt, n_comparison) %>% 
  filter(t_score!= "NA")

t_to_d_output <- tes(t = t_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = t_to_d)

t_to_d_output %<>% 
  dplyr::select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  dplyr::select(-var.d,-id)


prelims_data_ftconverted <- left_join(prelims_data_fconverted, t_to_d_output, by = "effect_size_num")
```

## Convert means/sds to d

```{r convert means and sds to d}
# Getting dataset of values that do not have 
mean_sd_data <- prelims_data_ftconverted %>% 
  dplyr::select(effect_size_num, mean_pt, mean_comparison, sd_pt, 
         sd_comparison, n_pt, n_comparison, cohens_d, d.x, d.y) %>% 
  filter(!is.na(mean_pt) & !is.na(mean_comparison) & !is.na(sd_pt) 
         & !is.na(sd_comparison) & is.na(cohens_d))

msd_to_d <- escalc(measure = "SMD", m1i = mean_pt, m2i = mean_comparison, sd1i = sd_pt, sd2i = sd_comparison, n1i = n_pt, n2i = n_comparison, data = mean_sd_data, vtype = "UB") #since we will have to unbias other cohen's d, told escalc to calculate an unbiased SMD

#yi in the output is SMD, vi is sampling variance

msd_to_d %<>% 
  dplyr::select(effect_size_num, yi, vi)

converted_data <- left_join(prelims_data_ftconverted, msd_to_d, by = "effect_size_num")

```

## Convert regression coefficients to d

We need SD of the DV - which most studies did not provide - in order to convert this. We will need to add these studies (k = 21) to our "Contact Authors" sheet if they are not already on there for some reason.

```{r data cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Other data prep
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  dplyr::select(-c(name, value)) %>% 
  pivot_longer(c(cohens_d_var, var_d.x, var_d.y, vi)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  dplyr::select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  dplyr::select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within),
         target_information = as.factor(target_information))
```

# Descriptive information about the studies that made it into the meta-analysis

```{r descriptive info before collapsing conditions}
# Number of papers
meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()

# Number of studies
meta_data %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()

# Number of unique samples due to some studies having multiple targets
meta_data %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()

# Unique effect sizes total
meta_data %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per outcome category
meta_data %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Checking if we need to collapse perspective taking conditions

### Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

### Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

### Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

# Making sure Interpersonal feels is now the reference level
levels(meta_data$outcome_type)

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

```{r effect size overall mod}
# Effect sizes in overall model
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per comparison and stereotyping category
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Overall meta-analytic effect before we look into theoretical moderators

```{r overall meta no mods}
meta_overall <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_overall
```

Across perspective taking condition and outcome, there is a small but statistically significant effect.

## Multivariate model with perspective taking only

```{r overall meta pt instruction mod}
#setting up dummy codes; stereotyping and pt_v_control are the respective comparison groups because they are coded as 1
contrasts(meta_overall1$pt_comparison) <- contr.treatment(2)
contrasts(meta_overall1$pt_comparison) 

# OVerall model across outcome type
meta_multi1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi1
```

The estimate of intercept represents when pt_comparison = 0 (i.e., pt_v_control). The effect of pt_comparison:pt_v_objective represents the difference in effect sizes compared to the reference level. The moderator test is significant.

```{r overall pt v control no outcome}
meta_overall1_control <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control")

meta_multi1_control <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1_control)
meta_multi1_control

```

When submitted to only pt v control, effect is not significantly different from 0 and the test for heterogeneity is significant.

```{r overall pt v objective no outcome}
meta_overall1_objective <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective")

meta_multi1_objective <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1_objective)
meta_multi1_objective

```

This effect is statistically significant from 0, but heterogeneity is significant.

## Pre-registered multivariate model with perspective taking and outcome type

```{r overall meta interaction mod}
contrasts(meta_overall1$outcome_type) <- contr.treatment(3)
contrasts(meta_overall1$outcome_type)

# When we include outcome type
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi2
```

The intercept is when both moderators = 0 (When the pt is compared to the objective condition and interpersonal feels is the reference). We are particularly interested in the interaction and estimates. We used interpersonal feels as the reference because it is the the most studied PT subfield - and the one in which a different meta-analysis already exists - and we are curious if the effect of perspective taking is the same across outcome types or differs. We used pt vs objective because, when combined with interpersonal feels, we would expect the largest effect in that cell. Therefore, we are interested if perspective taking in other outcome types follows this same pattern or differs.

The "pt_comparison2" contrast supports the theory that I just described. In comparison to the intercept - which represents the effect of pt vs objective in the interpersonal context - the objective control condition shows a decrease of effect size of .33 (this is with interpersonal feels context held constant). With pt v objective held constant and just changing outcome, the overlap outcome did not differ significantly from interpersonal feels (-.26), however, the stereotyping outcome was significantly lower (-.64). 

When we look at the interaction we see there are two significant effects for the contrasts. These are not as intuitively interpretable as the main effects. When we vary up both perspective taking and outcome in the fifth estimate (estimate "pt_comparison2:outcome_type2") looking at pt_v_objective in the overlap condition versus pt_v_control in interpersonal feels condition, we see there is a positive and significant change represented by the contrast estimate between the conditions. We also see a significant change that has an even larger positive change in terms of the slope between the pt_v_control and stereotyping condition versus the intercept.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition). The effect sizes for the interaction relate to the contrast, but cannot be used to find the effect.

To get the effect sizes for each PT comparison in each outcome that would correspond to those pairwise comparisons, we need to run individual meta-analyses on the subsets of the data-set (as seen in McAuliffe meta-analysis):

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r overall model pt vs control stereo}
control_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")


meta_control_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_stereo_subset )
```

```{r overall model pt v control overlap}
control_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")


meta_control_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset)
```

```{r overall model pt v control interpersonal}
control_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")


meta_control_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset)
```

```{r overall model pt v objective stereo}
obj_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")


meta_obj_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset)
```

```{r overall model pt v objective overlap}
obj_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")


meta_obj_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset)
```

```{r overall model pt v objective interpersonal }
obj_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")


meta_obj_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset)
```

```{r overall model effect size table}
effect_table <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo[1],
                                 meta_obj_stereo[1],
                                 meta_control_overlap[1],
                                 meta_obj_overlap[1],
                                 meta_control_inter[1],
                                 meta_obj_inter[1]),
                      pvalue = c(meta_control_stereo[5],
                                 meta_obj_stereo[5],
                                 meta_control_overlap[5],
                                 meta_obj_overlap[5],
                                 meta_control_inter[5],
                                 meta_obj_inter[5]),
                      ci_upper = c(meta_control_stereo[6],
                                 meta_obj_stereo[6],
                                 meta_control_overlap[6],
                                 meta_obj_overlap[6],
                                 meta_control_inter[6],
                                 meta_obj_inter[6]),
                      ci_lower = c(meta_control_stereo[7],
                                 meta_obj_stereo[7],
                                 meta_control_overlap[7],
                                 meta_obj_overlap[7],
                                 meta_control_inter[7],
                                 meta_obj_inter[7]))
effect_table 
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .58). They found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control, but again, we have more results included, and we are still in the same direction (.16). 

Neither perspective taking comparison is significantly different from zero. The effect size for pt_v_control & stereotyping is almost identical to the intercept of the original model (which it should be), which is comforting statistically.

Only the comparison of pt_v_control is statistically different from 0 for the Overlap/merging category. This is surprising and opposite of the pattern found in the interpersonal feelings category. The comparison of pt_v_objective is marginal, though the estimate itself is not much smaller. This is the category in which the number of studies included is small, which could be impacting these results.

### Sample size moderator analysis

```{r sample moderator overall multi}
meta_multi3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type*n_overall,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi3
```

When we include sample size as a moderator, the interactions found in the previous model disappear, and the effect size for intercept gets larger. The control comparison still remains significantly lower than the objective condition when outcome is held constant. Stereotyping is still significantly lower than interpersonal feelings when perspective taking comparison type remains constant. 

### Target information amount moderator

```{r sample moderator overall multi}
contrasts(meta_overall1$target_information)

meta_multi4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type*target_information,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi4
```

Target information is a variable that measures the amount of descriptive detail that was provided about the target (1 = an impoverished target, often just a photo; 2 = some detail, like a vignette; 3 = highly detailed, like a video or meeting in person). The impoverished target is the reference group.

When included as a moderator, the intercept was still significant, but the intercept estimate had lessened in size. The comparison of stereotyping to the interpersonal feels with pt comparison held still became marginal, while all other effects became NS. Interestingly, a significant three-way interaction with the contrast of "pt_comparison2:outcome_type2:target_information3" became significant with a very large coefficient for the contrast (.88). This contrast is saying there is a significant difference between when people are in pt_v_control in the overlap outcome and the participant is provided a lot of detailed information about the target to when they are in the pt_v_objective condition in the interpersonal feels condition with an impoverished target.

It is interesting that none of the other interactions are significant. It may be an artifact of the interpersonal feels condition having such a large effect normally, that it is difficult for these other conditions to make up for that effect difference with more detailed targets.

I should add effects per comparison cell, but that is too much for the prelims.

Pairwise comparisons:

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type3:target_information2

```{r pairwise comparisons overall meta target info mod 1}
anova(meta_multi4, btt=15:16)
```

NS - The difference between these two comparisons is outcome category (merging vs stereotyping), with pt_v_control and medium target information help constant.

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type2:target_information3 

```{r pairwise comparisons overall meta target info mod 2}
anova(meta_multi4, btt=15:17)
```

Sig - The difference between these two is target information, they are both the merging outcome category, but one is medium target information and one is detailed.

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type3:target_information3

```{r pairwise comparisons overall meta target info mod 3}
anova(meta_multi4, btt=15:18)
```

Sig - There is a significant difference between merging/medium target information and stereotyping/detailed target information.

* pt_comparison2:outcome_type3:target_information2 - pt_comparison2:outcome_type2:target_information3 

```{r pairwise comparisons overall meta target info mod 4}
anova(meta_multi4, btt=16:17)
```

NS - These is not a difference when stereotyping has medium target information and it is compared to merging with detailed information and pt_v_control is help constant. This is weird, since the contrast with opposing groupings (stereotyping/high detail vs merging/medium detail).

* pt_comparison2:outcome_type3:target_information2 - pt_comparison2:outcome_type3:target_information3 

```{r pairwise comparisons overall meta target info mod 5}
anova(meta_multi4, btt=16:18)
```

NS

* pt_comparison2:outcome_type2:target_information3 - pt_comparison2:outcome_type3:target_information3 

```{r pairwise comparisons overall meta target info mod 5}
anova(meta_multi4, btt=17:18)
```

Marginal - When pt_v_control and detailed target information is held constant, there is a difference between merging and stereotyping outcome groups.

### Target category moderator

# Stereotyping model

For this stereotyping model - like the literature - higher scores/higher effects, indicate more stereotyping. Lower scores indicate less stereotyping (e.g., more stereotype reduction).

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))


meta_stereo1 %<>% 
  filter(outcome_type == "1")

mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))


## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" | 
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")
```

## Number of unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Overall stereotyping model - no mods

```{r stereo model no mods}
stereo_nomods <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_nomods
```

Small and non-significant effect of perspective taking versus any comparison in the stereotyping outcome.

## Model comparing imagine-self vs imagine-other instructions with a combined control and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo1$pt_comparison) <- contr.treatment(4)
contrasts(meta_stereo1$pt_comparison) 

# OVerall model across outcome type
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1
```

The test of moderators is not significant, nor are any comparisons. The estimates do not provide us with estimates for each comparison, we will need to run individual models to examine this.

Theoretically important pairwise comparisons:

* self_v_control to other_v_control

```{r pairwise comparisons stereo 2}
linearHypothesis(stereo_meta1, c(0,1,0,-1))
```

* other_v_objective to other_v_control

```{r pairwise comparisons stereo 2}
linearHypothesis(stereo_meta1, c(0,0,1,-1))
```

Neither are significantly different from one another.

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r stereo self vs objective model}
meta_stereo1_selfobj <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_objective")

stereo_meta1_selfobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfobj)
```

```{r stereo other vs objective model}
meta_stereo1_otherobj <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_objective")

stereo_meta1_otherobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_otherobj)
```

```{r stereo self vs control model}
meta_stereo1_selfcontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_control")

stereo_meta1_selfcontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfcontrol)
```

```{r stereo other vs control model}
meta_stereo1_othercontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_control")

stereo_meta1_othercontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_othercontrol)
```

```{r stereo effect size table self vs other model}
stereo_effect_table1 <- cbind(label = c("self_v_control",
                                        "self_v_objective",
                                        "other_v_control",
                                        "other_v_objective"),
                              estimate = c(stereo_meta1_selfcontrolest[1],
                                           stereo_meta1_othercontrolest[1],
                                           stereo_meta1_selfobjest[1], 
                                           stereo_meta1_otherobjest[1]),
                              pvalue = c(stereo_meta1_selfcontrolest[5],
                                           stereo_meta1_othercontrolest[5],
                                           stereo_meta1_selfobjest[5], 
                                           stereo_meta1_otherobjest[5]),
                              ci_higher = c(stereo_meta1_selfcontrolest[6],
                                           stereo_meta1_othercontrolest[6],
                                           stereo_meta1_selfobjest[6], 
                                           stereo_meta1_otherobjest[6]),
                              ci_lower = c(stereo_meta1_selfcontrolest[7],
                                           stereo_meta1_othercontrolest[7],
                                           stereo_meta1_selfobjest[7], 
                                           stereo_meta1_otherobjest[7]))
stereo_effect_table1
```

As expected based on the initial model, the estimates are all small and non-significant.

### Sample size moderator 

```{r sample moderator stereo 1}
stereo_meta1_n <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)
stereo_meta1_n
```

Not significant

### Target group moderator - in-group/out-group

```{r target outgroup moderator stereo 1}
meta_stereo1 %<>% 
  mutate(target_out_minor = as.factor(target_out_minor)) %>% 
  filter(target_out_minor == "1" | target_out_minor == "2")

meta_stereo1 %<>% 
  mutate(target_out_minor = fct_relevel(target_out_minor,
                                        "2",
                                        "1"))

contrasts(meta_stereo1$target_out_minor)
# 1 = out-group or minority group member; 2 = not; I reorganized the levels so that 2 is the reference group

stereo_meta1_out <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)
stereo_meta1_out
```

Though the test of moderators is significant, none of the estimates are significantly different from one another.

## Model comparing "day in the life," other control, and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo2$pt_comparison) <- contr.treatment(3)
contrasts(meta_stereo2$pt_comparison) 

# OVerall model across outcome type
stereo_meta2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2
```

Pairwise comparisons to compare:

* pt_v_objective to pt_v_other control

```{r pairwise comparisons stereo 2}
anova(stereo_meta2, btt=2:3)
```

They do not significantly differ.

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r stereo day model effect}
meta_stereo2_day <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_day_control")

stereo_meta2_day <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_day)
```

```{r stereo other control model effect}
meta_stereo2_control <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_other_control")

stereo_meta2_control <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_control)
```

```{r stereo objective model effect}
meta_stereo2_objective <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_objective")

stereo_meta2_objective <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_objective)
```

```{r stereo effect table model 2}
stereo_effect_table2 <- cbind(label = c("pt_v_day_control",
                                        "pt_v_other_control",
                                        "pt_v_objective"),
                              estimate = c(stereo_meta2_day[1],
                                           stereo_meta2_control[1],
                                           stereo_meta2_objective[1]),
                              pvalue = c(stereo_meta2_day[5],
                                           stereo_meta2_control[5],
                                           stereo_meta2_objective[5]),
                              ci_higher = c(stereo_meta2_day[6],
                                           stereo_meta2_control[6],
                                           stereo_meta2_objective[6]),
                              ci_higher = c(stereo_meta2_day[7],
                                           stereo_meta2_control[7],
                                           stereo_meta2_objective[7]))
stereo_effect_table2 
```

The estimates are, again, all small and non-significant. It is interesting that the effect size are nearly the same between the "day in the life" control and the "other control," arguing against our discussion that they could be different.

### Target group moderator - in-group/out-group

```{r target outgroup moderator stereo 2}
meta_stereo2 %<>% 
  mutate(target_out_minor = as.factor(target_out_minor)) %>% 
  filter(target_out_minor == "1" | target_out_minor == "2")

meta_stereo2 %<>% 
  mutate(target_out_minor = fct_relevel(target_out_minor,
                                        "1",
                                        "2"))

contrasts(meta_stereo2$target_out_minor)
# 1 = out-group or minority group member; 2 = not; Out-group targets are the reference group

stereo_meta_out <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta_out
```

There are significant main effects now for instruction effect that were not there previously. The interaction is of most theoretical importance. The reference groups are pt_v_day_control and the out-group target, which we can see is significant and medium to large effect size in stereotype reduction. However, both contrasts for the interaction are significantly different from the intercept: pt_v_other_control/in-group and pt_v_objective/in-group target. These contrasts demonstrate there are significantly stronger negative effects in studies comparing both the pt_v_other_control and the pt_v_objective conditions when the targets are not out-group/minority targets. This may seem to support the backfiring literature, or be an artifact that a lot of perspective taking studies are done with elderly targets, which we did not code as out-group or minority.

We also see when running pairwise comparisons of these latter two contrasts that they too are significant from one another.

```{r pairwise comparisons targ out stereo 2}
anova(stereo_meta_out, btt=5:6)
```

### Target group moderator - in distress/in need

To check that the distress variable did not moderate because it included the elderly target in it, which it did not.

```{r target outgroup moderator stereo 2}
meta_stereo2 %<>% 
  mutate(target_empathetic_need = as.factor(target_empathetic_need))

contrasts(meta_stereo2$target_empathetic_need)
# 1 = in distress/in need target ; 2 = not; need targets are the reference group

stereo_meta_need <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_empathetic_need,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta_need
```
