---
title: 'Perspective Taking Meta-Analysis: Prelims Version'
author: "Kathryn Denning"
date: "1/11/2021"
output: 
  html_document:
    code_folding: "show"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---


```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(dmetar)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

# importing data
prelims_data <- import("meta_data.csv") 
```

# Descriptive information about the studies that made it into the meta-analysis

```{r descriptive info before collapsing conditions}
# Number of papers
meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()

# Number of studies
meta_data %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()

# Number of unique samples due to some studies having multiple targets
meta_data %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()

# Unique effect sizes total
meta_data %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per outcome category
meta_data %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Checking if we need to collapse perspective taking conditions

### Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

### Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

### Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

# Making sure Interpersonal feels is now the reference level
levels(meta_overall1$outcome_type)

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

```{r effect size overall mod}
# Effect sizes in overall model
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per comparison and stereotyping category
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Overall meta-analytic effect before we look into theoretical moderators

```{r overall meta no mods}
meta_overall <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_overall


# Instructions to hand calculate I2 are here https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate
W <- diag(1/meta_overall1$var_dunb)
X <- model.matrix(meta_overall)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
I2 <- 100 * sum(meta_overall$sigma2)/ (sum(meta_overall$sigma2) + (meta_overall$k-meta_overall$p)/sum(diag(P)))
I2

round(100 * (meta_overall$sigma2)/ (sum(meta_overall$sigma2) + (meta_overall$k-meta_overall$p)/sum(diag(P))), digits = 2)
#Function to get I2
get_I2_overall <- function(.x, .y) { #.x is the dataset supplied to the rma.mv function, .y is the output of that function
  W <- diag(1/.x$var_dunb)
  X <- model.matrix(.y)
  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
  I2 <- 100 * sum(.y$sigma2)/ (sum(.y$sigma2) + (.y$k-.y$p)/sum(diag(P)))
  return(I2)
}

#Check that functions gets same output as above & it does!
get_I2_overall(meta_overall1, meta_overall)

# total variance attributed to between and within-level clusters separately
## From left to right in every model unless specified:
## Between sample heterogeneity
## Between condition heterogeneity
## Between outcome heterogeneity
## Hetereogeneity due to outcome type crossed between studies

get_I2_var_levels <- function(.x, .y) { #.x is the dataset supplied to the rma.mv function, .y is the output of that function
  W <- diag(1/.x$var_dunb)
  X <- model.matrix(.y)
  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
  I2 <- round(100 * (.y$sigma2)/ (sum(.y$sigma2) + (.y$k-.y$p)/sum(diag(P))), digits = 2)
  return(I2)
}

get_I2_var_levels(meta_overall1, meta_overall)
```

Across perspective taking condition and outcome, there is a small but statistically significant effect. However, with Q statistically significant and I2 demonstrating that 95% of the total variance is due to heterogeneity, this effect is is pretty much meaningless. We need to look at smaller subsets of data. When we break this down by level, we see this is predominantly due to between-study variance (49%) and between outcome variance within-study (25%).

## Multivariate model with perspective taking only

```{r overall meta pt instruction mod}
#setting up dummy codes; stereotyping and pt_v_control are the respective comparison groups because they are coded as 1
contrasts(meta_overall1$pt_comparison) <- contr.treatment(2)
contrasts(meta_overall1$pt_comparison) 

# OVerall model across outcome type
meta_multi1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison - 1,
                      random = list( ~ pt_comparison| sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ pt_comparison | outcome_scale_group), data = meta_overall1)
meta_multi1

get_I2(meta_overall1, meta_multi1)
get_I2_var_levels(meta_overall1, meta_multi1)
```

The estimate of intercept represents when pt_comparison = 0 (i.e., pt_v_control). The effect of pt_comparison:pt_v_objective represents the difference in effect sizes compared to the reference level. The moderator test is significant.

```{r overall pt v control no outcome}
meta_overall1_control <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control")

meta_multi1_control <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1_control)
meta_multi1_control

get_I2_overall(meta_overall1_control, meta_multi1_control)
```

When submitted to only pt v control, effect is not significantly different from 0 and the test for heterogeneity is significant.

```{r overall pt v objective no outcome}
meta_overall1_objective <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective")

meta_multi1_objective <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1_objective)
meta_multi1_objective

get_I2_overall(meta_overall1_objective, meta_multi1_objective)
```

This effect is statistically significant from 0, but heterogeneity is significant.

## Pre-registered multivariate model with perspective taking and outcome type

```{r overall meta interaction mod}
contrasts(meta_overall1$outcome_type) <- contr.treatment(3)
contrasts(meta_overall1$outcome_type)

# When we include outcome type
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi2
get_I2(meta_overall1, meta_multi2)
```

The intercept is when both moderators = 0 (When the pt is compared to the objective condition and interpersonal feels is the reference). We are particularly interested in the interaction and estimates. We used interpersonal feels as the reference because it is the the most studied PT subfield - and the one in which a different meta-analysis already exists - and we are curious if the effect of perspective taking is the same across outcome types or differs. We used pt vs objective because, when combined with interpersonal feels, we would expect the largest effect in that cell. Therefore, we are interested if perspective taking in other outcome types follows this same pattern or differs.

The "pt_comparison2" contrast supports the theory that I just described. In comparison to the intercept - which represents the effect of pt vs objective in the interpersonal context - the objective control condition shows a decrease of effect size (this is with interpersonal feels context held constant). With pt v objective held constant and just changing outcome, the overlap outcome did not differ significantly from interpersonal feels, however, the stereotyping outcome was significantly lower. 

When we look at the interaction we see there is only one significant effect for the contrast. Theis is not as intuitively interpretable as the main effects. When we vary up both perspective taking and outcome in the fifth estimate (estimate "pt_comparison2:outcome_type2") looking at pt_v_objective in the overlap condition versus pt_v_control in interpersonal feels condition, we see there there is not a significant difference from the intercept. We do see a significant change in which the slope of the contrast is demonstrates a positive increase between the pt_v_control and stereotyping condition versus the intercept.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition). The effect sizes for the interaction relate to the contrast, but cannot be used to find the effect.

To get the effect sizes for each PT comparison in each outcome that would correspond to those pairwise comparisons, we need to run individual meta-analyses on the subsets of the data-set (as seen in McAuliffe meta-analysis):

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r overall model pt vs control stereo}
control_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")


meta_control_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_stereo_subset )
```

```{r overall model pt v control overlap}
control_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")


meta_control_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset)
```

```{r overall model pt v control interpersonal}
control_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")


meta_control_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset)
```

```{r overall model pt v objective stereo}
obj_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")


meta_obj_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset)
```

```{r overall model pt v objective overlap}
obj_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")


meta_obj_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset)
```

```{r overall model pt v objective interpersonal }
obj_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")


meta_obj_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset)
```

```{r overall model effect size table}
effect_table <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo[1],
                                 meta_obj_stereo[1],
                                 meta_control_overlap[1],
                                 meta_obj_overlap[1],
                                 meta_control_inter[1],
                                 meta_obj_inter[1]),
                      pvalue = c(meta_control_stereo[5],
                                 meta_obj_stereo[5],
                                 meta_control_overlap[5],
                                 meta_obj_overlap[5],
                                 meta_control_inter[5],
                                 meta_obj_inter[5]),
                      ci_upper = c(meta_control_stereo[6],
                                 meta_obj_stereo[6],
                                 meta_control_overlap[6],
                                 meta_obj_overlap[6],
                                 meta_control_inter[6],
                                 meta_obj_inter[6]),
                      ci_lower = c(meta_control_stereo[7],
                                 meta_obj_stereo[7],
                                 meta_control_overlap[7],
                                 meta_obj_overlap[7],
                                 meta_control_inter[7],
                                 meta_obj_inter[7]))
effect_table 
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .58). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control, but again, we have more results included, and we are still in the same direction. 

*Note for Sara: This change from before for imagine-other vs no-instructions in the interpersonal feels category is due to me adding three effect sizes converted from correlation coefficients that I had previously forgot to convert... I guess they were outliers, since it increased from .16 to .27*


For the categories other than Interpersonal Feels, only the comparison of pt_v_control is statistically different from 0 for the Overlap/merging category. This is surprising and opposite of the pattern found in the interpersonal feelings category. The comparison of pt_v_objective is marginal, though the estimate itself is not much smaller. This is the category in which the number of studies included is small, which could be impacting these results. We will have to see if these differ from each other when analyzed in their own model.

### Sample size moderator analysis

```{r sample moderator overall multi}
meta_multi3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type*n_overall,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi3

get_I2(meta_overall1, meta_multi3)
```

When we include sample size as a moderator, the interaction found in the previous model disappear, and the effect size for intercept gets larger. The control comparison is no longer significantly lower than the objective condition when outcome is held constant. Stereotyping is still significantly lower than interpersonal feelings when perspective taking comparison type remains constant. 

### Target information amount moderator

```{r sample moderator overall multi}
contrasts(meta_overall1$target_information)

meta_multi4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type*target_information,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi4

get_I2(meta_overall1, meta_multi4)
```

Target information is a variable that measures the amount of descriptive detail that was provided about the target (1 = an impoverished target, often just a photo; 2 = some detail, like a vignette; 3 = highly detailed, like a video or meeting in person). The impoverished target is the reference group.

When included as a moderator, the effect of the intercept becomes marginal and all other results are NS.

The comparison of stereotyping to the interpersonal feels with pt comparison held still became marginal, while all other effects became NS. This is interesting that the amount of information removes all effects of perspective taking, but does not significantly predict the effect size itself. It may speak to the large amount of hetereogeneity between the studies. 

# Outcome type moderating each instruction set: Alt interpretation to pre-reg

## Confirmatory: Pt v control

```{r}
control_sub <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control")


control_outcome_mod <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_sub)
control_outcome_mod

get_I2_overall(control_sub, control_outcome_mod)
get_I2_var_levels(control_sub, control_outcome_mod)

```

## Confirmatory: Pt v objective

```{r}
objective_sub <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective")


objective_outcome_mod <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = objective_sub)
objective_outcome_mod

get_I2_overall(objective_sub, objective_outcome_mod)
get_I2_var_levels(objective_sub, objective_outcome_mod)

```

These get estimates because no interactions by adding slopes to intercept.

# Stereotyping model

For this stereotyping model - like the literature - higher scores/higher effects, indicate more stereotyping. Lower scores indicate less stereotyping (e.g., more stereotype reduction).

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))


meta_stereo1 %<>% 
  filter(outcome_type == "1" %>% %>% )

mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))


## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" |
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")
```

## Number of unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Overall stereotyping model - no mods

```{r stereo model no mods}
stereo_nomods <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_nomods

get_I2(meta_stereo1, stereo_nomods)
```

Small and non-significant effect of perspective taking versus any comparison in the stereotyping outcome.

## Model comparing imagine-self vs imagine-other instructions with a combined control and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo1$pt_comparison) <- contr.treatment(4)
contrasts(meta_stereo1$pt_comparison) 

# OVerall model across outcome type
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1

get_I2(meta_stereo1, stereo_meta1)
```

The test of moderators is not significant, nor are any comparisons. The estimates do not provide us with estimates for each comparison, we will need to run individual models to examine this.

Theoretically important pairwise comparisons:

* self_v_control to other_v_control

```{r pairwise comparisons stereo 2}
linearHypothesis(stereo_meta1, c(0,1,0,-1))
```

* other_v_objective to other_v_control

```{r pairwise comparisons stereo 2}
linearHypothesis(stereo_meta1, c(0,0,1,-1))
```

Neither are significantly different from one another.

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r stereo self vs objective model}
meta_stereo1_selfobj <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_objective")

stereo_meta1_selfobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfobj)
```

```{r stereo other vs objective model}
meta_stereo1_otherobj <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_objective")

stereo_meta1_otherobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_otherobj)
```

```{r stereo self vs control model}
meta_stereo1_selfcontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_control")

stereo_meta1_selfcontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfcontrol)
```

```{r stereo other vs control model}
meta_stereo1_othercontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_control")

stereo_meta1_othercontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_othercontrol)
```

```{r stereo effect size table self vs other model}
stereo_effect_table1 <- cbind(label = c("self_v_control",
                                        "self_v_objective",
                                        "other_v_control",
                                        "other_v_objective"),
                              estimate = c(stereo_meta1_selfcontrolest[1],
                                           stereo_meta1_othercontrolest[1],
                                           stereo_meta1_selfobjest[1], 
                                           stereo_meta1_otherobjest[1]),
                              pvalue = c(stereo_meta1_selfcontrolest[5],
                                           stereo_meta1_othercontrolest[5],
                                           stereo_meta1_selfobjest[5], 
                                           stereo_meta1_otherobjest[5]),
                              ci_higher = c(stereo_meta1_selfcontrolest[6],
                                           stereo_meta1_othercontrolest[6],
                                           stereo_meta1_selfobjest[6], 
                                           stereo_meta1_otherobjest[6]),
                              ci_lower = c(stereo_meta1_selfcontrolest[7],
                                           stereo_meta1_othercontrolest[7],
                                           stereo_meta1_selfobjest[7], 
                                           stereo_meta1_otherobjest[7]))
stereo_effect_table1
```

As expected based on the initial model, the estimates are all small and non-significant.

### Sample size moderator 

```{r sample moderator stereo 1}
stereo_meta1_n <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)
stereo_meta1_n

get_I2(meta_stereo1, stereo_meta1_n)
```

Not significant

### Target moderator - information amount

```{r target outgroup moderator stereo 1}
meta_stereo1 %<>% 
  mutate(target_out_minor = as.factor(target_out_minor)) %>% 
  filter(target_out_minor == "1" | target_out_minor == "2")

meta_stereo1 %<>% 
  mutate(target_out_minor = fct_relevel(target_out_minor,
                                        "2",
                                        "1"))

contrasts(meta_stereo1$target_out_minor)
# 1 = out-group or minority group member; 2 = not; I reorganized the levels so that 2 is the reference group

stereo_meta1_targinfo <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)
stereo_meta1_targinfo

get_I2(meta_stereo1, stereo_meta1_targinfo)
```


### Target out-group moderator

```{r target outgroup moderator stereo 1}
meta_stereo1 %<>% 
  mutate(target_out_minor = as.factor(target_out_minor)) %>% 
  filter(target_out_minor == "1" | target_out_minor == "2")

meta_stereo1 %<>% 
  mutate(target_out_minor = fct_relevel(target_out_minor,
                                        "2",
                                        "1"))

contrasts(meta_stereo1$target_out_minor)
# 1 = out-group or minority group member; 2 = not; I reorganized the levels so that 2 is the reference group

stereo_meta1_out <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)
stereo_meta1_out

get_I2_var_levels(meta_stereo1, stereo_meta1_out)
```

Though the test of moderators is significant, none of the estimates are significantly different from the intercept. Also, some of the contrasts were redundant and were dropped from the model. 

## Model comparing "day in the life," other control, and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo2$pt_comparison) <- contr.treatment(3)
contrasts(meta_stereo2$pt_comparison) 

# OVerall model across outcome type
stereo_meta2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2

get_I2_var_levels(meta_stereo2, stereo_meta2)
```

Pairwise comparisons to compare:

* pt_v_objective to pt_v_other control

```{r pairwise comparisons stereo 2}
anova(stereo_meta2, btt=2:3)
```

They do not significantly differ.

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r stereo day model effect}
meta_stereo2_day <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_day_control")

stereo_meta2_day <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_day)
```

```{r stereo other control model effect}
meta_stereo2_control <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_other_control")

stereo_meta2_control <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_control)
```

```{r stereo objective model effect}
meta_stereo2_objective <-meta_stereo2 %>% 
  filter(pt_comparison == "pt_v_objective")

stereo_meta2_objective <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2_objective)
```

```{r stereo effect table model 2}
stereo_effect_table2 <- cbind(label = c("pt_v_day_control",
                                        "pt_v_other_control",
                                        "pt_v_objective"),
                              estimate = c(stereo_meta2_day[1],
                                           stereo_meta2_control[1],
                                           stereo_meta2_objective[1]),
                              pvalue = c(stereo_meta2_day[5],
                                           stereo_meta2_control[5],
                                           stereo_meta2_objective[5]),
                              ci_higher = c(stereo_meta2_day[6],
                                           stereo_meta2_control[6],
                                           stereo_meta2_objective[6]),
                              ci_higher = c(stereo_meta2_day[7],
                                           stereo_meta2_control[7],
                                           stereo_meta2_objective[7]))
stereo_effect_table2 
```

The estimates are, again, all small and non-significant. It is interesting that the effect size are nearly the same between the "day in the life" control and the "other control," arguing against our discussion that they could be different.

### Target group moderator

```{r target outgroup moderator stereo 2}
meta_stereo2 %<>% 
  mutate(target_out_minor = as.factor(target_out_minor)) %>% 
  filter(target_out_minor == "1" | target_out_minor == "2")

meta_stereo2 %<>% 
  mutate(target_out_minor = fct_relevel(target_out_minor,
                                        "1",
                                        "2"))

contrasts(meta_stereo2$target_out_minor)
# 1 = out-group or minority group member; 2 = not; Out-group targets are the reference group

stereo_meta_out <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta_out

get_I2(meta_stereo2, stereo_meta_out)
```

There are significant main effects now for instruction effect that were not there previously. The interaction is of most theoretical importance. The reference groups are pt_v_day_control and the out-group target, which we can see is significant and medium to large effect size in stereotype reduction. However, both contrasts for the interaction are significantly different from the intercept: pt_v_other_control/in-group and pt_v_objective/in-group target. These contrasts demonstrate there are significantly stronger negative effects (less bias/prejudice) in studies comparing both the pt_v_other_control and the pt_v_objective conditions when the targets are not out-group/minority targets. This may seem to support the backfiring literature, or be an artifact that a lot of perspective taking studies are done with elderly targets, which we did not code as out-group or minority.

We also see when running pairwise comparisons of these latter two contrasts that they too are significant from one another.

```{r pairwise comparisons targ out stereo 2}
anova(stereo_meta_out, btt=5:6)
```

### Target group moderator - in distress/in need

To check that the distress variable did not moderate because it included the elderly target in it, which it did not.

```{r target outgroup moderator stereo 2}
meta_stereo2 %<>% 
  mutate(target_empathetic_need = as.factor(target_empathetic_need))

contrasts(meta_stereo2$target_empathetic_need)
# 1 = in distress/in need target ; 2 = not; need targets are the reference group

stereo_meta_need <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_empathetic_need,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta_need

get_I2(meta_stereo2, stereo_meta_need)
```

# But hetereogeneity is still significant and high in all stereotyping and moderator outcomes...

Can it be eliminated by including scale as a predictor, not just part of the variance structure?

```{r stereo model imagine self vs other with scale as mod}
meta_stereo1$outcome_scale_group <- droplevels(meta_stereo1$outcome_scale_group)

contrasts(meta_stereo1$pt_comparison) 

stereo_meta1_scale <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1_scale

get_I2(meta_stereo2, stereo_meta1_scale)
```

Stereotyping is significantly moderated by scale. It seems we should look at the effect of PT instruction within the subset of data for each scale to determine if heterogeneity is no longer significant. 

## Lets look at the impact of perspective taking subsets of the dataset separated by scale type

This removes the crossed level of variance in our model

### Imagine-self vs Other compared to Objective and Control

#### In Feeling Therometer outcome only


```{r}
meta_stereo1 %>% 
  select(effect_size_num, outcome_scale_group) %>% 
  group_by(outcome_scale_group) %>% 
  count() #Lets start with 6, which is predominantly the feeling thermometer

feel_therom <- meta_stereo1 %>% 
  filter(outcome_scale_group == "6")

contrasts(feel_therom$pt_comparison) <- contr.treatment(4)
contrasts(feel_therom$pt_comparison) 

# No longer crossed because limited scale
therom1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = feel_therom)

therom1

get_I2(feel_therom, therom1)
```

#### In scales measuring explicit prejudice/discrimination

```{r}
explicit_scales <- meta_stereo1 %>% 
  filter(outcome_scale_group == "4")

contrasts(explicit_scales$pt_comparison) <- contr.treatment(4)
contrasts(explicit_scales$pt_comparison) 

# No longer crossed because limited scale
explicit_scales1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = explicit_scales)

explicit_scales1

get_I2(explicit_scales, explicit_scales1)
```

#### In measurements of target stereotypicality

```{r}
target_stereo <- meta_stereo1 %>% 
  filter(outcome_scale_group == "3")

contrasts(target_stereo$pt_comparison) <- contr.treatment(4)
contrasts(target_stereo$pt_comparison) 

# No longer crossed because limited scale
target_stereo1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = target_stereo)

target_stereo1 

get_I2(target_stereo, target_stereo1)
```

#### In measurements of implicit stereotyping

```{r}
implicit <- meta_stereo1 %>% 
  filter(outcome_scale_group == "1" | outcome_scale_group == "2")

contrasts(implicit$pt_comparison) <- contr.treatment(4)
contrasts(implicit$pt_comparison) 

# Since using two categories, added fourth layer back in
target_stereo_implicit <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = implicit)

target_stereo_implicit

get_I2(implicit, target_stereo_implicit)
```

### Day in the life coding

#### In Feeling Therometer outcome only


```{r}
feel_therom2 <- meta_stereo2 %>% 
  filter(outcome_scale_group == "6")

contrasts(feel_therom2$pt_comparison) <- contr.treatment(3)
contrasts(feel_therom2$pt_comparison) 

# No longer crossed because limited scale
therom2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = feel_therom2)

therom2

get_I2(feel_therom2, therom2)
```

#### In scales measuring explicit prejudice/discrimination

```{r}
explicit_scales2 <- meta_stereo2 %>% 
  filter(outcome_scale_group == "4")

contrasts(explicit_scales2$pt_comparison) <- contr.treatment(3)
contrasts(explicit_scales2$pt_comparison) 

# No longer crossed because limited scale
explicit_scales2_results <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = explicit_scales2)

explicit_scales2_results

get_I2(explicit_scales2, explicit_scales2_results)
```

#### In measurements of target stereotypicality

```{r}
target_stereo2 <- meta_stereo2 %>% 
  filter(outcome_scale_group == "3")

contrasts(target_stereo2$pt_comparison) <- contr.treatment(3)
contrasts(target_stereo2$pt_comparison) 

# No longer crossed because limited scale
target_stereo2_results <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var), data = target_stereo2)

target_stereo2_results 

get_I2(target_stereo2, target_stereo2_results)
```

#### In measurements of implicit stereotyping

```{r}
implicit2 <- meta_stereo2 %>% 
  filter(outcome_scale_group == "1" | outcome_scale_group == "2")

contrasts(implicit2$pt_comparison) <- contr.treatment(3)
contrasts(implicit2$pt_comparison) 

# Since using two categories, added fourth layer back in
target_stereo_implicit2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = implicit2)

target_stereo_implicit2

get_I2(implicit2, target_stereo_implicit2)
```

