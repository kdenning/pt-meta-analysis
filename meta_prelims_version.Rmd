---
title: 'Perspective Taking Meta-Analysis: Prelims Version'
author: "Kathryn Denning"
date: "1/9/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
library(rio)
library(here)
library(tidyverse)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

# importing data
prelims_data <- import("prelims_data.xlsx") 
```

# Convert effect sizes

## Convert F to d

```{r convert F to d}
f_to_d <- prelims_data %>% 
  select(effect_size_num, F_score, n_pt, n_comparison) %>% 
  filter(F_score != "NA")

f_to_d_output <- fes(f = F_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = f_to_d) #function drops 95CI when producing table for some reason

f_to_d_output %<>% 
  select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  select(-var.d,-id)

prelims_data_fconverted <- left_join(prelims_data, f_to_d_output, by = "effect_size_num")
```

## Convert t to d

```{r convert t to d}

t_to_d <- prelims_data_fconverted %>% 
  select(effect_size_num, t_score, n_pt, n_comparison) %>% 
  filter(t_score!= "NA")

t_to_d_output <- tes(t = t_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = t_to_d)

t_to_d_output %<>% 
  select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  select(-var.d,-id)


prelims_data_ftconverted <- left_join(prelims_data_fconverted, t_to_d_output, by = "effect_size_num")
```

## Convert means/sds to d

```{r convert means and sds to d}
# Getting dataset of values that do not have 
mean_sd_data <- prelims_data_ftconverted %>% 
  select(effect_size_num, mean_pt, mean_comparison, sd_pt, 
         sd_comparison, n_pt, n_comparison, cohens_d, d.x, d.y) %>% 
  filter(!is.na(mean_pt) & !is.na(mean_comparison) & !is.na(sd_pt) 
         & !is.na(sd_comparison) & is.na(cohens_d))

msd_to_d <- escalc(measure = "SMD", m1i = mean_pt, m2i = mean_comparison, sd1i = sd_pt, sd2i = sd_comparison, n1i = n_pt, n2i = n_comparison, data = mean_sd_data, vtype = "UB") #since we will have to unbias other cohen's d, told escalc to calculate an unbiased SMD

#yi in the output is SMD, vi is sampling variance

msd_to_d %<>% 
  select(effect_size_num, yi, vi)

converted_data <- left_join(prelims_data_ftconverted, msd_to_d, by = "effect_size_num")

```

## Convert regression coefficients to d

We need SD of the DV - which most studies did not provide - in order to convert this. We will need to add these studies (k = 21) to our "Contact Authors" sheet if they are not already on there for some reason.

# Other data prep

```{r data cleaning}
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  select(-c(name, value)) %>% 
  pivot_longer(c(cohens_d_var, var_d.x, var_d.y, vi)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within)) %>% 
  na.omit()
```

# Descriptive information about the studies that made it into the meta-analysis

```{r descriptive info before collapsing conditions}
# Number of papers
meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  select(authors) %>% 
  unique() %>% 
  count()

# Number of studies
meta_data %>% 
  select(study_num_total) %>% 
  unique() %>% 
  count()

# Number of unique samples due to some studies having multiple targets
meta_data %>% 
  select(sample_number_total) %>% 
  unique() %>% 
  count()

# Unique effect sizes total
meta_data %>% 
  select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per outcome category
meta_data %>% 
  select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Checking if we need to collapse perspective taking conditions

### Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.
* Before adding in moderators, we will add the PT unspecified comparisons to the analysis (collapsed across Imagine-self and Imagine-other) to see if they impact the results.

### Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor. We will still be able to conduct a second analysis where we had unspecified analyses in to see if they impact the results.

### Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. We will also add in unspecified at the end to see if it impacts the results.

# Overall Meta-analysis

```{r overall meta data prep}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(meta_overall1$pt_comparison)),
         dunb = ifelse(outcome_type == "1",
                       dunb*-1,
                       ifelse(outcome_type != "1",
                              dunb*1, NA)),
         outcome_type = recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) 

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.


# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL


# Effect sizes in overall model
meta_overall1 %>% 
  select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per comparison and stereotyping category
meta_overall1 %>% 
  select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Overall meta-analytic effect before we look into theoretical moderators

```{r overall meta no mods}
meta_overall <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_overall
```

Across perspective taking condition and outcome, there is a small but statiscially significant effect.

## Multivariate model with perspective taking only

```{r overall meta pt instruction mod}
#setting up dummy codes; stereotyping and pt_v_control are the respective comparison groups because they are coded as 1
contrasts(meta_overall1$pt_comparison) <- contr.treatment(2)
contrasts(meta_overall1$pt_comparison) 

# OVerall model across outcome type
meta_multi1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi1
```

The estimate of intercept represents when pt_comparison = 0 (i.e., pt_v_control). The effect of pt_comparison:pt_v_objective represents the difference in effect sizes compared to the refence level. The summary effect size would be found by adding them together. The moderator test is significant.

## Pre-registered multivariate model with perspective taking and outcome type

```{r overall meta interaction mod}
contrasts(meta_overall1$outcome_type) <- contr.treatment(3)
contrasts(meta_overall1$outcome_type) 

# When we include outcome type
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall1)
meta_multi2
```

The intercept is when both moderators = 0 (When the pt is compared to the control, and stereotyping is the reference). We are particularly interested in the interaction and estimates. The fifth estimate (pt_comparison2:outcome_type2) represents the change comparison in perspective taking conditions between stereotyping and merging. The sixth comparison represents (pt_comparison2:outcome_type3) comparison in perspective taking conditions between stereotyping and interpersonal feelings.

All possible pairwise comparisons:

```{r pairwise comparisons}
summary(glht(meta_multi2, 
             linfct=contrMat(c("pt_v_control:Stereotyping" = 1,
                               "pt_v_control:Overlap"=1,
                               "pt_v_control:Interpersonal Feels"=1,
                               "pt_v_objective:Stereotyping"=1,
                               "pt_v_objective:Overlap"=1,
                               "pt_v_objective:Interpersonal Feels" = 1), 
                                             type="Tukey")), 
        test=adjusted("none"))

```

We want to answer:

Is there an average effect size per comparison, per outcome? This can't be done by dropping the intercept like Wolfgang (creator suggested about non MLM structures) because then the MLM structure does not work. McAuliffe ran individual meta-analyses for each comparison... 

The multivariate overall model tells us there are statistical differences between context and condition (but does not provide pairwise comparison of merging vs interpersonal feelings), but does not tell us if (by themselves) they differ from 0. The effect sizes it is providing are too confounded by other factors.

To get the effect sizes for each PT comparison in each outcome that would correspond to those pairwise comparisons, we need to run individual meta-analyses on the subsets of the dataset (as seen in McAuliffe meta-analysis):

* PT vs Control in the Stereotyping outcome context

```{r overall model pt vs control stereo}
control_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")


meta_control_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_stereo_subset )
meta_control_stereo
```

* PT vs control in the overlap outcome context

```{r overall model pt v control overlap}
control_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")


meta_control_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset)
meta_control_overlap
```

* PT vs control in the interpersonal feelings outcome context

```{r overall model pt v control interpersonal}
control_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")


meta_control_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset)
meta_control_inter
```

* PT vs objective in the stereotyping outcome context

```{r overall model pt v objective stereo}
obj_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")


meta_obj_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset)
meta_obj_stereo
```

* PT vs objective in the overlap outcome context

```{r overall model pt v objective overlap}
obj_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")


meta_obj_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset)
meta_obj_overlap
```

* PT vs Objective in the interpersonal feelings outcome context

```{r overall model pt v objective interpersonal }
obj_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")


meta_obj_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset)
meta_obj_inter
```

Effect size table:

```{r overall model effect size table}
effect_table <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo[1],
                                 meta_obj_stereo[1],
                                 meta_control_overlap[1],
                                 meta_obj_overlap[1],
                                 meta_control_inter[1],
                                 meta_obj_inter[1]),
                      pvalue = c(meta_control_stereo[5],
                                 meta_obj_stereo[5],
                                 meta_control_overlap[5],
                                 meta_obj_overlap[5],
                                 meta_control_inter[5],
                                 meta_obj_inter[5]),
                      ci_upper = c(meta_control_stereo[6],
                                 meta_obj_stereo[6],
                                 meta_control_overlap[6],
                                 meta_obj_overlap[6],
                                 meta_control_inter[6],
                                 meta_obj_inter[6]),
                      ci_lower = c(meta_control_stereo[7],
                                 meta_obj_stereo[7],
                                 meta_control_overlap[7],
                                 meta_obj_overlap[7],
                                 meta_control_inter[7],
                                 meta_obj_inter[7]))
effect_table 
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .58). They found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control, but again, we have more results included, and we are still in the same direction (.16). 

Neither perspective taking comparison is significantly different from zero. The effect size for pt_v_control & stereotyping is almost identical to the intercept of the original model (which it should be), which is comforting statistically.

Only the comparison of pt_v_control is statistically different from 0 for the Overlap/merging category. This is surprising and opposite of the pattern found in the interpersonal feelings category. The comparison of pt_v_objective is marginal, though the estimate itself is not much smaller. This is the category in which the number of studies included is small, which could be impacting these results.

## Adding in PT unspecified

```{r meta overall ptus data prep}
meta_overall_ptus <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6", "10", "11"),
                                      pt_v_objective = c("3", "7", "12"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "1",
                       dunb*-1,
                       ifelse(outcome_type != "1",
                              dunb*1, NA)),
         outcome_type = recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) 

```

### Overall meta-analytic effect before we look into theoretical moderators with PTUS

```{r meta overall ptus no mods}
meta_overall_ptus_model <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)
meta_overall_ptus_model
```

Slightly larger, but not much different than without adding PT unspecified.

### Multivariate model with perspective taking only with PTUS

```{r meta overall ptus pt mod}
#setting up dummy codes; stereotyping and pt_v_control are the respective comparison groups because they are coded as 1
contrasts(meta_overall_ptus$pt_comparison) <- contr.treatment(2)
contrasts(meta_overall_ptus$pt_comparison) 

# OVerall model across outcome type
meta_multi1_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)

meta_multi1_ptus
```

Pattern did not change - slight increase in intercept estimate (representing pt_v_control) and slight decrease in pt_v_objective estimate, but significance did not change.

## Pre-registered multivariate model with perspective taking and outcome type with PTUS

```{r meta overall interaction ptus}
contrasts(meta_overall_ptus$outcome_type) <- contr.treatment(3)
contrasts(meta_overall_ptus$outcome_type) 

# When we include outcome type
meta_multi2_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)
meta_multi2_ptus
```

On the interaction (the part that really matters to us), the pattern of results did not change.

All possible pairwise comparisons with PTUS included:

```{r pairwise comparisons ptus}
summary(glht(meta_multi2_ptus, linfct=contrMat(c("pt_v_control:Stereotyping"=1,
                                            "pt_v_control:Overlap"=1,
                                            "pt_v_control:Interpersonal Feels"=1,
                                       "pt_v_objective:Stereotyping"=1,
                                       "pt_v_objective:Overlap"=1,
                                       "pt_v_objective:Interpersonal Feels"=1), 
                                       type="Tukey")), 
        test=adjusted("none"))

```

When adding in PTUS, a few more pairwise comparisons become significant or trending.The comparison that is newly significant that was previously NS: pt_v_objective:Interpersonal Feels - pt_v_objective:Stereotyping == 0. The comparison that was previously NS that is now trending: pt_v_objective:Overlap - pt_v_control:Overlap == 0 (but .099 trending...).     

* PT vs Control in the Stereotyping outcome context with PTUS added

```{r overall meta pt v control stereo ptus}
meta_overall_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")


meta_control_stereo_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_subset_ptus)
meta_control_stereo_ptus
```

* PT vs control in the overlap outcome context

```{r overall meta pt v control overlap ptus}
control_overlap_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")


meta_control_overlap_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset_ptus)
meta_control_overlap_ptus
```

* PT vs control in the interpersonal feelings outcome context

```{r overall meta pt v control interpersonal ptus}
control_inter_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")


meta_control_inter_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset_ptus)
meta_control_inter_ptus
```

* PT vs objective in the stereotyping outcome context

```{r overall meta pt vs objective stereo ptus}
obj_stereo_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")


meta_obj_stereo_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset_ptus)
meta_obj_stereo_ptus
```

* PT vs objective in the overlap outcome context

```{r overall meta pt vs objective overlap ptus}
obj_overlap_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")


meta_obj_overlap_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset_ptus)
meta_obj_overlap_ptus
```

* PT vs objective in the interpersonal feelings outcome context

```{r overall meta pt vs objective interpersonal ptus}
obj_inter_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")


meta_obj_inter_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset_ptus)
meta_obj_inter_ptus
```

Effect size table with PTUS:

```{r overall meta effect table ptus}
effect_table_ptus <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo_ptus[1],
                                 meta_obj_stereo_ptus[1],
                                 meta_control_overlap_ptus[1],
                                 meta_obj_overlap_ptus[1],
                                 meta_control_inter_ptus[1],
                                 meta_obj_inter_ptus[1]),
                      pvalue = c(meta_control_stereo_ptus[5],
                                 meta_obj_stereo_ptus[5],
                                 meta_control_overlap_ptus[5],
                                 meta_obj_overlap_ptus[5],
                                 meta_control_inter_ptus[5],
                                 meta_obj_inter_ptus[5]),
                      ci_upper = c(meta_control_stereo_ptus[6],
                                 meta_obj_stereo_ptus[6],
                                 meta_control_overlap_ptus[6],
                                 meta_obj_overlap_ptus[6],
                                 meta_control_inter_ptus[6],
                                 meta_obj_inter_ptus[6]),
                      ci_lower = c(meta_control_stereo_ptus[7],
                                 meta_obj_stereo_ptus[7],
                                 meta_control_overlap_ptus[7],
                                 meta_obj_overlap_ptus[7],
                                 meta_control_inter_ptus[7],
                                 meta_obj_inter_ptus[7]))
effect_table_ptus 
```

In comparing the two tables, the only significant difference in effect sizes is that the estimate in "PT vs control" condition in the stereotyping outcome increases from .039 to .101. However, it is still not significantly different from 0. The other estimates are all approximately the same.

# Stereotyping model

For this stereotyping model - like the literature - higher scores/higher effects, indicate more stereotyping. Lower scores indicate less stereotyping (e.g., more stereotype reduction).

```{r stereo model data prep}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo1 %<>% 
  filter(outcome_type == "1")

## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" | 
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")
  
```

## Number of unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Overall stereotyping model - no mods

```{r stereo model no mods}
stereo_nomods <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_nomods
```

Small and non-significant effect of perspective taking versus any comparison in the stereotyping outcome.

## Model comparing imagine-self vs imagine-other instructions with a combined control and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo1$pt_comparison) <- contr.treatment(4)
contrasts(meta_stereo1$pt_comparison) 

# OVerall model across outcome type
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1
```

The test of moderators is not significant, nor are any comparisons. The estimates do not provide us with estimates for each comparison, we will need to run individual models to examine this.

```{r stereo self vs objective model}
meta_stereo1_selfobj <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_objective")

stereo_meta1_selfobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfobj)

stereo_meta1_selfobjest
```

```{r stereo other vs objective model}
meta_stereo1_otherobj <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_objective")

stereo_meta1_otherobjest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_otherobj)

stereo_meta1_otherobjest
```

```{r stereo self vs control model}
meta_stereo1_selfcontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "self_v_control")

stereo_meta1_selfcontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_selfcontrol)

stereo_meta1_selfcontrolest
```

```{r stereo other vs control model}
meta_stereo1_othercontrol <-meta_stereo1 %>% 
  filter(pt_comparison == "other_v_control")

stereo_meta1_othercontrolest <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_othercontrol)

stereo_meta1_othercontrolest
```

```{r stereo effect size table self vs other model}
stereo_effect_table1 <- cbind(label = c("self_v_control",
                                        "self_v_objective",
                                        "other_v_control",
                                        "other_v_objective"),
                              estimate = c(stereo_meta1_selfcontrolest[1],
                                           stereo_meta1_othercontrolest[1],
                                           stereo_meta1_selfobjest[1], 
                                           stereo_meta1_otherobjest[1]),
                              pvalue = c(stereo_meta1_selfcontrolest[5],
                                           stereo_meta1_othercontrolest[5],
                                           stereo_meta1_selfobjest[5], 
                                           stereo_meta1_otherobjest[5]),
                              ci_higher = c(stereo_meta1_selfcontrolest[6],
                                           stereo_meta1_othercontrolest[6],
                                           stereo_meta1_selfobjest[6], 
                                           stereo_meta1_otherobjest[6]),
                              ci_lower = c(stereo_meta1_selfcontrolest[7],
                                           stereo_meta1_othercontrolest[7],
                                           stereo_meta1_selfobjest[7], 
                                           stereo_meta1_otherobjest[7]))
stereo_effect_table1
```

As expected based on the intial model, the estimates are all small and non-significant.