---
title: "Extra code"
author: "Kathryn Denning"
date: "1/12/2021"
output: html_document
---
## Output from individual models to obtain estimates per comparison:

```{r meta models per multi subsets with I2}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
pt_filter1 <- c("pt_v_control", "pt_v_control", "pt_v_control", 
                  "pt_v_objective", "pt_v_objective", "pt_v_objective") 

outcome_filter1 <- c("Stereotyping", "Overlap", "Interpersonal Feels",
                     "Stereotyping", "Overlap", "Interpersonal Feels")

## Get an empty list the length of the number of subsetted datasets we need
df_4_overall <- list(length = length(pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(pt_filter1)){
  df_4_overall[[i]] <- meta_overall1 %>%
    filter(pt_comparison == pt_filter1[i] & 
             outcome_type == outcome_filter1[i])
  }

# Mapping over each subsetted dataset to run the meta-analysis
meta_output <- map(df_4_overall, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_multi <- get_I2_overall_clean(df_4_overall, meta_output)

# I2 value per level of nested and crossed variance
I2_var_levels <-map2(df_4_overall, meta_output, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_multi <- get_btw_study_I2(I2_var_levels)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_multi <- get_within_study_I2(I2_var_levels)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_multi <- get_crossed_study_I2(I2_var_levels) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k <- get_k_clean(meta_output)
estimates <- get_ests_clean(meta_output)
pval <- get_pval_clean(meta_output)
ci_upper <- get_ci_upper_clean(meta_output)
ci_lower <- get_ci_lower_clean(meta_output)
Q_val <- get_Q_clean(meta_output)
Q_pval <- get_Q_p_clean(meta_output)

# Creating the labels for the dataframe
comparison <- c("Pt vs Control in Stereotyping",
                "PT vs Control in Overlap",
                "PT vs Control in Interpersonal feelings",
                "PT vs Objective in Stereotyping",
                "PT vs Objective in Overlap",
                "PT vs Objective in Interpersonal feelings")

# Combinind everything into one dataframe
multi_combined <- cbind(comparison, 
                        k,
                        estimates,
                        pval,
                        ci_upper,
                        ci_lower,
                        Q_val,
                        Q_pval,
                        I2_overall_multi,
                        btw_I2_multi,
                        within_I2_multi,
                        crossed_I2_multi)

knitr::kable(multi_combined)
```


### Output from individual models to obtain estimates per comparison:

```{r meta models for stereo 1 subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
stereo_pt_filter1 <- c("self_v_objective", "self_v_control", 
                      "other_v_objective", "other_v_control") 


## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo1 <- list(length = length(stereo_pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter1)){ # change () in seq_along to filters
  df_4_stereo1[[i]] <- meta_stereo1 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter1[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output1 <- map(df_4_stereo1, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_stereo1 <- get_I2_overall_clean(df_4_stereo1, stereosub_output1)

# I2 value per level of nested and crossed variance
I2_var_levels_stereo1 <-map2(df_4_stereo1, stereosub_output1, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_stereo1 <- get_btw_study_I2(I2_var_levels_stereo1)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_stereo1 <- get_within_study_I2(I2_var_levels_stereo1)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_stereo1 <- get_crossed_study_I2(I2_var_levels_stereo1) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_stereo1 <- get_k_clean(stereosub_output1)
estimates_stereo1 <- get_ests_clean(stereosub_output1)
pval_stereo1 <- get_pval_clean(stereosub_output1)
ci_upper_stereo1 <- get_ci_upper_clean(stereosub_output1)
ci_lower_stereo1 <- get_ci_lower_clean(stereosub_output1)
Q_val_stereo1 <- get_Q_clean(stereosub_output1)
Q_pval_stereo1 <- get_Q_p_clean(stereosub_output1)

# Creating the labels for the dataframe
comparison_stereo1 <- c("self_v_objective", "self_v_control",
                "other_v_objective", "other_v_control") 

# Combinind everything into one dataframe
stereo1_combined <- cbind(comparison_stereo1,
                          k_stereo1,
                          estimates_stereo1,
                          pval_stereo1,
                          ci_upper_stereo1,
                          ci_lower_stereo1,
                          Q_val_stereo1,
                          Q_pval_stereo1,
                          I2_overall_stereo1,
                          btw_I2_stereo1,
                          within_I2_stereo1,
                          crossed_I2_stereo1)

knitr::kable(stereo1_combined)
```

### Output from individual models to obtain estimates per comparison:

```{r meta models for stereo 2 subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
stereo_pt_filter2 <- c("pt_v_day_control", "pt_v_other_control",
                       "pt_v_objective") 

## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo2 <- list(length = length(stereo_pt_filter2))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter2)){ # change () in seq_along to filters
  df_4_stereo2[[i]] <- meta_stereo2 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter2[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output2 <- map(df_4_stereo2, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_stereo2 <- get_I2_overall_clean(df_4_stereo2, stereosub_output2)

# I2 value per level of nested and crossed variance
I2_var_levels_stereo2 <-map2(df_4_stereo2, stereosub_output2, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_stereo2 <- get_btw_study_I2(I2_var_levels_stereo2)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_stereo2 <- get_within_study_I2(I2_var_levels_stereo2)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_stereo2 <- get_crossed_study_I2(I2_var_levels_stereo2) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_stereo2 <- get_k_clean(stereosub_output2)
estimates_stereo2 <- get_ests_clean(stereosub_output2)
pval_stereo2 <- get_pval_clean(stereosub_output2)
ci_upper_stereo2 <- get_ci_upper_clean(stereosub_output2)
ci_lower_stereo2 <- get_ci_lower_clean(stereosub_output2)
Q_val_stereo2 <- get_Q_clean(stereosub_output2)
Q_pval_stereo2 <- get_Q_p_clean(stereosub_output2)

# Creating the labels for the dataframe
comparison_stereo2 <- c("pt_v_day_control", "pt_v_other_control",
                       "pt_v_objective") 

# Combinind everything into one dataframe
stereo2_combined <- cbind(comparison_stereo2,
                         k_stereo2,
                         estimates_stereo2,
                         pval_stereo2,
                         ci_upper_stereo2,
                         ci_lower_stereo2,
                         Q_val_stereo2,
                         Q_pval_stereo2,
                         I2_overall_stereo2,
                         btw_I2_stereo2,
                         within_I2_stereo2,
                         crossed_I2_stereo2)

knitr::kable(stereo2_combined)
```

### Output from individual models to obtain estimates per comparison:

```{r meta models for feels subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
comparisons_feels <- c("self_v_objective", "self_v_control", 
                      "other_v_objective", "other_v_control") 


## Get an empty list the length of the number of subsetted datasets we need
df_4_feels <- list(length = length(comparisons_feels))

## The for loop getting us subsetted datasets
for (i in seq_along(comparisons_feels)){ # change () in seq_along to filters
  df_4_feels[[i]] <- feels %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == comparisons_feels[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
feels_sub_output <- map(df_4_feels, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_feels <- get_I2_overall_clean(df_4_feels, feels_sub_output)

# I2 value per level of nested and crossed variance
I2_var_levels_feels <-map2(df_4_feels, feels_sub_output, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_feels <- get_btw_study_I2(I2_var_levels_feels)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_feels <- get_within_study_I2(I2_var_levels_feels)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_feels <- get_crossed_study_I2(I2_var_levels_feels) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_feels <- get_k_clean(feels_sub_output)
estimates_feels <- get_ests_clean(feels_sub_output)
pval_feels <- get_pval_clean(feels_sub_output)
ci_upper_feels <- get_ci_upper_clean(feels_sub_output)
ci_lower_feels <- get_ci_lower_clean(feels_sub_output)
Q_val_feels <- get_Q_clean(feels_sub_output)
Q_pval_feels <- get_Q_p_clean(feels_sub_output)

# Combining everything into one dataframe
feels_combined <- cbind(comparisons_feels,
                        k_feels,
                        estimates_feels,
                        pval_feels,
                        ci_upper_feels,
                        ci_lower_feels,
                        Q_val_feels,
                        Q_pval_feels,
                        I2_overall_feels,
                        btw_I2_feels,
                        within_I2_feels,
                        crossed_I2_feels)

knitr::kable(feels_combined)
```

# Moderators

```{r}
meta_stereo1_mod_wide <- meta_stereo1 %>% 
  filter(target_ingroup_nonspecific == "1" | target_ingroup_nonspecific == "2") %>% 
  droplevels(meta_stereo1$target_ingroup_nonspecific) %>% 
  mutate(target_empathetic_need = as.factor(target_empathetic_need))

moderator_filter1 <- c("target_adversary", "target_empathetic_need",
               "target_ingroup_nonspecific","target_out_minor")

for (x in moderator_filter1) {
  stereo_meta1_targinfo <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ x,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1_mod_wide)
  print(stereo_meta1_targinfo)
}

```

```{r stereo 1 target mods, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
meta_stereo1_mod <- meta_stereo1 %>% 
  filter(target_ingroup_nonspecific == "1" | target_ingroup_nonspecific == "2") %>% 
  droplevels(meta_stereo1$target_ingroup_nonspecific) %>% 
  mutate(target_empathetic_need = as.factor(target_empathetic_need)) %>% 
  pivot_longer(target_ingroup_nonspecific:target_empathetic_need) %>% 
  mutate(target_mod = as.factor(name),
         target_mod_level = value) %>% 
  dplyr::select(-value, - name)

# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function- using the same pt comparisons as before

moderator_filter[i] <- c("target_adversary", "target_empathetic_need",
               "target_ingroup_nonspecific","target_out_minor")

## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo1_mod <- list(length = length(stereo_pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter1)){ # change () in seq_along to filters
  df_4_stereo1[[i]] <- meta_stereo1 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter1[i] &
             target_mod == moderator_filter[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output1 <- map(df_4_stereo1, ~rma.mv(dunb,
                         var_dunb,
                         
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))


# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
pt_filter1 <- c("pt_v_control", "pt_v_control", "pt_v_control", 
                  "pt_v_objective", "pt_v_objective", "pt_v_objective") 

outcome_filter1 <- c("Stereotyping", "Overlap", "Interpersonal Feels",
                     "Stereotyping", "Overlap", "Interpersonal Feels")

## Get an empty list the length of the number of subsetted datasets we need
df_4_overall <- list(length = length(pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(pt_filter1)){
  df_4_overall[[i]] <- meta_overall1 %>%
    filter(pt_comparison == pt_filter1[i] & 
             outcome_type == outcome_filter1[i])
  }

# Mapping over each subsetted dataset to run the meta-analysis
meta_output <- map(df_4_overall, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

```


## Adding in PT unspecified

```{r meta overall ptus data prep}
meta_overall_ptus <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6", "10", "11"),
                                      pt_v_objective = c("3", "7", "12"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "1",
                       dunb*-1,
                       ifelse(outcome_type != "1",
                              dunb*1, NA)),
         outcome_type = recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) 

```

### Overall meta-analytic effect before we look into theoretical moderators with PTUS

```{r meta overall ptus no mods}
meta_overall_ptus_model <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)
meta_overall_ptus_model
```

Slightly larger, but not much different than without adding PT unspecified.

### Multivariate model with perspective taking only with PTUS

```{r meta overall ptus pt mod}
#setting up dummy codes; stereotyping and pt_v_control are the respective comparison groups because they are coded as 1
contrasts(meta_overall_ptus$pt_comparison) <- contr.treatment(2)
contrasts(meta_overall_ptus$pt_comparison) 

# OVerall model across outcome type
meta_multi1_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)

meta_multi1_ptus
```

Pattern did not change - slight increase in intercept estimate (representing pt_v_control) and slight decrease in pt_v_objective estimate, but significance did not change.

## Pre-registered multivariate model with perspective taking and outcome type with PTUS

```{r meta overall interaction ptus}
contrasts(meta_overall_ptus$outcome_type) <- contr.treatment(3)
contrasts(meta_overall_ptus$outcome_type) 

# When we include outcome type
meta_multi2_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_ptus)
meta_multi2_ptus
```

On the interaction (the part that really matters to us), the pattern of results did not change.

All possible pairwise comparisons with PTUS included:

```{r pairwise comparisons ptus}
summary(glht(meta_multi2_ptus, linfct=contrMat(c("pt_v_control:Stereotyping"=1,
                                            "pt_v_control:Overlap"=1,
                                            "pt_v_control:Interpersonal Feels"=1,
                                       "pt_v_objective:Stereotyping"=1,
                                       "pt_v_objective:Overlap"=1,
                                       "pt_v_objective:Interpersonal Feels"=1), 
                                       type="Tukey")), 
        test=adjusted("none"))

```

When adding in PTUS, a few more pairwise comparisons become significant or trending.The comparison that is newly significant that was previously NS: pt_v_objective:Interpersonal Feels - pt_v_objective:Stereotyping == 0. The comparison that was previously NS that is now trending: pt_v_objective:Overlap - pt_v_control:Overlap == 0 (but .099 trending...).  

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r overall meta pt v control stereo ptus}
meta_overall_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")


meta_control_stereo_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_overall_subset_ptus)
```

```{r overall meta pt v control overlap ptus}
control_overlap_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")


meta_control_overlap_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset_ptus)
```

```{r overall meta pt v control interpersonal ptus}
control_inter_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")
```

Pairwise comparisons:

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type3:target_information2

```{r pairwise comparisons overall meta target info mod 1}
anova(meta_multi4, btt=15:16)
```

NS - The difference between these two comparisons is outcome category (merging vs stereotyping), with pt_v_control and medium target information help constant.

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type2:target_information3 

```{r pairwise comparisons overall meta target info mod 2}
anova(meta_multi4, btt=15:17)
```

Sig - The difference between these two is target information, they are both the merging outcome category, but one is medium target information and one is detailed.

* pt_comparison2:outcome_type2:target_information2 - pt_comparison2:outcome_type3:target_information3

```{r pairwise comparisons overall meta target info mod 3}
anova(meta_multi4, btt=15:18)
```

Sig - There is a significant difference between merging/medium target information and stereotyping/detailed target information.

* pt_comparison2:outcome_type3:target_information2 - pt_comparison2:outcome_type2:target_information3 

```{r pairwise comparisons overall meta target info mod 4}
anova(meta_multi4, btt=16:17)
```

NS - These is not a difference when stereotyping has medium target information and it is compared to merging with detailed information and pt_v_control is help constant. This is weird, since the contrast with opposing groupings (stereotyping/high detail vs merging/medium detail).

* pt_comparison2:outcome_type3:target_information2 - pt_comparison2:outcome_type3:target_information3 

```{r pairwise comparisons overall meta target info mod 5}
anova(meta_multi4, btt=16:18)
```

NS

* pt_comparison2:outcome_type2:target_information3 - pt_comparison2:outcome_type3:target_information3 

```{r pairwise comparisons overall meta target info mod 5}
anova(meta_multi4, btt=17:18)
```

Marginal - When pt_v_control and detailed target information is held constant, there is a difference between merging and stereotyping outcome groups.


meta_control_inter_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset_ptus)
```

```{r overall meta pt vs objective stereo ptus}
obj_stereo_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")


meta_obj_stereo_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset_ptus)
```

```{r overall meta pt vs objective overlap ptus}
obj_overlap_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")


meta_obj_overlap_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset_ptus)
```

```{r overall meta pt vs objective interpersonal ptus}
obj_inter_subset_ptus <- meta_overall_ptus %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")


meta_obj_inter_ptus <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset_ptus)
```

```{r overall meta effect table ptus}
effect_table_ptus <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo_ptus[1],
                                 meta_obj_stereo_ptus[1],
                                 meta_control_overlap_ptus[1],
                                 meta_obj_overlap_ptus[1],
                                 meta_control_inter_ptus[1],
                                 meta_obj_inter_ptus[1]),
                      pvalue = c(meta_control_stereo_ptus[5],
                                 meta_obj_stereo_ptus[5],
                                 meta_control_overlap_ptus[5],
                                 meta_obj_overlap_ptus[5],
                                 meta_control_inter_ptus[5],
                                 meta_obj_inter_ptus[5]),
                      ci_upper = c(meta_control_stereo_ptus[6],
                                 meta_obj_stereo_ptus[6],
                                 meta_control_overlap_ptus[6],
                                 meta_obj_overlap_ptus[6],
                                 meta_control_inter_ptus[6],
                                 meta_obj_inter_ptus[6]),
                      ci_lower = c(meta_control_stereo_ptus[7],
                                 meta_obj_stereo_ptus[7],
                                 meta_control_overlap_ptus[7],
                                 meta_obj_overlap_ptus[7],
                                 meta_control_inter_ptus[7],
                                 meta_obj_inter_ptus[7]))
effect_table_ptus 
```

In comparing the two tables, the only significant difference in effect sizes is that the estimate in "PT vs control" condition in the stereotyping outcome increases from .039 to .101. However, it is still not significantly different from 0. The other estimates are all approximately the same.

# Code from when I wrote every all multi models by hand

```{r overall model pt vs control stereo}
control_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Stereotyping")

meta_control_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_stereo_subset )

stereo_control_I2 <- get_I2_overall(control_stereo_subset, meta_control_stereo)
get_I2_var_levels(control_stereo_subset, meta_control_stereo)
```

```{r overall model pt v control overlap}
control_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Overlap")

meta_control_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_overlap_subset)

overlap_control_I2 <- get_I2_overall(control_overlap_subset, meta_control_overlap)
```

```{r overall model pt v control interpersonal}
control_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_control" & outcome_type == "Interpersonal Feels")

meta_control_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = control_inter_subset)

inter_control_I2 <- get_I2_overall(control_inter_subset, meta_control_inter)
```

```{r overall model pt v objective stereo}
obj_stereo_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Stereotyping")

meta_obj_stereo <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_stereo_subset)

stereo_object_I2 <- get_I2_overall(obj_stereo_subset, meta_obj_stereo)
```

```{r overall model pt v objective overlap}
obj_overlap_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Overlap")

meta_obj_overlap <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_overlap_subset)

overlap_object_I2 <- get_I2_overall(obj_overlap_subset, meta_obj_overlap)
```

```{r overall model pt v objective interpersonal }
obj_inter_subset <- meta_overall1 %>% 
  filter(pt_comparison == "pt_v_objective" & outcome_type == "Interpersonal Feels")

meta_obj_inter <- rma.mv(dunb, 
                      var_dunb, 
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = obj_inter_subset)

inter_object_I2 <- get_I2_overall(obj_inter_subset, meta_obj_inter)
```

```{r overall model effect size table}
effect_table <- cbind(label = c("Pt vs Control in Stereotyping", 
                              "PT vs Objective in Stereotyping",
                              "PT vs Control in Overlap",
                              "PT vs Cobjective in Overlap",
                              "PT vs Control in Interpersonal feelings",
                              "PT vs Objective in Interpersonal feelings"),
                      estimate = c(meta_control_stereo[1],
                                 meta_obj_stereo[1],
                                 meta_control_overlap[1],
                                 meta_obj_overlap[1],
                                 meta_control_inter[1],
                                 meta_obj_inter[1]),
                      pvalue = c(meta_control_stereo[5],
                                 meta_obj_stereo[5],
                                 meta_control_overlap[5],
                                 meta_obj_overlap[5],
                                 meta_control_inter[5],
                                 meta_obj_inter[5]),
                      ci_upper = c(meta_control_stereo[6],
                                 meta_obj_stereo[6],
                                 meta_control_overlap[6],
                                 meta_obj_overlap[6],
                                 meta_control_inter[6],
                                 meta_obj_inter[6]),
                      ci_lower = c(meta_control_stereo[7],
                                 meta_obj_stereo[7],
                                 meta_control_overlap[7],
                                 meta_obj_overlap[7],
                                 meta_control_inter[7],
                                 meta_obj_inter[7]),
                      I2 = c(stereo_control_I2[1],
                             stereo_object_I2[1],
                             overlap_control_I2[1],
                             overlap_object_I2[1],
                             inter_control_I2[1],
                             inter_object_I2[1]))

effect_table
```

# Paralell iteration code from before I wrote the function
```{r}
## Get an empty list the length of the number of subsetted datasets we need
df_4_overall <- list(length = length(pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(pt_filter1)){
  df_4_overall[[i]] <- meta_overall1 %>%
    filter(pt_comparison == pt_filter1[i] & 
             outcome_type == outcome_filter1[i])
  }

# Mapping over each subsetted dataset to run the meta-analysis
meta_output <- map(df_4_overall, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))
```