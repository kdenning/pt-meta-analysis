---
title: "Multivariate Results"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 1
      code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, include = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
#install.packages("metapower")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)
library(metapower)
library(weightr)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("functions/meta_functions.R")

# data prep
## Using function I wrote to clean data
meta_data <- meta_clean_func(converted_data)
```

# Checking if conditions need collapsed {.tabset .tabset-fade .tabset-pills}

## Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

## Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

## Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Descriptive Data Check {.tabset .tabset-fade .tabset-pills}

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping"),
         authors = as.factor(authors),
         author_mod = as.factor(author_mod)) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.
```

## Histograms

```{r}
hist(meta_overall1$d)
hist(meta_overall1$dunb)
hist(meta_overall1$n_overall)
```

The histograms for the outcome - both unbiased and not - looks evenly distributed. N is skewed, but that is not surprising.

## Descriptives

```{r}
descrip_data <- meta_overall1 %>% 
  select(d, dunb, var, var_dunb, authors, outcomes_within_sample_var, pt_comparison, outcome_type, outcome_scale_group, target_information, target_ingroup_nonspecific, target_out_minor, target_adversary, target_empathetic_need, n_overall)

psych::describe(descrip_data)
```

This all looks correct

```{r}
cor_data <- descrip_data %>% 
  select(d, dunb, var, var_dunb, n_overall)

cor(cor_data)
```

- Is it weird that d and dunb are only slightly correlated, while var and var_dunb are almost 100%? Does this matter?
- Makes sense that variance is negatively correlated with sample size, as variance gets smaller as sample size increases and, thus, they have an inverse relationship.

## Boxplot 

```{r}
descrip_data %<>% 
  mutate(pt_comparison = dplyr::recode(pt_comparison,
                                       `pt_v_objective` = "v_obj",
                                       `pt_v_control` = "v_ctrl"),
         outcome_type = dplyr::recode(outcome_type,
                                      `Interpersonal Feels` = "feels",
                                      `Overlap` = "ovrlp",
                                      `Stereotyping` = "stereo"))

boxplot(dunb ~ pt_comparison*outcome_type, data = descrip_data,
        col = (c("gold", "darkgreen")),
        main = "Effect sizes by predictors for multivariate model")
```

There are some outlier data points, which was also indicated by the histogram. Check out those that are >= 2 or -2 below to see if they were coded incorrectly by us or in the effect size conversion errored.

## Checking Outliers

**Conclusion based on checks below: The outliers are correctly coded and there are not errors in the analysis. This appears to be valid data that passed our inclusion criteria and, therefore, should be included in the analysis.**

### Outliers >= 2 or -2

```{r}
out_dat <- meta_overall1 %>% 
  filter(dunb >= 2 | dunb <= -2) #7 studies with outliers this large
```

All effect sizes were hand calculated by us using escalc in batches, so going to check that looks normal.

### Checking the first effect size (Abbate et al)

#### Checking escalc with individual data point

```{r}
abbate_eff4_check <- escalc(measure = "SMD", m1i = 1.75, m2i = 3.08, sd1i = 0.36, sd2i = 0.4, n1i = 24, n2i = 19, vtype = "UB") #exactly the same using individual code as running in group
```

It is identical

#### Hand calculating

```{r}
d_num <- (1.75-3.08)
sd_pool <- sqrt((.36^2+.4^2)/2)


d <- d_num/sd_pool
d
```

Slight variation, but pretty much the same - escalc seems to be doing things correctly. Overall, not a problem in the calculation leading to thsi outlier.

### Checking conversions from r (Shih et al; effet size 329)

Entered the data into this website and it was exactly the same: https://www.escal.site/

### Hand calculation

````{r}
d = 2*.87/ sqrt(1 - .87^2)
d
```

Also roughly the same - it appears the calculation is correct. The correlation is VERY large, so this makes sense. Need to re-check the data (for the 5th time) from the articles to see if what we have in the data is correct.

### Checked data from articles

**Looked to see if data was correct that was pulled from the papers:**

Abbate et al - correct
Edwards et al - correct
Lopez-Perez et al - correct
Shih et al (effect size 329) - correct
Shih et al (effect size 331) - correct
Skorinko & Sinclair (360, 361) - correct; these are where we separated an interaction and calculated main effects per subset (stereotypical and less stereotypical stimuli participants)

# Descriptive info {.tabset .tabset-fade .tabset-pills}

## Number of papers

```{r descriptive info before collapsing conditions}
meta_overall1 %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()
```

## Number of studies

**Some papers had multiple studies.**

```{r}
meta_overall1 %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()
```

## Number of samples

**Some studies samples were separated based on different participant groups due to other variables (e.g., different cultures).**

```{r}
meta_overall1 %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

## Unique effect sizes

```{r effect size overall mod}
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()
```

## Average n per cell

**To be used in power analysis**

```{r n per cell multi}
meta_overall1 %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

## Effect sizes per outcome category

```{r}
meta_overall1 %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Effect sizes per comparison and outcome category

```{r effect sizes overall model per outcome}
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

# Overall Multivariate Meta-analysis {.tabset .tabset-fade .tabset-pills}

## Model results

**Contrasts:**

```{r multi contrasts}
contrasts(meta_overall1$outcome_type)
```

### Nesting effect size within sample x outcome scale of measurement

```{r overall meta interaction mod}
meta_multi1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = meta_overall1)

meta_multi1
```

**After looking at the high amount of heterogeneity explained by sample (below), decided to see if nesting within paper explained a large amount as well.**

### Adding a third level nesting within paper (not pre-registered)

```{r overall meta interaction mod with author nesting}
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = meta_overall1)

meta_multi2
```

The intercept is when pt is compared to the objective condition and interpersonal feels is the outcome. When looking at the main effect, this indicates that - in the interpersonal feeling category - the pt v control condition leads to significantly lower effect sizes than the pt vs objective. When comparing pt v objective, there is no difference between the interpersonal feels and overlap conditions, but the stereotyping condition is significantly less than the interpersonal feels condition. The estimates representing the contrasts signify the "difference between the differences" (e.g., how large is the difference between the difference between pt comparison levels and outcome levels being compared in this contrast?). This model demonstrates this difference of differences is significant when comparing the stereotyping outcome and pt v control to the interpersonal feels outcome when pt v objective, but only marginal when comparing pt v control in the overlap outcome to the same reference group.

## Marginal main effects

**Using model with author**

```{r emmeans multi}
qdrq1 <- qdrg(object = meta_multi2, data = meta_overall1)

emmeans_multi <- summary(emmeans(qdrq1, "pt_comparison","outcome_type"))
emmeans_multi

emmeans_multi %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  facet_grid(cols = vars(outcome_type)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Interaction effect on perspective taking comparison effects",
       subtitle = "Higher effect sizes = more positive outcomes",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs Objective','Vs Control'))
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across instructions here and with other effects to a .57). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control (.32), but again, we have more results included. We also obtained our results in the interactive MLM model, not individual meta-analysis models as McAuliffe et al did. MLM models employ shrinkage on the estimates that would not occur when subsetting data down to the individual comparisons, which also accounts for differences between our results and McAuliffe results. This also allows us to retain a larger k.

For the categories other than Interpersonal Feels, none of the other comparisons are statistically different from 0. Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

```{r check if model is over parametized, eval = FALSE}
# Checking if model is over-parametized
# code is very slow, so will not include in markdown
par(mfrow=c(2,2))
profile(meta_multi2)
# Looks fine
```

## Heterogeneity

### I2 overall

#### Without paper

```{r overall I2 multi}
get_I2_overall(meta_overall1, meta_multi1)
```

Overall heterogeneity is very high. 91.13% of the variance can be explained due to differences in the studies not explained by our predictors. 

#### With paper

```{r overall I2 multi with paper}
get_I2_overall(meta_overall1, meta_multi2)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

#### Without paper

```{r levels I2 multi}
get_I2_levels(meta_multi1)
```

Most of this heterogeneity is explained by between-study variance, followed by between-outcome (within-study) variance. 

#### With paper

```{r levels I2 multi with paper}
get_I2_levels(meta_multi2)
```

## Post-hoc Power

* The effect sizes are obtained from the marginal means for each of the cells
* Sample size is the average found earlier (n = 59) to the nearest number the r function would take
* k is using all effect sizes coded (375)

```{r power multi}
subgroup_power(n_groups = 6, 
               effect_sizes = c(.57, .32, .34, .30, -.07, -.001), 
               sample_size = 120,
               k = 375,
               es_type = "d")
```

# Publication Bias {.tabset .tabset-fade .tabset-pills}

## Histograms of p-values

### Any outcome category

```{r overall p hist}
p_val_data <- converted_data %>% 
  select(authors, Year, study_num_total, study_num_per_article, 
         sample_number_total, p_value, pt_comparison, interpersonl_subcategory) %>% 
  mutate(p_value = as.numeric(p_value)) %>% 
  na.omit() %>% 
   mutate(p_val_cat = case_when(p_value <=.01 ~ "<=.01",
                               p_value >.01 & p_value <=.02 ~ ">.01 - <=.02",
                               p_value >.02 & p_value <=.05 ~ ">.02 - <=.05",
                               p_value >.05 & p_value <=.10 ~ ">.05 - <=.10",
                               p_value >.10 & p_value <=.30 ~ ">.10 - <=.30",
                               p_value >.30 & p_value <=.50 ~ ">.30 - <=.50",
                               p_value >.50 & p_value <=.70 ~ ">.50 - <=.70",
                               p_value >.70 & p_value <=.90 ~ ">.70 - <=.90",
                               p_value >.90 ~ ">.9"))
  

hist(p_val_data$p_value,
     main = "Histogram of all p-values across outcome",
     xlab = "p-values",
     xlim = c(0, 1.0),
     breaks = 100)
abline(v = .05, col="red", lwd=1, lty=1)
```

**Table of p-values:**

```{r overall p table}
p_val_table <- p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count() %>% 
  mutate(p_val_cat = as.factor(p_val_cat)) 
p_val_table
```

**Percent of non-sig. p-values:**

```{r no sig overall p}
total_pvals <- sum(p_val_table$n)
nonsig_pvals_table <- p_val_table %>% 
  filter(p_val_cat != "<=.01" & 
           p_val_cat != ">.01 - <=.02" &
           p_val_cat !=">.02 - <=.05") 
nonsig_pvals <- sum(nonsig_pvals_table$n)

# Amount of p-values that are not significant
nonsig_percent <- nonsig_pvals/total_pvals*100
nonsig_percent
```

**Percent of sig. p-values:**

```{r sig overall p}
# Amount of p-values that are significant
100 - nonsig_percent
```

### Stereotyping

```{r stereo p val hist}
stereo_p_val_data <- p_val_data %>% 
  filter(interpersonl_subcategory == 1)

hist(stereo_p_val_data$p_value,
     main = "Histogram of p-values for stereotyping outcome",
     xlab = "p-values",
     xlim = c(0, 1.0),
     breaks = 100)
abline(v = .05, col="red", lwd=1, lty=1)
```

**Table of p-values:**

```{r stereo p val table}
p_val_table_stereo <- stereo_p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count() %>% 
  mutate(p_val_cat = as.factor(p_val_cat)) 
p_val_table_stereo
```

**Percent of non-sig. p-values:**

```{r stereo p vals non sig}
stereo_total_pvals <- sum(p_val_table_stereo$n)
stereo_nonsig_pvals_table <- p_val_table_stereo %>% 
  filter(p_val_cat != "<=.01" & 
           p_val_cat != ">.01 - <=.02" &
           p_val_cat !=">.02 - <=.05") 
stereo_nonsig_pvals <- sum(stereo_nonsig_pvals_table$n)

# Amount of p-values that are not significant
stereo_nonsig_percent <- stereo_nonsig_pvals/stereo_total_pvals*100
stereo_nonsig_percent
```

**Percent of sig. p-values:**

```{r stereo p vals sig}
# Amount of p-values that are significant
100 - stereo_nonsig_percent
```

### Merging/Overlap

```{r mergin p val hist}
overlap_p_val_data <- p_val_data %>% 
  filter(interpersonl_subcategory == 2)

hist(overlap_p_val_data$p_value,
     main = "Histogram of p-values for merging/overlap outcome",
     xlab = "p-values",
     xlim = c(0, 1.0),
     breaks = 100)
abline(v = .05, col="red", lwd=1, lty=1)
```

**Table of p-values:**

```{r merging p val table}
overlap_p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count()

p_val_table_overlap <- overlap_p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count() %>% 
  mutate(p_val_cat = as.factor(p_val_cat)) 
p_val_table_overlap
```

**Percent of non-sig. p-values:**

```{r merging pval no sig}
overlap_total_pvals <- sum(p_val_table_overlap$n)
overlap_nonsig_pvals_table <- p_val_table_overlap %>% 
  filter(p_val_cat != "<=.01" & 
           p_val_cat != ">.01 - <=.02" &
           p_val_cat !=">.02 - <=.05") 
overlap_nonsig_pvals <- sum(overlap_nonsig_pvals_table$n)


# Amount of studies that are not significant
overlap_nonsig_percent <- overlap_nonsig_pvals/overlap_total_pvals*100
overlap_nonsig_percent
```

**Percent of sig. p-values:**

```{r merging pval sig}
# Amount of studies that are significant
100-overlap_nonsig_percent
```

### Interpersonal

```{r feels p val hist}
intpersonal_p_val_data <- p_val_data %>% 
  filter(interpersonl_subcategory == 3)

hist(intpersonal_p_val_data$p_value,
     main = "Histogram of p-values for interpersonal feelings outcome",
     xlab = "p-values",
     xlim = c(0, 1.0),
     breaks = 100)
abline(v = .05, col="red", lwd=1, lty=1)
```

**Table of p-values:**

```{r feels p val table}
intpersonal_p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count()

p_val_table_interpersonal <- intpersonal_p_val_data %>% 
  select(p_val_cat) %>% 
  group_by(p_val_cat) %>% 
  count() %>% 
  mutate(p_val_cat = as.factor(p_val_cat)) 
p_val_table_interpersonal
```

**Percent of non-sig. p-values:**

```{r feels p val non sig}
interpersonal_total_pvals <- sum(p_val_table_interpersonal$n)
p_val_table_interpersonal_nosig <- p_val_table_interpersonal %>% 
  filter(p_val_cat != "<=.01" & 
           p_val_cat != ">.01 - <=.02" &
           p_val_cat !=">.02 - <=.05") 
interpersonal_nonsig_pvals <- sum(p_val_table_interpersonal_nosig$n)

# Amount of p-values that are non-significant
interpersonal_nonsig_percent <- interpersonal_nonsig_pvals/interpersonal_total_pvals*100
interpersonal_nonsig_percent
```

**Percent of sig. p-values:**

```{r feels p val sig}
# Amount of p-values that are significant
100 - interpersonal_nonsig_percent
```

## Egger's MLM - Overall Model

```{r}
# Getting modified covariate that removes artifactual correlation between SMD ES and its related variance
eggers_data <- meta_overall1 %>% 
    mutate(Va = 4/(n_overall), 
         sda = sqrt(Va),
         sdi = sqrt(var_dunb))

# Adapted from Rodgers & Putjevosky (2019) https://osf.io/7ak2m/?view_only=e74f9ddad3834e0e8c0bae9ef6b0441c

mlma_egg_mod_sda <- rma.mv(dunb ~ 1 + sda, 
                           V = var_dunb, 
                           random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data, test = "t")


mlma_egg_mod_se <- rma.mv(dunb ~ 1 + sdi, 
                           V = var_dunb, 
                           random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data, test = "t")


mlma_results_sda <- with(mlma_egg_mod_sda, data.frame(Egger_test = "Modified Covariate",
                                                      beta = b[2], 
                                                      se = se[2], 
                                                      p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

mlma_results_se <- with(mlma_egg_mod_se, data.frame(Egger_test = "Standard Error",
                                                    beta = b[2], 
                                                    se = se[2], 
                                                    p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

egger_overall_results <- rbind(mlma_results_sda, mlma_results_se)
egger_overall_results
```
