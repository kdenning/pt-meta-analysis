---
title: "Multivariate Results"
author: "Kathryn Denning"
date: "1/14/2021"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
#install.packages("metapower")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)
library(metapower)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("functions/meta_functions.R")

# data prep
## Using function I wrote to clean data
meta_data <- meta_clean_func(converted_data)
```

# Checking if conditions need collapsed

## Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

## Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

## Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Multivariate Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

## Descriptive info

### Number of papers

```{r descriptive info before collapsing conditions}
meta_overall1 %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()
```

### Number of studies per paper

```{r}
meta_overall1 %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()
```

### Number of unique samples

```{r}
meta_overall1 %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```


### Unique effect sizes

```{r effect size overall mod}
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell multi}
meta_overall1 %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

### Effect sizes per outcome category

```{r}
meta_overall1 %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

### Effect sizes per comparison and outcome category

```{r effect sizes overall model per outcome}
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Model results

**Contrasts:**

```{r multi contrasts}
contrasts(meta_overall1$outcome_type)
```

```{r overall meta interaction mod}
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), data = meta_overall1)

meta_multi2
```

The intercept is when When pt is compared to the objective condition and interpersonal feels is the outcome. When looking at the main effect, this indicates that - in the interpersonal feeling category - the pt v control condition leads to significantly lower effect sizes the pt vs objective. When comparing pt v objective, there is no difference between the interpersonal feels and overlap conditions, but the stereotyping condition is significantly less than the interpersonal feels condition. The estimates representing the contrasts signify the "difference between the differences" (e.g., how large is the difference between the difference between pt comparison levels and outcome levels being compared in this contrast?). This model demonstrates this difference of differences is significant when comparing the stereotyping outcome and pt v control to the interpersonal feels outcome when pt v objective, but only marginal when comparing pt v control in the overlap outcome to the same reference group.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition) and if they differ from 0. These were obtained using emmeans.

## Marginal main effects

```{r emmeans multi}
qdrq1 <- qdrg(object = meta_multi2, data = meta_overall1)

emmeans_multi <- summary(emmeans(qdrq1, "pt_comparison","outcome_type"))
emmeans_multi

emmeans_multi %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  facet_grid(cols = vars(outcome_type)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Interaction effect on perspective taking comparison effects",
       subtitle = "Higher effect sizes = more positive outcomes",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs Objective','Vs Control'))
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .53). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control (.28), but again, we have more results included. We also obtained our results in the interactive MLM model, not individual meta-analysis models as McAuliffe et al did. MLM models employ shrinkage on the estimates that would not occur when subsetting data down to the individual comparisons, which also accounts for differences between our results and McAuliffe results. This also allows us to retain a larger k.

For the categories other than Interpersonal Feels, none of the other comparisons are statistically different from 0. Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

```{r check if model is over parametized}
# Checking if model is over-parametized
# code is very slow, so will not include in markdown
par(mfrow=c(2,2))
profile(meta_multi2)
# Looks fine
```

### I2 overall

```{r overall I2 multi}
get_I2_overall(meta_overall1, meta_multi2)
```

Overall heterogeneity is very high. 91.13% of the variance can be explained due to differences in the studies not explained by our predictors. 

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 multi}
get_I2_levels(meta_multi2)
```

Most of this heterogeneity is explained by between-study variance, followed by between-outcome (within-study) variance. 

### Post-hoc Power

* The effect sizes are obtained from the marginal means for each of the cells
* Sample size is the average found earlier (n = 59) to the nearest number the r function would take
* k is using all effect sizes coded (375)

```{r power multi}
subgroup_power(n_groups = 6, 
               effect_sizes = c(.54, .30, .34, .30, -.07, -.01), 
               sample_size = 120,
               k = 375,
               es_type = "d")
```

