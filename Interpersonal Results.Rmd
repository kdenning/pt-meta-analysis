---
title: "Interpersonal Results"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 1
      code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, include = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
#install.packages("metapower")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)
library(metapower)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("functions/meta_functions.R")

# data prep
## Using function I wrote to clean data
meta_data <- meta_clean_func(converted_data)
```

# Descriptives {.tabset .tabset-fade .tabset-pills}

```{r feels data prep, include = FALSE}
feels <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "6"),# there are no day in the life controls in the interpersonal feels category
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"),
         authors = as.factor(authors))
feels %<>% 
  filter(outcome_type == "3")

```

### Number of overall effects:

```{r number of effect sizes feels}
feels %>% 
  count()
```

### Effects per comparison group:

```{r effects pt feels data}
feels %>% 
  group_by(pt_comparison) %>% 
  count()
```

19 is one under our limit of 20, so this will need to be taken as exploratory for the purposes of this preliminary analysis for my prelims. I want to look at these comparisons because they speak to McAuliffe et al's meta-analytic results.

### Number of samples:

```{r overlap samples}
feels %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{r n per cell overlap}
feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

# Interpersonal Feelings Results {.tabset .tabset-fade .tabset-pills}

## Model results

**Contrasts:**

```{r feels pt contrasts}
contrasts(feels$pt_comparison)
```

### Results nesting outcomes within sample x by scale type

```{r r feels model pt}
feels$outcome_scale_group <- droplevels(feels$outcome_scale_group)

feels_model <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), 
                      data = feels)

feels_model
```

### Adding paper as nesting factor

```{r r feels model pt auth}
feels_model_auth <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), 
                      data = feels)

feels_model_auth
```

Looking at the sigma's, it is clear that paper accounts for a large amount of variance and should be included in the model. We will proceed with this model.

The intercept (self vs objective) is significantly different from 0. The simple effects indicate that self v control and other v control lead to significantly lower effect sizes than self v objective. Other v objective did not differ from the intercept.

### Checking if model is over-parametized

```{r check if model is over parametized, eval = FALSE}
# Checking if model is over-parametized
par(mfrow=c(2,2))
profile(feels_model_auth)
```

Looks good!

### Marginal main effects:

#### Without author/paper:

```{r emmeans feels breakdown}
qdrq_feels1 <- qdrg(object = feels_model_auth, data = feels)

emmeans_feels1 <- summary(emmeans(qdrq_feels1, "pt_comparison"))
emmeans_feels1

emmeans_feels1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in interpersonal only",
       subtitle = "Higher effect sizes = better outcome",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

The marginal main effects indicate that all effect sizes differ from 0. However, both self and other vs objective have medium to large effect sizes, and other and self v control have a small effect size.

## Heterogeneity

### Overall I2 for model

#### Without paper:

```{r overall I2 feels}
get_I2_overall(feels, feels_model)
```

High heterogeneity.

#### With paper:

```{r overall I2 feels auth}
get_I2_overall(feels, feels_model_auth)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

#### Without paper:

```{r levels I2 feels no paper}
get_I2_levels(feels_model)
```

Most heterogeneity is explained by between-studies variance.

#### With paper:

```{r levels I2 feels paper}
get_I2_levels(feels_model_auth)
```

When including paper in the model, most variance is explained by paper.

## Post-hoc power

* k = 184 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power feels}
subgroup_power(n_groups = 4, 
               effect_sizes = c(.54, .31, .56, .30), 
               sample_size = 124,
               k = 184,
               es_type = "d")
```

## Forest plot

```{r}
par(mfrow=c(1,1))
forest(feels_model_auth, 
       slab = paste(feels$authors, 
                    feels$Year, sep = ", "),
       header="Author(s) and Year",
       main = "Interpersonal feelings forest plot",
       ylim = c(-7, 203),
       rows = c(-2:16, 23:57, 64:125, 132:201), 
       order = order(feels$pt_comparison),
       addfit=FALSE,
       cex=0.3,
       top = 0)

# self vs obj: 19
# self v control: 35
# other v objective: 62
# other v control: 70

text(x=-9.7, y=-5, labels=c("Self vs Objective"), cex = .6)
addpoly(x = 0.544, 
        sei = 0.138, 
        ci.lb = 0.2739,
        ci.ub = 0.815, 
        rows = -5,
        cex = .6)

text(x=-9.8, y=20, labels=c("Self vs control"), cex = .6)
addpoly(x = 0.315, 
        sei = 0.135, 
        ci.lb = 0.0509,
        ci.ub = 0.579, 
        rows = 20,
        cex = .6)

text(x=-9.7, y=60.5, labels=c("Other vs Objective"), cex = .6)
addpoly(x = 0.559, 
        sei = 0.130, 
        ci.lb = 0.3044,
        ci.ub = 0.813, 
        rows = 60.5,
        cex = .6)

text(x=-9.8, y=129, labels=c("Other vs control"), cex = .6)
addpoly(x = 0.304, 
        sei = 0.130, 
        ci.lb = 0.0485,
        ci.ub = 0.560, 
        rows = 129,
        cex = .6)
```

# Moderator Results {.tabset .tabset-fade .tabset-pills}

## Scales of measurement

```{r feels model scales mod}
feels %>% 
  group_by(outcome_scale_group) %>% 
  count()

feels_mod <- feels %>% 
  filter(outcome_scale_group != 16) %>% 
  unique() #removing this level since there is only one study in it

feels_mod$outcome_scale_group <- droplevels(feels_mod$outcome_scale_group)

#model with interaction
feels_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var), 
                      data = feels_mod)

feels_model_mod1
```

Scale group 10 (Empathy) is the reference group. Dropped scale group 16 since there was only one study in it. The analysis dropped redundant comparisons that have 0 studies per cell. There was one significant interaction - Self v control:outcome scale group 15 (negative emotions; was reverse coded already).

### Heterogeneity

#### I2 overall

```{r I2 overall scales}
get_I2_overall(feels_mod, feels_model_mod1)
```

Scale of measurement only lowered the I2 by 2%. 

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels scale mods}
get_I2_levels(feels_model_mod1)
```

Most heterogeneity is explained by between-study variance, followed by between-outcomes variance.

## Target - amount of information

```{r feels targ info mod}
feels_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod2
```

None are significant. Emmeans won't work because redundant predictors were dropped from rma.mv model.

### Heterogeneity

#### I2 overall

```{r I2 overall feels targ info}
get_I2_overall(feels_mod, feels_model_mod2)
```

High heterogeneity - moderator did not help.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels feels targ info}
get_I2_levels(feels_model_mod2)
```

Most variance explained by between-study variance, with almost 12% explained by scales of measurement.

## Target - In need/In distress target grouping (relevant to outcome category)

```{r feels targ need mod}
feels_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_emapthetic_need,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod3
```

The intercept is significantly different from 0. The difference between other v control and intercept is marginal. There are no interactions.

### Marginal main effects

```{r emmeans feels empathetic need targ mod}
qdrq_feels3 <- qdrg(object = feels_model_mod3, data = feels_mod)

emmeans_feels3 <- summary(emmeans(qdrq_feels3, "pt_comparison", "target_emapthetic_need"))
emmeans_feels3
```

When the target is empathetic (1), only when the instructions are other focused and the comparison is the control condition is there no difference from 0. Self and other vs control are not different from 0 when the target is empathetic. When the target is not empathetic, the only condition that does not differ from 0 is self vs control.

### Heterogeneity

#### I2 overall

```{r feels need targ I2 overall}
get_I2_overall(feels_mod, feels_model_mod3)
```

High heterogeneity

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r feels need targ I2 levels}
get_I2_levels(feels_model_mod3)
```

Most heterogeneity explained by between-studies variance, with over 13% by scales of measurement.

## Sample size

```{r sample size mod feels}
feels_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod4
````

Intercept different from 0. No interaction with sample size.

### Heterogeneity

#### I2 overall

```{r n feels I2 overall}
get_I2_overall(feels_mod, feels_model_mod4)
```

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r n feels I2 var levels}
get_I2_levels(feels_model_mod4)
```

# Publication Bias - Egger's Test

```{r}
# Getting modified covariate that removes artifactual correlation between SMD ES and its related variance
eggers_data_feels <- feels %>% 
    mutate(Va = 4/(n_overall), 
         sda = sqrt(Va),
         sdi = sqrt(var_dunb))

# Adapted from Rodgers & Putjevosky (2019) https://osf.io/7ak2m/?view_only=e74f9ddad3834e0e8c0bae9ef6b0441c

mlma_egg_mod_sda_feels <- rma.mv(dunb ~ 1 + sda, 
                           V = var_dunb, 
                           random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data_feels, test = "t")


mlma_egg_mod_se_feels <- rma.mv(dunb ~ 1 + sdi, 
                           V = var_dunb, 
                           random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data_feels, test = "t")


mlma_results_sda_feels <- with(mlma_egg_mod_sda_feels, 
                                 data.frame(Egger_test = "Modified Covariate",
                                                      beta = b[2], 
                                                      se = se[2], 
                                                      p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

mlma_results_se_feels <- with(mlma_egg_mod_se_feels, 
                                data.frame(Egger_test = "Standard Error",
                                                    beta = b[2], 
                                                    se = se[2], 
                                                    p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

egger_feels_results <- rbind(mlma_results_sda_feels, mlma_results_se_feels)
egger_feels_results
```

# Checking sub-sample heterogeneity

```{r}
feels %>% 
  mutate(outcome_scale_group = dplyr::recode(outcome_scale_group,
                                             `10` = "empathic emotions",
                                             `11` = "helping behavior",
                                             `13` = "harmful behavior",
                                             `14` = "Other pos/sympathetic emotions",
                                             `15` = "Negative emotions directed toward target",
                                             `16` = "Perceptions of perspective taker")) %>%   # CHECK harmful behavior AND negative emotions are REVERSE CODED 
  count(outcome_scale_group)
```

Cell with most effect sizes is empathic empotions (emapthy, empathic concern).

### Within empathic emotions and each comparison, the cells become uneven.

```{r}
feels %>% 
  filter(outcome_scale_group == 10) %>% 
  group_by(pt_comparison) %>% 
  count()
```

## Meta sub-example w/ pt as moderator

```{r}
heterotest1_feels <- feels %>% 
  filter(outcome_scale_group == 10)

heterotest1_feels$outcome_scale_group <- droplevels(heterotest1_feels$outcome_scale_group)

feels_model_heterotest1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var), data = heterotest1_feels)

feels_model_heterotest1
```

### Heterogeneity

#### Overall

```{r}
get_I2_overall(heterotest1_feels, feels_model_heterotest1)
```

#### Levels

```{r}
get_I2_levels(feels_model_heterotest1)
```

### Post-hoc power

```{r n per cell stereo hetero test 1}
heterotest1_feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

**Rounded up on sample size because "sample size must be a multiple of n_groups."**

```{r power stereo hetero test 1}
subgroup_power(n_groups = 4, 
               effect_sizes = c(1.03, -.39, .07, -.37), 
               sample_size = 68,
               k = 66,
               es_type = "d")
```

## Meta sub-example w/ one pt condition

** Using "other_v_objective" because it had most effect sizes per cell.**

```{r}
heterotest2_feels <- heterotest1_feels %>% 
  filter(pt_comparison == "other_v_objective")

feels_model_heterotest2 <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | authors/sample_number_total/outcomes_within_sample_var), data = heterotest2_feels)

feels_model_heterotest2
```

### Heterogeneity

#### Overall

```{r}
get_I2_overall(heterotest2_feels, feels_model_heterotest2)
```

#### Levels

```{r}
get_I2_levels(feels_model_heterotest2)
```

### Post-hoc power


```{r n per cell stereo hetero test 2}
heterotest2_feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

Code below did not work. However, author had a shiny app using same code, so using that with the below data we found that power at approximately 90% heterogeneity is still high with the incredibly large effect size we found: .99

Shiny app: https://jason-griffin.shinyapps.io/shiny_metapower/

```{r power stereo hetero test 2, eval = FALSE}
mpower(effect_size = .92, study_size = 53, k = 33, i2 = .92, es_type= "d", 
       test_type = "two-tailed", p = 0.05)
```
