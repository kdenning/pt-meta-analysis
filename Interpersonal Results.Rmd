---
title: "Interpersonal Results"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 1
      code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, include = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
#install.packages("metapower")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)
library(metapower)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("functions/meta_functions.R")

# data prep
## Using function I wrote to clean data
meta_data <- meta_clean_func(converted_data)
```

# Interpersonal Feelings Results {.tabset .tabset-fade .tabset-pills}

```{r feels data prep, include = FALSE}
feels <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "6"),# there are no day in the life controls in the interpersonal feels category
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
feels %<>% 
  filter(outcome_type == "3")

```

## Descriptives

### Number of overall effects:

```{r number of effect sizes feels}
feels %>% 
  count()
```

### Effects per comparison group:

```{r effects pt feels data}
feels %>% 
  group_by(pt_comparison) %>% 
  count()
```

19 is one under our limit of 20, so this will need to be taken as exploratory for the purposes of this preliminary analysis for my prelims. I want to look at these comparisons because they speak to McAuliffe et al's meta-analytic results.

### Number of unique samples

```{r overlap samples}
feels %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell overlap}
feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

## Model results

**Contrasts:**

```{r feels pt contrasts}
contrasts(feels$pt_comparison)
```

**Results:**

```{r r feels model pt}
feels$outcome_scale_group <- droplevels(feels$outcome_scale_group)

feels_model <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num, 
                                    ~ 1 | outcome_scale_group), data = feels)

feels_model
```

The intercept (self vs objective) is significantly different from 0. The simple effects indicate that self v control and other v control lead to significantly lower effect sizes than self v objective. Other v objective did not differ from the intercept.

### Checking if model is over-parametized

You can look at the code, but it appeared fine and we will not include the code run due to the code taking so long.

```{r check if model is over parametized, eval = FALSE}
# Checking if model is over-parametized
# code is very slow, so will not include in markdown
par(mfrow=c(2,2))
profile(feels_model)
```

### Marginal main effects:

```{r emmeans feels breakdown}
qdrq_feels1 <- qdrg(object = feels_model, data = feels)

emmeans_feels1 <- summary(emmeans(qdrq_feels1, "pt_comparison"))
emmeans_feels1

emmeans_feels1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in interpersonal only",
       subtitle = "higher effect sizes = better outcome",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

The marginal main effects indicate that all effect sizes except self v control significantly differ from 0. However, both self and other vs objective have medium to large effect sizes, and other v control has a small effect size.

## Heterogeneity

### Overall I2 for model

```{r overall I2 feels}
get_I2_overall(feels, feels_model)
```

High heterogeneity.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 feels}
get_I2_levels(feels_model)
```

Most heterogeneity is explained by between-studies variance. A little more than 11% is explained by between-outcomes variance.

## Post-hoc power

* k = 184 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power feels}
subgroup_power(n_groups = 4, 
               effect_sizes = c(.51, .26, .55, .29), 
               sample_size = 124,
               k = 184,
               es_type = "d")
```

# Moderator Results {.tabset .tabset-fade .tabset-pills}

## Scales of measurement

```{r feels model scales mod}
feels %>% 
  group_by(outcome_scale_group) %>% 
  count()

feels_mod <- feels %>% 
  filter(outcome_scale_group != 16) %>% 
  unique() #removing this level since there is only one study in it

feels_mod$outcome_scale_group <- droplevels(feels_mod$outcome_scale_group)

#model with interaction
feels_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num), 
                      data = feels_mod)

feels_model_mod1
```

Scale group 10 (Empathy) is the reference group. Dropped scale group 16 since there was only one study in it. The analysis dropped redundant comparisons that have 0 studies per cell. There were no significant interactions (so one instruction did not work better with one scale than another in comparison to intercept). 

### Heterogeneity

#### I2 overall

```{r I2 overall scales}
get_I2_overall(feels_mod, feels_model_mod1)
```

Scale of measurement only lowered the I2 by 2%. 

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels scale mods}
get_I2_levels(feels_model_mod1)
```

Most heterogeneity is explained by between-study variance, followed by between-outcomes variance.

## Target - amount of information

```{r feels targ info mod}
feels_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod2
```

None are significant. Emmeans won't work because redundant predictors were dropped from rma.mv model.

### Heterogeneity

#### I2 overall

```{r I2 overall feels targ info}
get_I2_overall(feels_mod, feels_model_mod2)
```

High heterogeneity - moderator did not help.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels feels targ info}
get_I2_levels(feels_model_mod2)
```

Most variance explained by between-study variance, with almost 12% explained by scales of measurement.

## Target - In need/In distress target grouping (relevant to outcome category)

```{r feels targ need mod}
feels_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_emapthetic_need,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod3
```

The intercept is significantly different from 0. The difference between other v control and intercept is marginal. There are no interactions.

### Marginal main effects

```{r emmeans feels empathetic need targ mod}
qdrq_feels3 <- qdrg(object = feels_model_mod3, data = feels_mod)

emmeans_feels3 <- summary(emmeans(qdrq_feels3, "pt_comparison", "target_emapthetic_need"))
emmeans_feels3
```

Self and other vs objective still are different from 0 when the target is empathetic. Self and other vs control are not different from 0 when the target is empathetic. When the target is not empathetic, the only condition that does not differ from 0 is self vs control.

### Heterogeneity

#### I2 overall

```{r feels need targ I2 overall}
get_I2_overall(feels_mod, feels_model_mod3)
```

High heterogeneity

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r feels need targ I2 levels}
get_I2_levels(feels_model_mod3)
```

Most heterogeneity explained by between-studies variance, with over 13% by scales of measurement.

## Sample size

```{r sample size mod feels}
feels_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod4
````

Intercept different from 0. No interaction with sample size.

### Heterogeneity

#### I2 overall

```{r n feels I2 overall}
get_I2_overall(feels_mod, feels_model_mod4)
```

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r n feels I2 var levels}
get_I2_levels(feels_model_mod4)
```

# Publication Bias - Egger's Test

```{r}
# Getting modified covariate that removes artifactual correlation between SMD ES and its related variance
eggers_data_feels <- feels %>% 
    mutate(Va = 4/(n_overall), 
         sda = sqrt(Va),
         sdi = sqrt(var_dunb))

# Adapted from Rodgers & Putjevosky (2019) https://osf.io/7ak2m/?view_only=e74f9ddad3834e0e8c0bae9ef6b0441c

mlma_egg_mod_sda_feels <- rma.mv(dunb ~ 1 + sda, 
                           V = var_dunb, 
                           random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data_feels, test = "t")


mlma_egg_mod_se_feels <- rma.mv(dunb ~ 1 + sdi, 
                           V = var_dunb, 
                           random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num,
                                    ~ 1 | outcome_scale_group), 
                           data = eggers_data_feels, test = "t")


mlma_results_sda_feels <- with(mlma_egg_mod_sda_feels, 
                                 data.frame(Egger_test = "Modified Covariate",
                                                      beta = b[2], 
                                                      se = se[2], 
                                                      p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

mlma_results_se_feels <- with(mlma_egg_mod_se_feels, 
                                data.frame(Egger_test = "Standard Error",
                                                    beta = b[2], 
                                                    se = se[2], 
                                                    p_val = pt(zval[2], dfs, lower.tail = FALSE))) #this is the t-value

egger_feels_results <- rbind(mlma_results_sda_feels, mlma_results_se_feels)
egger_feels_results
```

# Checking sub-sample heterogeneity

```{r}
feels %>% 
  mutate(outcome_scale_group = dplyr::recode(outcome_scale_group,
                                             `10` = "empathic emotions",
                                             `11` = "helping behavior",
                                             `13` = "harmful behavior",
                                             `14` = "Other pos/sympathetic emotions",
                                             `15` = "Negative emotions directed toward target",
                                             `16` = "Perceptions of perspective taker")) %>%   # CHECK harmful behavior AND negative emotions are REVERSE CODED 
  count(outcome_scale_group)
```

Cell with most effect sizes is empathic empotions (emapthy, empathic concern).

### Within empathic emotions and each comparison, the cells become uneven.

```{r}
feels %>% 
  filter(outcome_scale_group == 10) %>% 
  group_by(pt_comparison) %>% 
  count()
```

## Meta sub-example w/ pt as moderator

```{r}
heterotest1_feels <- feels %>% 
  filter(outcome_scale_group == 10)

heterotest1_feels$outcome_scale_group <- droplevels(heterotest1_feels$outcome_scale_group)

feels_model_heterotest1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var/effect_size_num), data = heterotest1_feels)

feels_model_heterotest1
```

### Heterogeneity

#### Overall

```{r}
get_I2_overall(heterotest1_feels, feels_model_heterotest1)
```

#### Levels

```{r}
get_I2_levels(feels_model_heterotest1)
```

### Post-hoc power

```{r n per cell stereo hetero test 1}
heterotest1_feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

**Rounded up on sample size because "sample size must be a multiple of n_groups."**

```{r power stereo hetero test 1}
subgroup_power(n_groups = 4, 
               effect_sizes = c(1.03, -.39, .07, -.37), 
               sample_size = 68,
               k = 66,
               es_type = "d")
```

## Meta sub-example w/ one pt condition

** Using "other_v_objective" because it had most effect sizes per cell.**

```{r}
heterotest2_feels <- heterotest1_feels %>% 
  filter(pt_comparison == "other_v_objective")

feels_model_heterotest2 <- rma.mv(dunb, 
                      var_dunb,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/effect_size_num), data = heterotest2_feels)

feels_model_heterotest2
```

### Heterogeneity

#### Overall

```{r}
get_I2_overall(heterotest2_feels, feels_model_heterotest2)
```

#### Levels

```{r}
get_I2_levels(feels_model_heterotest2)
```

### Post-hoc power


```{r n per cell stereo hetero test 2}
heterotest2_feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

Code below did not work. However, author had a shiny app using same code, so using that with the below data we found that power at approximately 90% heterogeneity is still high with the incredibly large effect size we found: .99

Shiny app: https://jason-griffin.shinyapps.io/shiny_metapower/

```{r power stereo hetero test 2, eval = FALSE}
mpower(effect_size = .92, study_size = 53, k = 33, i2 = .92, es_type= "d", 
       test_type = "two-tailed", p = 0.05)
```
