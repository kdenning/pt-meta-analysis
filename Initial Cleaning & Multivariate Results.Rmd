---
title: "Multivariate Results"
author: "Kathryn Denning"
date: "1/14/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(dmetar)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
prelims_data <- import("prelims_data.xlsx") 
```

```{r functions I wrote for this doc}
# Instructions to hand calculate I2 are here https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate
#Function to get I2
get_I2_overall <- function(.x, .y) { #.x is the dataset supplied to the rma.mv function, .y is the output of that function
  W <- diag(1/.x$var_dunb)
  X <- model.matrix(.y)
  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
  I2 <- 100 * sum(.y$sigma2)/ (sum(.y$sigma2) + (.y$k-.y$p)/sum(diag(P)))
  return(I2)
}

# And getting it cleaned per a list of results

get_I2_overall_clean <- function(x, y){ #x is data that was mapped, y is meta results
  I2 <- data.frame(map2(x, y, ~get_I2_overall(.x, .y)))
  I2 %>% 
    pivot_longer(1:ncol(I2)) %>% 
    mutate(I2_overall = value) %>% 
    select(-value, -name)
}

# total variance attributed to between and within-level clusters separately
## From left to right in every model unless specified:
## Between sample heterogeneity
## Between condition heterogeneity
## Between outcome heterogeneity
## Hetereogeneity due to outcome type crossed between studies

get_I2_var_levels <- function(.x, .y) { #.x is the dataset supplied to the rma.mv function, .y is the output of that function
  W <- diag(1/.x$var_dunb)
  X <- model.matrix(.y)
  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
  I2 <- round(100 * (.y$sigma2)/ (sum(.y$sigma2) + (.y$k-.y$p)/sum(diag(P))), digits = 2)
  return(I2)
}

# Get column for between study hetereogeneity to put in table
get_btw_study_I2 <- function(x){#x is output from function that gets I2 var levels per study
  between_study_I2 <- list(length = length(x))
  
  for (i in seq_along(x)){
    between_study_I2[[i]] <- x[[i]][1]
    }
  
  btw_study_I2 <- data.frame(between_study_I2) 
  btw_study_I2 %>% 
    pivot_longer(1:ncol(btw_study_I2)) %>% 
    mutate(I2_btw_study = value) %>% 
    select(-value, -name)
}

# Get column for within study heterogeneity
get_within_study_I2 <- function(x){#ne meta-analyses with three-level nesting structure, this woudl correspond to within-study variance (within-outcome variance)
  within_study_I2 <- list(length = length(x))
  
  for (i in seq_along(x)){
    within_study_I2[[i]] <- x[[i]][2]
    }
  
  within_I2 <- data.frame(within_study_I2) 
  within_I2 %>% 
    pivot_longer(1:ncol(within_I2)) %>% 
    mutate(I2_within_study = value) %>% 
    select(-value, -name)
}

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later

get_crossed_study_I2 <- function(x){#x is output from function that gets I2 var levels per study
  crossed_study_I2 <- list(length = length(x))
  
  for (i in seq_along(x)){
    crossed_study_I2[[i]] <- x[[i]][3]
    }
  
  crossed_I2 <- data.frame(crossed_study_I2) 
  crossed_I2 %>% 
    pivot_longer(1:ncol(crossed_I2)) %>% 
    mutate(I2_crossed_scale = value) %>% 
    select(-value, -name)
}

# Function that subsets data into multiple datasets based on two filters & runs an rma.mv meta-analysis 
filtered_meta <- function(x, y, z){ #x is the overall dataset, y is the first set of filters, z is the second set of filters
  df <- list(length = length(y))
  
  for (i in seq_along(y)){
  df[[i]] <- x %>%
    dplyr::filter(pt_comparison == y[i] & 
                    outcome_type == z[i])
  }
  
  return(map(df, ~rma.mv(dunb,
                  var_dunb,
                  random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                                ~ 1 | outcome_scale_group),
                  data = .x)))
}

# Functions to extract values from rma.mv output; needed to help when cleaning my nested output
get_pval_rma <- function(x){
  x$pval
}

get_ci_ub_rma <- function(x){
  x$ci.ub
}

get_ci_lb_rma <- function(x){
  x$ci.lb
}

get_Q_rma <- function(x){
  x$QE
}

get_Q_p_rma <- function(x){
  x$QEp
}

# Functions to clean values extracted from rma.mv output

get_ests_clean <- function(x){
  est <- map(x, ~coef(.x)) %>% 
  as.data.frame()
  est %>% 
    pivot_longer(1:ncol(est)) %>% 
    mutate(estimate = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 3)
}

get_pval_clean <- function(x){#x is data frame
  pvals <- map(x, ~get_pval_rma(.x)) %>% 
  as.data.frame()
  pvals %>% 
    pivot_longer(1:ncol(pvals)) %>% 
    mutate(est_pval = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 3)  
}

get_ci_upper_clean <- function(x) {
  uppers <- map(x, ~get_ci_ub_rma(.x)) %>% 
  as.data.frame()
  uppers %>% 
    pivot_longer(1:ncol(uppers)) %>% 
    mutate(ci_upper = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 3)
}

get_ci_lower_clean <- function(x) {
  lower <- map(x, ~get_ci_lb_rma(.x)) %>% 
  as.data.frame()
  lower %>% 
    pivot_longer(1:ncol(lower)) %>% 
    mutate(ci_upper = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 3)
}

get_Q_clean <- function(x){
  Q <- map(x, ~get_Q_rma(.x)) %>% 
  as.data.frame()
  Q %>% 
    pivot_longer(1:ncol(Q)) %>% 
    mutate(Q = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 3)
}

get_Q_p_clean <- function(x){
  Q_ps <- map(x, ~get_Q_p_rma(.x)) %>% 
  as.data.frame()
  Q_ps %>% 
    pivot_longer(1:ncol(Q_ps)) %>% 
    mutate(Q_pval = value) %>% 
    dplyr::select(-name, - value) %>% 
    round(digits = 4)
}
```

# Convert effect sizes

## Convert F to d

```{r convert F to d}
f_to_d <- prelims_data %>% 
  filter(F_score != "NA")

f_to_d_output <- fes(f = F_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = f_to_d) #function drops 95CI when producing table for some reason

f_to_d_output %<>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  dplyr::select(-c(var.d,id)) %>% 
  dplyr::select(effect_size_num, d, var_d) 

prelims_data_fconverted <- left_join(prelims_data, f_to_d_output, by = "effect_size_num")
```

## Convert t to d

```{r convert t to d}

t_to_d <- prelims_data_fconverted %>% 
  dplyr::select(effect_size_num, t_score, n_pt, n_comparison) %>% 
  filter(t_score!= "NA")

t_to_d_output <- tes(t = t_score, n.1 = n_pt, n.2 = n_comparison, level = 95, id = effect_size_num, data = t_to_d)

t_to_d_output %<>% 
  dplyr::select(id, d, var.d) %>% 
  mutate(effect_size_num = id,
         var_d = var.d) %>% 
  dplyr::select(-var.d,-id)


prelims_data_ftconverted <- left_join(prelims_data_fconverted, t_to_d_output, by = "effect_size_num")
```

## Convert means/sds to d

```{r convert means and sds to d}
# Getting dataset of values that do not have 
mean_sd_data <- prelims_data_ftconverted %>% 
  dplyr::select(effect_size_num, mean_pt, mean_comparison, sd_pt, 
         sd_comparison, n_pt, n_comparison, cohens_d, d.x, d.y) %>% 
  filter(!is.na(mean_pt) & !is.na(mean_comparison) & !is.na(sd_pt) 
         & !is.na(sd_comparison) & is.na(cohens_d))

msd_to_d <- escalc(measure = "SMD", m1i = mean_pt, m2i = mean_comparison, sd1i = sd_pt, sd2i = sd_comparison, n1i = n_pt, n2i = n_comparison, data = mean_sd_data, vtype = "UB") #since we will have to unbias other cohen's d, told escalc to calculate an unbiased SMD

#yi in the output is SMD, vi is sampling variance

msd_to_d %<>% 
  dplyr::select(effect_size_num, yi, vi)

prelims_data_ftmeanconverted <- left_join(prelims_data_ftconverted, msd_to_d, by = "effect_size_num")

```

## Convert r to d

```{r convert r to d}
r_to_d_data <- prelims_data_ftmeanconverted %>% 
  filter(effect_size_type == "r") %>% 
  dplyr::select(effect_size_num, effect_size, n_pt, n_comparison) %>% 
  mutate(effect_size = as.numeric(effect_size))

r_to_d_func <- function(.x) {
  return(effectsize::r_to_d(.x))
}

d_values <- map(.x = r_to_d_data$effect_size, .f = r_to_d_func)

d_values <- data.frame(unlist(d_values)) %>% 
  mutate(d_values = unlist.d_values.) %>% 
  dplyr::select(-unlist.d_values.)

r_to_d_converted <- cbind(r_to_d_data, d_values) %>% 
  mutate(d_values_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((d_values^2)/(2*(n_pt+n_comparison))))) %>% 
  dplyr::select(-n_pt, -n_comparison, - effect_size)

converted_data <- left_join(prelims_data_ftmeanconverted, r_to_d_converted, by = "effect_size_num")
```

## Convert regression coefficients to d

We need SD of the DV - which most studies did not provide - in order to convert this. We will need to add these studies (k = 21) to our "Contact Authors" sheet if they are not already on there for some reason.

```{r data cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Other data prep
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(coded_cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi, d_values)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  dplyr::select(-c(name, value)) %>% 
  pivot_longer(c(coded_cohens_d_var, var_d.x, var_d.y, vi, d_values_var)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  dplyr::select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  dplyr::select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within),
         target_information = as.factor(target_information))

# Saving a clean version of data to be read in different R files
write.csv(meta_data, "meta_data.csv")
```

# Descriptive information about the studies that made it into the meta-analysis

```{r descriptive info before collapsing conditions}
# Number of papers
meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()

# Number of studies
meta_data %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()

# Number of unique samples due to some studies having multiple targets
meta_data %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()

# Unique effect sizes total
meta_data %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per outcome category
meta_data %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Checking if we need to collapse perspective taking conditions

### Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

### Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

### Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

# Making sure Interpersonal feels is now the reference level
levels(meta_overall1$outcome_type)

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

```{r effect size overall mod}
# Effect sizes in overall model
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per comparison and stereotyping category
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```


# Overall Meta-analysis

## Pre-registered multivariate model with perspective taking and outcome type

```{r overall meta interaction mod}
contrasts(meta_overall1$outcome_type)

# When we include outcome type
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_overall1)

meta_multi2
get_I2_overall(meta_overall1, meta_multi2)
get_I2_var_levels(meta_overall1, meta_multi2)
```

The intercept is when both moderators = 0 (When the pt is compared to the objective condition and interpersonal feels is the reference). We are particularly interested in the interaction and estimates. We used interpersonal feels as the reference because it is the the most studied PT sub-field - and the one in which a different meta-analysis already exists - and we are curious if the effect of perspective taking is the same across outcome types or differs. We used pt vs objective because, when combined with interpersonal feels, we would expect the largest effect in that cell. Therefore, we are interested if perspective taking in other outcome types follows this same pattern or differs.

The "pt_comparisonpt_v_control" contrast supports the theory that I just described. In comparison to the intercept - which represents the effect of pt vs objective in the interpersonal context - the objective control condition shows a decrease of effect size (this is with interpersonal feels context held constant). With pt v objective held constant and just changing outcome, the overlap outcome did not differ significantly from interpersonal feels, however, the stereotyping outcome was significantly lower. 

When we look at the interaction we see there is only one significant effect for the contrast. This is not as intuitively interpretable as the main effects. When we vary up both perspective taking and outcome in the fifth estimate (estimate "pt_comparisonpt_v_control:outcome_typeOverlap") looking at pt_v_objective in the overlap condition versus pt_v_control in interpersonal feels condition, we see there there is not a significant difference from the intercept. We do see a significant change in which the slope of the contrast demonstrates a positive increase between the pt_v_control and stereotyping condition versus the intercept.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition). The effect sizes for the interaction relate to the contrast, but cannot be used to find the effect.

To get the effect sizes for each PT comparison in each outcome that would correspond to those pairwise comparisons, we need to run individual meta-analyses on the subsets of the data-set (as seen in McAuliffe meta-analysis).

### Output from individual models to obtain effect sizes per instruction and comparison set:

```{r meta models per multi subsets with I2}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
pt_filter1 <- c("pt_v_control", "pt_v_control", "pt_v_control", 
                  "pt_v_objective", "pt_v_objective", "pt_v_objective") 

outcome_filter1 <- c("Stereotyping", "Overlap", "Interpersonal Feels",
                     "Stereotyping", "Overlap", "Interpersonal Feels")

## Get an empty list the length of the number of subsetted datasets we need
df_4_overall <- list(length = length(pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(pt_filter1)){
  df_4_overall[[i]] <- meta_overall1 %>%
    filter(pt_comparison == pt_filter1[i] & 
             outcome_type == outcome_filter1[i])
  }

# Mapping over each subsetted dataset to run the meta-analysis
meta_output <- map(df_4_overall, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
get_I2_overall_clean(df_4_overall, meta_output)

# I2 value per level of nested and crossed variance
I2_var_levels <-map2(df_4_overall, meta_output, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
get_btw_study_I2(I2_var_levels)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
get_within_study_I2(I2_var_levels)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
get_crossed_study_I2(I2_var_levels) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
estimates <- get_ests_clean(meta_output)
pval <- get_pval_clean(meta_output)
ci_upper <- get_ci_upper_clean(meta_output)
ci_lower <- get_ci_lower_clean(meta_output)
Q_val <- get_Q_clean(meta_output)
Q_pval <- get_Q_p_clean(meta_output)

# Creating the labels for the dataframe
comparison <- c("Pt vs Control in Stereotyping",
                "PT vs Control in Overlap",
                "PT vs Control in Interpersonal feelings",
                "PT vs Objective in Stereotyping",
                "PT vs Objective in Overlap",
                "PT vs Objective in Interpersonal feelings")

# Combinind everything into one dataframe
cbind(data.frame(comparison), 
      data.frame(estimates),
      data.frame(pval),
      data.frame(ci_upper),
      data.frame(ci_lower),
      data.frame(Q_val),
      data.frame(Q_pval),
      btw_study_I2,
      within_I2,
      cross_I2)
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .58). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control, but again, we have more results included, and we are still in the same direction. 

*Note for Sara: This change from before for imagine-other vs no-instructions in the interpersonal feels category is due to me adding three effect sizes converted from correlation coefficients that I had previously forgot to convert... I guess they were outliers, since it increased from .16 to .27*


For the categories other than Interpersonal Feels, only the comparison of pt_v_control is statistically different from 0 for the Overlap/merging category. This is surprising and opposite of the pattern found in the interpersonal feelings category. The comparison of pt_v_objective is marginal, though the estimate itself is not much smaller. This is the category in which the number of studies included is small, which could be impacting these results. We will have to see if these differ from each other when analyzed in their own model.


Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

# Stereotyping

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))


meta_stereo1 %<>% 
  filter(outcome_type == "1")


## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" |
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")
```

## Number of unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Model comparing imagine-self vs imagine-other instructions with a combined control and objective comparisons

```{r stereo model imagine self vs other}
contrasts(meta_stereo1$pt_comparison) 

# OVerall model across outcome type
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/conditions_within_sample_var/outcomes_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1

get_I2_overall(meta_stereo1, stereo_meta1)
```

The test of moderators is not significant, nor are any comparisons. The estimates do not provide us with estimates for each comparison, we will need to run individual models to examine this.

## Estimates for each comparison

```{r meta models for stereo 1 subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
stereo_pt_filter1 <- c("self_v_objective", "self_v_control", 
                      "other_v_objective", "other_v_control") 


## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo1 <- list(length = length(stereo_pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter1)){ # change () in seq_along to filters
  df_4_stereo1[[i]] <- meta_stereo1 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter1[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output1 <- map(df_4_stereo1, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_stereo1 <- get_I2_overall_clean(df_4_stereo1, stereosub_output1)

# I2 value per level of nested and crossed variance
I2_var_levels_stereo1 <-map2(df_4_stereo1, stereosub_output1, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_stereo1 <- get_btw_study_I2(I2_var_levels_stereo1)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_stereo1 <- get_within_study_I2(I2_var_levels_stereo1)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_stereo1 <- get_crossed_study_I2(I2_var_levels_stereo1) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
estimates_stereo1 <- get_ests_clean(stereosub_output1)
pval_stereo1 <- get_pval_clean(stereosub_output1)
ci_upper_stereo1 <- get_ci_upper_clean(stereosub_output1)
ci_lower_stereo1 <- get_ci_lower_clean(stereosub_output1)
Q_val_stereo1 <- get_Q_clean(stereosub_output1)
Q_pval_stereo1 <- get_Q_p_clean(stereosub_output1)

# Creating the labels for the dataframe
comparison_stereo1 <- c("self_v_objective", "self_v_control",
                "other_v_objective", "other_v_control") 

# Combinind everything into one dataframe
cbind(comparison_stereo1, 
      estimates_stereo1,
      pval_stereo1,
      ci_upper_stereo1,
      ci_lower_stereo1,
      Q_val_stereo1,
      Q_pval_stereo1,
      I2_overall_stereo1,
      btw_I2_stereo1,
      within_I2_stereo1,
      crossed_I2_stereo1)
```

