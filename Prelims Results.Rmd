---
title: "Multivariate Results"
author: "Kathryn Denning"
date: "1/14/2021"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("meta_functions.R")
```

```{r data cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# data prep
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(coded_cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi, d_values)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  dplyr::select(-c(name, value)) %>% 
  pivot_longer(c(coded_cohens_d_var, var_d.x, var_d.y, vi, d_values_var)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  dplyr::select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  dplyr::select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within),
         target_information = as.factor(target_information))
```

# Checking if conditions need collapsed

## Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

## Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

## Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Multivariate Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

## Descriptive info

### Number of papers

```{r descriptive info before collapsing conditions}
meta_overall1 %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()
```

### Number of studies per paper

```{r}
meta_overall1 %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()
```

### Number of unique samples

```{r}
meta_overall1 %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```


### Unique effect sizes

```{r effect size overall mod}
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()
```

### Effect sizes per outcome category

```{r}
meta_overall1 %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

### Effect sizes per comparison and outcome category

```{r effect sizes overall model per outcome}
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Model results

**Contrasts:**

```{r multi contrasts}
contrasts(meta_overall1$outcome_type)
```

```{r overall meta interaction mod}
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_overall1)

meta_multi2
```

The intercept is when When pt is compared to the objective condition and interpersonal feels is the outcome. When looking at the main effect, this indicates that - in the interpersonal feeling category - the pt v control condition leads to significantly lower effect sizes the pt vs objective. When comparing pt v objective, there is no difference between the interpersonal feels and overlap conditions, but the stereotyping condition is significantly less than the interpersonal feels condition. The estimates representing the contrasts signify the "difference between the differences" (e.g., how large is the difference between the difference between pt comparison levels and outcome levels being compared in this contrast?). This model demonstrates this difference of differences is significant when comparing the stereotyping outcome and pt v control to the interpersonal feels outcome when pt v objective, but only marginal when comparing pt v control in the overlap outcome to the same reference group.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition) and if they differ from 0. These were obtained using emmeans.

## Marginal means

```{r emmeans multi}
qdrq1 <- qdrg(object = meta_multi2, data = meta_overall1)

emmeans_multi <- summary(emmeans(qdrq1, "pt_comparison","outcome_type"))
emmeans_multi

emmeans_multi %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  facet_grid(cols = vars(outcome_type)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Interaction effect on perspective taking comparison effects",
       subtitle = "Higher effect sizes = more positive outcomes",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs Objective','Vs Control'))
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .54). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control (.29), but again, we have more results included. We also obtained our results in the interactive MLM model, not individual meta-analysis models as McAuliffe et al did. MLM models employ shrinkage on the estimates that would not occur when subsetting data down to the individual comparisons, which also accounts for differences between our results and McAuliffe results. This also allows us to retain a larger k.

For the categories other than Interpersonal Feels, none of the other comparisons are statistically different from 0. Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

### I2 overall

```{r overall I2 multi}
get_I2_overall(meta_overall1, meta_multi2)
```

Overall heterogeneity is very high. 91.13% of the variance can be explained due to differences in the studies not explained by our predictors. 

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 multi}
get_I2_var_levels(meta_overall1, meta_multi2)
```

Most of this heterogeneity is explained by between-study variance, followed by between-outcome (within-study) variance. 

# Stereotyping

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
meta_stereo1 %<>% 
  filter(outcome_type == "1")

meta_stereo1$outcome_scale_group <- droplevels(meta_stereo1$outcome_scale_group)

## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" |
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")

meta_stereo2$outcome_scale_group <- droplevels(meta_stereo2$outcome_scale_group)
```

## Unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Imagine-self vs imagine-other Model

**Contrasts:**

```{r stereo1 conrasts}
contrasts(meta_stereo1$pt_comparison) 
```

```{r stereo model imagine self vs other}
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1
```

The intercept is not significantly different from zero, nor do any effects differ from it.

### Marginal means

```{r emmeans stereo1}
qdrq_stereo1 <- qdrg(object = stereo_meta1, data = meta_stereo1)

emmeans_stereo1 <- summary(emmeans(qdrq_stereo1, "pt_comparison"))
emmeans_stereo1

emmeans_stereo1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in stereotyping only",
       subtitle = "Lower effect sizes = more prejudice reduction (better)",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

None of the marginal means differ from zero. There is an interesting pattern in that self or other vs objective lead to higher effect sizes (less stereotype prejudice reduction). However, with nothing differing from zero, this does not mean anything statistically.

### I2 overall

```{r overall I2 stereo 1}
get_I2_overall(meta_stereo1, stereo_meta1)
```

Again, very high.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 1}
get_I2_var_levels(meta_stereo1, stereo_meta1)
```

Most of the heterogeneity is explained by between-studies variance, with about 16% also explained by between-conditions (within-study) variance. This differed from the heterogeneity pattern found in the overall interaction model.

## Moderators for imagine self vs other model

### Scales of measurement

```{r stereo 1 mod1}
meta_stereo1 %>% 
  group_by(pt_comparison, outcome_scale_group) %>% 
  count() %>% 
  filter(n <2) 

#getting rid of cells with only 1
meta_stereo1_mod1_dat <- meta_stereo1 %>% 
  filter(!c(pt_comparison == "self_v_objective" & outcome_scale_group == "1" |
           pt_comparison == "self_v_control" & outcome_scale_group == "1"))

stereo_meta1_mod1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), data = meta_stereo1_mod1_dat)

stereo_meta1_mod1

```

I dropped cells with only 1 study and rma.mv dropped cells with 0. Nothing is significant from one another, and the intercept is not significantly different from 0. Any model with redudancies (0's in cells) cannot be modeled using emmeans to get marginal means.

#### I2 overall

```{r I2 overall stereo1 mod scales}
get_I2_overall(meta_stereo1_mod1_dat, stereo_meta1_mod1)
```

Still very high, but slightly lower than without the moderater.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod scales}
get_I2_var_levels(meta_stereo1_mod1_dat, stereo_meta1_mod1)
```

Without the crossed variance structure (because it is a moderator), more of that variance shifted to the between-conditions level. Most of the variance is still represented by the between-studies level.

### Target moderator - information amount

```{r stereo 1 mod2}
stereo_meta1_mod2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1_mod2
```

Nothing is significant.

```{r emmeans stereo1 mod2 }
qdrq_stereo1_mod2 <- qdrg(object = stereo_meta1_mod2, data = meta_stereo1)

emmeans_stereo1_mod2 <- summary(emmeans(qdrq_stereo1_mod2, "pt_comparison", "target_information"))
emmeans_stereo1_mod2
```

None of the effect sizes significantly differ from 0 when looking at the marginal means when broken down by target information.

#### I2 overall

```{r I2 overall stereo1 mod2}
get_I2_overall(meta_stereo1, stereo_meta1_mod2)
```

Compared to the original stereotyping model this is moderator, the overall heterogeneity is slightly higher.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod2}
get_I2_var_levels(meta_stereo1, stereo_meta1_mod2)
```

Most of the variance is between-studies, with a somewhat significant amount explained by between-outcome (within-study) variance.

### Target moderator - target out-group

```{r stereo 1 mod3}
stereo1_group_data <- meta_stereo1 %>% 
  select(dunb, var_dunb, pt_comparison, target_out_minor, sample_number_total,
         outcomes_within_sample_var, conditions_within_sample_var, outcome_scale_group) %>% 
  na.omit()

stereo_meta1_mod3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = stereo1_group_data)

stereo_meta1_mod3
```

Nothing is significant.

```{r emmeans stereo1 mod3}
qdrq_stereo1_mod3 <- qdrg(object = stereo_meta1_mod3, data = stereo1_group_data)

emmeans_stereo1_mod3 <- summary(emmeans(qdrq_stereo1_mod3, "pt_comparison", "target_out_minor"))
emmeans_stereo1_mod3
```

None of the effect sizes significantly differ from 0 when looking at the marginal means when broken down by by target group.

#### I2 overall

```{r I2 overall stereo1 mod3}
get_I2_overall(stereo1_group_data, stereo_meta1_mod3)
```

About the same as the original model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod3}
get_I2_var_levels(stereo1_group_data, stereo_meta1_mod3)
```

Most of the variance is explained by between-studies or between-conditions (within-study) variance. Interestingly, there is 0% variance explained by between-outcomes variance.

### Sample size

```{r stereo 1 mod4}
stereo_meta1_mod4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1_mod4
```

Sample size does not interact with pt comparison.

#### I2 overall

```{r I2 overall stereo1 mod4}
get_I2_overall(meta_stereo1, stereo_meta1_mod4)
```

Heterogeneity is about the same as the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod4}
get_I2_var_levels(meta_stereo1, stereo_meta1_mod4)
```

Most variance is explained by between-studies variance, followed by between-outcomes (within-study) variance.

## Model comparing "day in the life," other control, and objective comparisons

**Contrasts:**

```{r stereo2 contrasts}
contrasts(meta_stereo2$pt_comparison) 
```

```{r stereo model day in the life vs other control vs objective}
# OVerall model across outcome type
stereo_meta2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2
```

Pairwise comparisons to compare:

* pt_v_objective to pt_v_other control

```{r pairwise comparisons stereo 2}
anova(stereo_meta2, btt=2:3)
```

They do not significantly differ.

### Marginal means

```{r emmeans stereo2}
qdrq_stereo2 <- qdrg(object = stereo_meta2, data = meta_stereo2)

emmeans_stereo2 <- summary(emmeans(qdrq_stereo2, "pt_comparison"))
emmeans_stereo2

emmeans_stereo2 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "'Day in the life' control vs other comparison conditions in stereotyping only",
       subtitle = "Lower effect sizes = more prejudice reduction (better)",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs `Day in the life` control','Vs other control',
                              'Vs objective'))
```

The marginal means do not differ from 0.

### Overall I2

```{r overall I2 stereo 2}
get_I2_overall(meta_stereo2, stereo_meta2)
```

Heterogeneity is high.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 2}
get_I2_var_levels(meta_stereo2, stereo_meta2)
```

Most hetereogeneity is explained by between-study variance, followed by between-condition variance.

## Moderators for "day in the life" model

### Scales of measurement

```{r stereo 2 mod1}
stereo_meta2_mod1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), data = meta_stereo2)

stereo_meta2_mod1
```

Nothing is significant from one another, and the intercept is not significantly different from 0. Outcome scale group 5 (Essay stereotypicality) is marginally different from the intercept outcome scale group 1 (IAT). Rma.mv dropped cells with 0 due to redundancies, and therefore cannot be modeled in emmeans.

#### I2 overall

```{r I2 overall stereo1 mod scales}
get_I2_overall(meta_stereo2, stereo_meta2_mod1)
```

About the same as the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod scales}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod1)
```

Most hetereogeneity is explained by between-studies variance, followed by between-outcomes (within-study) variance.

### Target moderator - information amount

```{r stereo 2 mod2}
stereo_meta2_mod2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2_mod2
```

Pt v other control with medium target information is marginally different from the intercept (p vs "day  in the life" control with impoverished information). Nothing else is significant from the intercept, nor is the intercept significant.

#### Marginal means

```{r emmeans stereo1 mod2 }
qdrq_stereo2_mod2 <- qdrg(object = stereo_meta2_mod2, data = meta_stereo2)

emmeans_stereo2_mod2 <- summary(emmeans(qdrq_stereo2_mod2, "pt_comparison", "target_information"))
emmeans_stereo2_mod2
```

None of the effect sizes significantly differ from 0 when looking at the marginal means when broken down by target information.

#### I2 overall

```{r I2 overall stereo1 mod2}
get_I2_overall(meta_stereo2, stereo_meta2_mod2)
```

Slightly higher than the original model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod2}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod2)
```

Most hetereogeneity is explained by between-studies variance, followed by between-outcomes (within-study) variance.

### Target moderator - target out-group

```{r stereo 1 mod3}
stereo2_group_data <- meta_stereo1 %>% 
  select(dunb, var_dunb, pt_comparison, target_out_minor, sample_number_total,
         outcomes_within_sample_var, conditions_within_sample_var, outcome_scale_group) %>% 
  na.omit()

stereo_meta2_mod3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = stereo2_group_data)

stereo_meta2_mod3
```

Nothing is significant.

#### Marginal means

```{r emmeans stereo1 mod3}
qdrq_stereo2_mod3 <- qdrg(object = stereo_meta2_mod3, data = stereo2_group_data)

emmeans_stereo2_mod3 <- summary(emmeans(qdrq_stereo2_mod3, "pt_comparison", "target_out_minor"))
emmeans_stereo2_mod3
```

None of the effect sizes significantly differ from 0 when looking at the marginal means when broken down by by target group.

#### I2 overall

```{r I2 overall stereo1 mod3}
get_I2_overall(stereo2_group_data, stereo_meta2_mod3)
```

About the same as the model without moderaters.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod3}
get_I2_var_levels(stereo2_group_data, stereo_meta2_mod3)
```

Most heterogeneity explained by between-studies variance, or between-conditions variance.

### Sample size

```{r stereo 1 mod4}
stereo_meta2_mod4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2_mod4
```

Sample size does not interact with pt comparison. None of the effects are significant.

#### I2 overall

```{r I2 overall stereo1 mod4}
get_I2_overall(meta_stereo2, stereo_meta2_mod4)
```

Heterogeneity is the same as in the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod4}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod4)
```

Hetereogeneity is explained by between-studies variance the most, followed almost equally by between-conditions and between-outcomes variance.

# Overlap

```{r overlap meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
overlap_data <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

overlap_data %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control")) %>% 
  filter(outcome_type == "Overlap")
```

## Number of overall effects:

```{r effects overlap data}
overlap_data %>% 
  count()
```

## Effects per comparison group:

```{r effects pt overlap data}
overlap_data %>% 
  group_by(pt_comparison) %>% 
  count()
```

## Comparing PT comparisons in one model without other outcomes:

**Contrasts:**

```{r overlap contrasts}
contrasts(overlap_data$pt_comparison)
```

```{r overlap model pt}
overlap_data$outcome_scale_group <- droplevels(overlap_data$outcome_scale_group)
levels(overlap_data$outcome_scale_group)

overlap_pt <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = overlap_data)

overlap_pt
```

### I2 overall model

```{r I2 overall overlap}
get_I2_var_levels(overlap_data, overlap_pt)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels overlap}
get_I2_overall(overlap_data, overlap_pt)
```

```{r emmeans overlap}
qdrq_overlap <- qdrg(object = overlap_pt, data = overlap_data)

emmeans_overlap <- summary(emmeans(qdrq_overlap, "pt_comparison"))
emmeans_overlap 
```

This comparison is the same as the overall model. Without the interaction, the values change slightly, but the pattern and significance do not.

## Moderators

### Scale of measurement

```{r r feels model pt}
overlap_data %>% 
  group_by(outcome_scale_group) %>% 
  count()

overlap_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = overlap_data)

overlap_model_mod1

get_I2_overall(overlap_data, overlap_model_mod1)
```

Scale 7 (Self-report IOS merging/scales) while in the pt vs objective comparison is the reference group. There are simple effects in which the effect sizes become significantly lower for the pt vs objective comparison when merging is measured from overlap of traits/attributes or measures of social distancing. There are no interactions. 

The marginal main effects are reported below. When using a self-reported merging scale both pt vs objective and control lead to medium to large effect sizes that differed from 0. When using both the attribution and social distance scales, neither comparison significantly differed from 0.

```{r emmeans feels scale}
qdrq_overlap2 <- qdrg(object = overlap_model_mod1, data = overlap_data)

emmeans_overlap2 <- summary(emmeans(qdrq_overlap2, "pt_comparison", "outcome_scale_group"))
emmeans_overlap2
```

### Target - amount of information

```{r}
overlap_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = overlap_data)

overlap_model_mod2

get_I2_overall(overlap_data, overlap_model_mod2)
```

None are significant. I2 lowered. With there being no redundcies in the model, I wonder if we can model an interaction with scale?

```{r emmeans overlap in-group targ mod}
qdrq_overlap3 <- qdrg(object = overlap_model_mod2, data = overlap_data)

emmeans_overlap3 <- summary(emmeans(qdrq_overlap2, "pt_comparison", "target_information"))
emmeans_overlap3
```

When target information is impoverished and pt is compared to the control, the effect significantly differs from 0 and is medium-sized. Regardless of comparison condition, the effect does not significantly differ from 0 when target information was rated a 2. When target information is rated as highly detailed, both pt comparisons differ from 0. The objective comparison condition led to a medium effect size, while the control comparison lead a large effect size. 

```{r}
#Modeled target information as a main effect since it had no interactions and, because, when I tried to model it with interactions it led to redundancies

overlap_data %>% 
  group_by(pt_comparison, outcome_scale_group, target_information) %>% 
  count() 

2*3*3

#Should be 18 but there are 13, so the redundancies come from the fact that there are cells with 0 in it, which rma.mv is dropping: https://stats.stackexchange.com/questions/223918/multilevel-metaregression-in-r-redundant-predictors-dropped-metafor

overlap_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = overlap_data)

overlap_model_mod3

get_I2_overall(overlap_data, overlap_model_mod3)
```

The I2 decreases having all three predictors in the model, though it is concerning that some cells are 0 and that was causing redundcies. 

### Target - In-group target grouping (relevant to outcome category)

```{r}
overlap_data <- overlap_data %>% 
  filter(target_ingroup_nonspecific == "1" | target_ingroup_nonspecific == "2")

overlap_data$target_ingroup_nonspecific <- droplevels(overlap_data$target_ingroup_nonspecific)

overlap_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_ingroup_nonspecific,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = overlap_data)

overlap_model_mod4

get_I2_overall(overlap_data, overlap_model_mod4)
```

Tried running an out-group moderator model as well, but it had a convergence issue.

The intercept (pt_v_objective when the target is your in-group) is significantly different from 0. No effects differ from the intercept.


```{r emmeans overlap in-group targ mod}
qdrq_overlap4 <- qdrg(object = overlap_model_mod4, data = overlap_data)

emmeans_overlap4 <- summary(emmeans(qdrq_overlap3, "pt_comparison", "target_ingroup_nonspecific"))
emmeans_overlap4
```

The marginal means indicate that the average effect size is highest when pt is compared to the objective condition and the target is an in-group target, followed by when the target is an in-group and pt is compared to the control. Both significantly differed from 0. The effect sizes when the target is not an in-group member border being non-signfiant and are about the same regardless of if pt is being compared to the objective or control condition. Both are lower than the in-group effect sizes.

### Sample size

```{r sample size mod overlap}
overlap_model_mod5 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = overlap_data)

overlap_model_mod5

get_I2_overall(overlap_data, overlap_model_mod5)
```

Intercept different from 0. No interaction with sample size.

# Interpersonal Feelings

```{r}
feels <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "6"),# there are no day in the life controls in the interpersonal feels category
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
feels %<>% 
  filter(outcome_type == "3")

```

## Number of overall effects:

```{r number of effect sizes feels}
feels %>% 
  count()
```

## Effects per comparison group:

```{r effects pt feels data}
feels %>% 
  group_by(pt_comparison) %>% 
  count()
```

19 is one under our limit of 20, so this will need to be taken as exploratory for the purposes of this preliminary analysis for my prelims. I want to look at these comparisons because they speak to McAuliffe et al's meta-analytic results.

## Model results

**Contrasts:**

```{r feels pt contrasts}
contrasts(feels$pt_comparison)
```

```{r r feels model pt}
feels$outcome_scale_group <- droplevels(feels$outcome_scale_group)

feels_model <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = feels)

feels_model
```

### Overall I2 for model

```{r overall I2 feels}
get_I2_overall(feels, feels_model)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 feels}
get_I2_var_levels(feels, feels_model)
```

```{r emmeans feels breakdown}
qdrq_feels1 <- qdrg(object = feels_model, data = feels)

emmeans_feels1 <- summary(emmeans(qdrq_feels1, "pt_comparison"))
emmeans_feels1

emmeans_feels1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in interpersonal only",
       subtitle = "higher effect sizes = better outcome",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

## Moderators

### Scale of measurement

```{r r feels model pt}
levels(feels$outcome_scale_group)

feels %>% 
  group_by(outcome_scale_group) %>% 
  count()

feels_mod <- feels %>% 
  filter(outcome_scale_group != 16) %>% 
  unique()

feels_mod$outcome_scale_group <- droplevels(feels_mod$outcome_scale_group)

#model with interaction
feels_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = feels_mod)

feels_model_mod1

get_I2_overall(feels_mod, feels_model_mod1)

#model with no interaction for emmeans object
feels_model_mod1_noint <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison + outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = feels_mod)

feels_model_mod1_noint 
```

Scale group 10 (Empathy) is the reference group. Dropped scale group 16 since there was only one study in it. The analysis dropped redundant comparisons in the interaction contrasts (only ran one interaction estimate for scale group 13 and 15). There were no significant interactions (so one instruction did not work better with one scale than another in comparison to intercept). Scale of measurement only lowered the I2 by 2%. 

```{r emmeans feels scale}
#Emmeans could not run on the interaction model because of the redundancies that were dropped, so re-ran the model without the interactions since none were significant
qdrq_feels2 <- qdrg(object = feels_model_mod1_noint, data = feels_mod)

emmeans_feels2 <- summary(emmeans(qdrq_feels2, "pt_comparison", "outcome_scale_group"))
emmeans_feels2
```

### Target - amount of information

```{r}
feels_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod2

get_I2_overall(feels_mod, feels_model_mod2)
```

None are significant. Emmeans won't work because redundant predictors were dropped from ram.mv model.

### Target - In need/In distress target grouping (relevant to outcome category)

```{r}
feels_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_emapthetic_need,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod3

get_I2_overall(feels_mod, feels_model_mod3)
```

```{r emmeans feels empathetic need targ mod}
#Emmeans could not run on the interaction model because of the redundancies that were dropped, so re-ran the model without the interactions since none were significant
qdrq_feels3 <- qdrg(object = feels_model_mod3, data = feels_mod)

emmeans_feels3 <- summary(emmeans(qdrq_feels3, "pt_comparison", "target_emapthetic_need"))
emmeans_feels3
```

### Sample size

```{r sample size mod overlap}
feels_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod4

get_I2_overall(feels_mod, feels_model_mod4)
get_I2_var_levels(feels_mod, feels_model_mod4)
```

Intercept different from 0. No interaction with sample size.
