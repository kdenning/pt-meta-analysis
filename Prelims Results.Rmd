---
title: "Multivariate Results"
author: "Kathryn Denning"
date: "1/14/2021"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(dmetar)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("meta_functions.R")
```

```{r data cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Other data prep
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(coded_cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi, d_values)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  dplyr::select(-c(name, value)) %>% 
  pivot_longer(c(coded_cohens_d_var, var_d.x, var_d.y, vi, d_values_var)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  dplyr::select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  dplyr::select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within),
         target_information = as.factor(target_information))
```

# Descriptive information about included studies
## Number of papers

```{r descriptive info before collapsing conditions}
meta_data %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()
```

## Number of studies per paper

```{r}
meta_data %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()
```

## Number of unique samples

```{r}
meta_data %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

## Unique effect sizes total

```{r}
meta_data %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()
```

## Effect sizes per outcome category

```{r}
meta_data %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

## Checking if we need to collapse perspective taking conditions

### Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

### Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

### Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Multivariate Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

```{r effect size overall mod}
# Effect sizes in overall model
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()

# Effect sizes per comparison and stereotyping category
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```


## Multivariate model results

**Contrasts:**

```{r multi contrasts}
contrasts(meta_overall1$outcome_type)
```

```{r overall meta interaction mod}
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_overall1)

meta_multi2
```

### I2 overall

```{r overall I2 multi}
get_I2_overall(meta_overall1, meta_multi2)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 multi}
get_I2_var_levels(meta_overall1, meta_multi2)
```

The intercept is when both moderators = 0 (When the pt is compared to the objective condition and interpersonal feels is the reference). We are particularly interested in the interaction and estimates. We used interpersonal feels as the reference because it is the the most studied PT sub-field - and the one in which a different meta-analysis already exists - and we are curious if the effect of perspective taking is the same across outcome types or differs. We used pt vs objective because, when combined with interpersonal feels, we would expect the largest effect in that cell. Therefore, we are interested if perspective taking in other outcome types follows this same pattern or differs.

The "pt_comparisonpt_v_control" contrast supports the theory that I just described. In comparison to the intercept - which represents the effect of pt vs objective in the interpersonal context - the objective control condition shows a decrease of effect size (this is with interpersonal feels context held constant). With pt v objective held constant and just changing outcome, the overlap outcome did not differ significantly from interpersonal feels, however, the stereotyping outcome was significantly lower. 

When we look at the interaction we see there is only one significant effect for the contrast. This is not as intuitively interpretable as the main effects. When we vary up both perspective taking and outcome in the fifth estimate (estimate "pt_comparisonpt_v_control:outcome_typeOverlap") looking at pt_v_objective in the overlap condition versus pt_v_control in interpersonal feels condition, we see there there is not a significant difference from the intercept. We do see a significant change in which the slope of the contrast demonstrates a positive increase between the pt_v_control and stereotyping condition versus the intercept.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition). The effect sizes for the interaction relate to the contrast, but cannot be used to find the effect.

To get the effect sizes for each PT comparison in each outcome that would correspond to those pairwise comparisons, we need to run individual meta-analyses on the subsets of the data-set (as seen in McAuliffe meta-analysis).

## Output from individual models to obtain estimates per comparison:

```{r meta models per multi subsets with I2}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
pt_filter1 <- c("pt_v_control", "pt_v_control", "pt_v_control", 
                  "pt_v_objective", "pt_v_objective", "pt_v_objective") 

outcome_filter1 <- c("Stereotyping", "Overlap", "Interpersonal Feels",
                     "Stereotyping", "Overlap", "Interpersonal Feels")

## Get an empty list the length of the number of subsetted datasets we need
df_4_overall <- list(length = length(pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(pt_filter1)){
  df_4_overall[[i]] <- meta_overall1 %>%
    filter(pt_comparison == pt_filter1[i] & 
             outcome_type == outcome_filter1[i])
  }

# Mapping over each subsetted dataset to run the meta-analysis
meta_output <- map(df_4_overall, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_multi <- get_I2_overall_clean(df_4_overall, meta_output)

# I2 value per level of nested and crossed variance
I2_var_levels <-map2(df_4_overall, meta_output, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_multi <- get_btw_study_I2(I2_var_levels)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_multi <- get_within_study_I2(I2_var_levels)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_multi <- get_crossed_study_I2(I2_var_levels) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k <- get_k_clean(meta_output)
estimates <- get_ests_clean(meta_output)
pval <- get_pval_clean(meta_output)
ci_upper <- get_ci_upper_clean(meta_output)
ci_lower <- get_ci_lower_clean(meta_output)
Q_val <- get_Q_clean(meta_output)
Q_pval <- get_Q_p_clean(meta_output)

# Creating the labels for the dataframe
comparison <- c("Pt vs Control in Stereotyping",
                "PT vs Control in Overlap",
                "PT vs Control in Interpersonal feelings",
                "PT vs Objective in Stereotyping",
                "PT vs Objective in Overlap",
                "PT vs Objective in Interpersonal feelings")

# Combinind everything into one dataframe
multi_combined <- cbind(comparison, 
                        k,
                        estimates,
                        pval,
                        ci_upper,
                        ci_lower,
                        Q_val,
                        Q_pval,
                        I2_overall_multi,
                        btw_I2_multi,
                        within_I2_multi,
                        crossed_I2_multi)

knitr::kable(multi_combined)
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .58). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control, but again, we have more results included, and we are still in the same direction. 

*Note for Sara H: This change from before for imagine-other vs no-instructions in the interpersonal feels category is due to me adding three effect sizes converted from correlation coefficients that I had previously forgot to convert... I guess they were outliers, since it increased from .16 to .27*


For the categories other than Interpersonal Feels, only the comparison of pt_v_control is statistically different from 0 for the Overlap/merging category. This is surprising and opposite of the pattern found in the interpersonal feelings category. The comparison of pt_v_objective is marginal, though the estimate itself is not much smaller. This is the category in which the number of studies included is small, which could be impacting these results. We will have to see if these differ from each other when analyzed in their own model.


Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

# Stereotyping

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
meta_stereo1 %<>% 
  filter(outcome_type == "1")

## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" |
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")
```

## Number of unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

## Model looking at imagine-self vs imagine-other

**Contrasts:**

```{r stereo1 conrasts}
contrasts(meta_stereo1$pt_comparison) 
```

```{r stereo model imagine self vs other}
# OVerall model across outcome type
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1
```

### I2 overall

```{r overall I2 stereo 1}
get_I2_overall(meta_stereo1, stereo_meta1)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 1}
get_I2_var_levels(meta_stereo1, stereo_meta1)
```

The test of moderators is not significant, nor are any comparisons. The estimates do not provide us with estimates for each comparison, we will need to run individual models to examine this.

### Output from individual models to obtain estimates per comparison:

```{r meta models for stereo 1 subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
stereo_pt_filter1 <- c("self_v_objective", "self_v_control", 
                      "other_v_objective", "other_v_control") 


## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo1 <- list(length = length(stereo_pt_filter1))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter1)){ # change () in seq_along to filters
  df_4_stereo1[[i]] <- meta_stereo1 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter1[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output1 <- map(df_4_stereo1, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_stereo1 <- get_I2_overall_clean(df_4_stereo1, stereosub_output1)

# I2 value per level of nested and crossed variance
I2_var_levels_stereo1 <-map2(df_4_stereo1, stereosub_output1, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_stereo1 <- get_btw_study_I2(I2_var_levels_stereo1)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_stereo1 <- get_within_study_I2(I2_var_levels_stereo1)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_stereo1 <- get_crossed_study_I2(I2_var_levels_stereo1) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_stereo1 <- get_k_clean(stereosub_output1)
estimates_stereo1 <- get_ests_clean(stereosub_output1)
pval_stereo1 <- get_pval_clean(stereosub_output1)
ci_upper_stereo1 <- get_ci_upper_clean(stereosub_output1)
ci_lower_stereo1 <- get_ci_lower_clean(stereosub_output1)
Q_val_stereo1 <- get_Q_clean(stereosub_output1)
Q_pval_stereo1 <- get_Q_p_clean(stereosub_output1)

# Creating the labels for the dataframe
comparison_stereo1 <- c("self_v_objective", "self_v_control",
                "other_v_objective", "other_v_control") 

# Combinind everything into one dataframe
stereo1_combined <- cbind(comparison_stereo1,
                          k_stereo1,
                          estimates_stereo1,
                          pval_stereo1,
                          ci_upper_stereo1,
                          ci_lower_stereo1,
                          Q_val_stereo1,
                          Q_pval_stereo1,
                          I2_overall_stereo1,
                          btw_I2_stereo1,
                          within_I2_stereo1,
                          crossed_I2_stereo1)

knitr::kable(stereo1_combined)
```

## Model comparing "day in the life," other control, and objective comparisons

**Contrasts:**

```{r stereo2 contrasts}
contrasts(meta_stereo2$pt_comparison) 
```

```{r stereo model day in the life vs other control vs objective}
# OVerall model across outcome type
stereo_meta2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2
```

### Overall I2

```{r overall I2 stereo 2}
get_I2_overall(meta_stereo2, stereo_meta2)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 2}
get_I2_var_levels(meta_stereo2, stereo_meta2)
```

Pairwise comparisons to compare:

* pt_v_objective to pt_v_other control

```{r pairwise comparisons stereo 2}
anova(stereo_meta2, btt=2:3)
```

They do not significantly differ.

### Output from individual models to obtain estimates per comparison:

```{r meta models for stereo 2 subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
stereo_pt_filter2 <- c("pt_v_day_control", "pt_v_other_control",
                       "pt_v_objective") 

## Get an empty list the length of the number of subsetted datasets we need
df_4_stereo2 <- list(length = length(stereo_pt_filter2))

## The for loop getting us subsetted datasets
for (i in seq_along(stereo_pt_filter2)){ # change () in seq_along to filters
  df_4_stereo2[[i]] <- meta_stereo2 %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == stereo_pt_filter2[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
stereosub_output2 <- map(df_4_stereo2, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_stereo2 <- get_I2_overall_clean(df_4_stereo2, stereosub_output2)

# I2 value per level of nested and crossed variance
I2_var_levels_stereo2 <-map2(df_4_stereo2, stereosub_output2, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_stereo2 <- get_btw_study_I2(I2_var_levels_stereo2)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_stereo2 <- get_within_study_I2(I2_var_levels_stereo2)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_stereo2 <- get_crossed_study_I2(I2_var_levels_stereo2) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_stereo2 <- get_k_clean(stereosub_output2)
estimates_stereo2 <- get_ests_clean(stereosub_output2)
pval_stereo2 <- get_pval_clean(stereosub_output2)
ci_upper_stereo2 <- get_ci_upper_clean(stereosub_output2)
ci_lower_stereo2 <- get_ci_lower_clean(stereosub_output2)
Q_val_stereo2 <- get_Q_clean(stereosub_output2)
Q_pval_stereo2 <- get_Q_p_clean(stereosub_output2)

# Creating the labels for the dataframe
comparison_stereo2 <- c("pt_v_day_control", "pt_v_other_control",
                       "pt_v_objective") 

# Combinind everything into one dataframe
stereo2_combined <- cbind(comparison_stereo2,
                         k_stereo2,
                         estimates_stereo2,
                         pval_stereo2,
                         ci_upper_stereo2,
                         ci_lower_stereo2,
                         Q_val_stereo2,
                         Q_pval_stereo2,
                         I2_overall_stereo2,
                         btw_I2_stereo2,
                         within_I2_stereo2,
                         crossed_I2_stereo2)

knitr::kable(stereo2_combined)
```

# Overlap

```{r overlap meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
overlap_data <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

overlap_data %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control")) %>% 
  filter(outcome_type == "Overlap")
```

## Number of overall effects:

```{r effects overlap data}
overlap_data %>% 
  count()
```

## Effects per comparison group:

```{r effects pt overlap data}
overlap_data %>% 
  group_by(pt_comparison) %>% 
  count()
```

## Comparing PT comparisons in one model without other outcomes:

**Contrasts:**

```{r overlap contrasts}
contrasts(overlap_data$pt_comparison)
```

```{r overlap model pt}
overlap_pt <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = overlap_data)

overlap_pt
```

### I2 overall model

```{r I2 overall overlap}
get_I2_var_levels(overlap_data, overlap_pt)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels overlap}
get_I2_overall(overlap_data, overlap_pt)
```

Since this comparison is the same as the overall multivariate model, we already have the subsetted dataset results per each comparison.

# Interpersonal Feelings

```{r}
feels <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "6"),# there are no day in the life controls in the interpersonal feels category
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
feels %<>% 
  filter(outcome_type == "3")

```

## Number of overall effects:

```{r number of effect sizes feels}
feels %>% 
  count()
```

## Effects per comparison group:

```{r effects pt feels data}
feels %>% 
  group_by(pt_comparison) %>% 
  count()
```

19 is one under our limit of 20, so this will need to be taken as exploratory for the purposes of this preliminary analysis for my prelims.

## Model results

**Contrasts:**

```{r feels pt contrasts}
contrasts(feels$pt_comparison)
```

```{r r feels model pt}
feels_model <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = feels)

feels_model
```

### Overall I2 for model

```{r overall I2 feels}
get_I2_var_levels(feels, feels_model)
```

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 feels}
get_I2_overall(feels, feels_model)
```

### Output from individual models to obtain estimates per comparison:

```{r meta models for feels subsets}
# Get subsets of the overall dataset corresponding to each comparison & run meta-analyses on each
## Get labels for the function
comparisons_feels <- c("self_v_objective", "self_v_control", 
                      "other_v_objective", "other_v_control") 


## Get an empty list the length of the number of subsetted datasets we need
df_4_feels <- list(length = length(comparisons_feels))

## The for loop getting us subsetted datasets
for (i in seq_along(comparisons_feels)){ # change () in seq_along to filters
  df_4_feels[[i]] <- feels %>%  # change data that is subsetted [[i]] to the empty list above & change the piped data to the correct cleaned dataset for this analysis
    filter(pt_comparison == comparisons_feels[i]) # change () in seq_along to filters
  }

# Mapping over each subsetted dataset to run the meta-analysis
feels_sub_output <- map(df_4_feels, ~rma.mv(dunb,
                         var_dunb,
                         random = list( ~ 1 | sample_number_total/outcomes_within_sample_var,
                                        ~ 1 | outcome_scale_group),
                         data = .x))

# Overall I2 value for model
I2_overall_feels <- get_I2_overall_clean(df_4_feels, feels_sub_output)

# I2 value per level of nested and crossed variance
I2_var_levels_feels <-map2(df_4_feels, feels_sub_output, ~get_I2_var_levels(.x, .y))

### Get column for between study hetereogeneity to put in table
btw_I2_feels <- get_btw_study_I2(I2_var_levels_feels)

### Getting values for between outcome hetereogeneity (within outcome) for dataframe
within_I2_feels <- get_within_study_I2(I2_var_levels_feels)

### Getting values for crossed scale hetereogeneity into dataframe column to put in table later
crossed_I2_feels <- get_crossed_study_I2(I2_var_levels_feels) 

#Using extraction and cleaning functions I wrote, extract and clean data from model output
k_feels <- get_k_clean(feels_sub_output)
estimates_feels <- get_ests_clean(feels_sub_output)
pval_feels <- get_pval_clean(feels_sub_output)
ci_upper_feels <- get_ci_upper_clean(feels_sub_output)
ci_lower_feels <- get_ci_lower_clean(feels_sub_output)
Q_val_feels <- get_Q_clean(feels_sub_output)
Q_pval_feels <- get_Q_p_clean(feels_sub_output)

# Combining everything into one dataframe
feels_combined <- cbind(comparisons_feels,
                        k_feels,
                        estimates_feels,
                        pval_feels,
                        ci_upper_feels,
                        ci_lower_feels,
                        Q_val_feels,
                        Q_pval_feels,
                        I2_overall_feels,
                        btw_I2_feels,
                        within_I2_feels,
                        crossed_I2_feels)

knitr::kable(feels_combined)
```
