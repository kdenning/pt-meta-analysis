---
title: "Multivariate Results"
author: "Kathryn Denning"
date: "1/14/2021"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("emmeans")
#install.packages("metafor")
#install.packages("Cairo")
#install.packages("tinytex")
#install.packages("compute.es")
#install.packages("multcomp")
#install.packages("car")
#install.packages("emmeans")
#install.packages("metapower")
library(rio)
library(here)
library(magrittr)
library(janitor)
library(lme4)
library(emmeans)
library(Cairo)
library(tinytex)
library(metafor)
library(compute.es)
library(multcomp)
library(car)
library(tidyverse)
library(emmeans)
library(metapower)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Windows messes up my plots, these lines of code make them clearer
knitr::opts_knit$set(dev.args = list(type = "cairo"))

trace(grDevices:::png, quote({
  if (missing(type) && missing(antialias)) {
    type <- "cairo-png"
    antialias <- "subpixel"
  }
}), print = FALSE)

options(scipen=999)

# importing data
converted_data <- import("converted_data.csv") 

# Importing functions I wrote from R document in this project
source("meta_functions.R")
```

```{r data cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# data prep
meta_data <- converted_data %>%  
  mutate(cohens_d = as.numeric(cohens_d),
         reverse = as.factor(reverse)) %>% 
  mutate(coded_cohens_d_var = (((n_pt + n_comparison)/(n_pt*n_comparison)) + ((cohens_d^2)/(2*(n_pt+n_comparison))))) %>% #calculated variance by hand for the d's we extracted from articles directly; the ones we converted also came with var calculations
  pivot_longer(c(cohens_d, d.x, d.y, yi, d_values)) %>% #  Wrangling all converted d's into one column
  mutate(d = value) %>% 
  filter(!is.na(d)) %>% 
  dplyr::select(-c(name, value)) %>% 
  pivot_longer(c(coded_cohens_d_var, var_d.x, var_d.y, vi, d_values_var)) %>% #wrangling all variances into one column
  mutate(var = value) %>% 
  filter(!is.na(var)) %>% 
  dplyr::select(-c(name, value)) %>% 
  mutate(df = (n_pt + n_comparison - 2), #getting degrees of freedom
         J = 1- (3/((4*df)-1)), #calculating hedges correction factor for d unbiased
         dunb = J*d, #d unbiased
         var_dunb = ((J^2)*var),  #variance for d unbiased
         lowCI_dunb = dunb-1.96*sqrt(var_dunb), #getting 95% CI's for d unbiased
         upCI_dunb  = dunb+1.96*sqrt(var_dunb)) %>% 
  mutate(dunb = ifelse(reverse == "yes", 
                       dunb*-1,
                       ifelse(reverse == "no",
                              dunb*1, NA))) %>% #reverse scored dunb that needed to be 
  dplyr::select(-c(DOI, Notes, outcome_description_from_coding, target_long_description,
            weird_sample, F_score, t_score, effect_size_type,
            effect_direction, p_value, mean_pt, sd_pt, se_pt, 
            mean_comparison, sd_comparison,se_comparison, 
            `Note about directionality of cohen's d`)) %>% 
  mutate(sample_number_total = as.factor(sample_number_total),
         pt_comparison = as.factor(pt_comparison),
         outcome_type = as.factor(outcome_type),
         target_ingroup_nonspecific = as.factor(target_ingroup_nonspecific),
         outcome_scale_group = as.factor(outcome_scale_group),
         target_out_minor = as.factor(target_out_minor),
         target_adversary = as.factor(target_adversary),
         target_emapthetic_need = as.factor(target_empathetic_need),
         between_within = as.factor(between_within),
         target_information = as.factor(target_information))
```

# Checking if conditions need collapsed

## Stereotyping outcome

```{r k per condition stereotyping}
# Number of effect sizes per comparison we coded 
k_per_comparison <- meta_data %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() %>% mutate(pt_comparison = dplyr::recode(pt_comparison,
                                `1` = "Self PT vs Day control",
                                `2` = "Self PT vs Other control",
                                `3` = "Self PT vs Objective",
                                `4` = "Self PT vs Suppression",
                                `5` = "Other PT vs Day control",
                                `6` = "Other PT vs Other control",
                                `7` = "Other PT vs Objective",
                                `9` = "Other PT vs Self PT",
                                `10` = "PT US vs Day control",
                                `11` = "PT US vs other control",
                                `12` = "PT US vs Objective",
                                `13` = "PT US vs Suppression"))

# For stereotyping outcome
k_per_comparison %>% 
  filter(outcome_type == 1)
```

Our pre-registered rule was that there must be k = 20 per cell. As this does not apply to every relevant cell, we will need to collapse until we reach this. Also, for the prelims analysis, we will not be adding in perspective taking unspecified (PTUS), but will be adding that to relevant comparisons to see if it affects the analyses for published versions. Also, for all analyses in addition to the meta-analyses specified below, we will conduct moderator analyses with sample size and target information.

For stereotyping, we will do the following:

* Drop any comparison including suppression, as there are not enough even when collapsed (k = 3).
* We will fun analyses using two of the following comparisons to get at different theoretical comparisons:
    + We will collapse across the "Day in the life" and "Other" control conditions. This will allow us to analyze Imagine-self versus Imagine-other perspective taking in comparison to a broad control and the objective comparison condition. 
    + We will then collapse across Imagine-self and Imagine-other to examine the if there is difference between the effect of perspective taking instructions when the comparison is the "Day in the life control" versus "Other control" or objective comparison.

## Merging/Overlap outcome

```{r k per condition overlap}
# For overlap/merging outcome
k_per_comparison %>% 
  filter(outcome_type == 2)
```

Due to the lower number of studies in this outcome category, we will have to collapse across both Imagine-self and Imagine-other perspective taking instructions as well as "Day in the life" and "Other control" comparisons. As this is the most limiting of the outcomes, we will have to collapse this same way in all outcome categories for the overall meta-analysis when we will include outcome as a predictor.

## Interpersonal feelings outcome

```{r k per condition interpersonal feels}
# For interpersonal feelings
k_per_comparison %>% 
  filter(outcome_type == 3)
```

We will have to collapse across across "day in the life" and "other" control conditions. As our "Self PT versus Objective" is only one study under our pre-registered limit - and this is a preliminary analysis for a doctoral requirement - we will include that category for this analysis with the expectation that when we include regression coefficients and data from contacted authors, it will be over our limit (spoiler alert: it will). Therefore, for the analysis specifically examining interpersonal feelings outcomes, we will not collapse across Imagine-self and Imagine-other instruction comparisons. 

# Overall Multivariate Meta-analysis

```{r overall meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
meta_overall1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)),
         dunb = ifelse(outcome_type == "Stereotyping",
                       dunb*-1,
                       ifelse(outcome_type != "Stereotyping",
                              dunb*1, NA))) %>% 
  mutate(outcome_type = fct_relevel(outcome_type, "Interpersonal Feels",
                                    "Overlap", "Stereotyping"))

meta_overall1 %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control"))

#for this overall model, needed to reverse score the stereotyping outcome again; the initial reverse score had made higher scores on the scale = more stereotyping (the more popular direction of research); the other outcome categories meant higher scores = more positive outcomes. For the same analysis, they should all be going in the same direction.

# WILL WANT TO VERIFY REVERSE SCORING BEFORE PUBLICATION, AS VERY CRUCIAL
```

## Descriptive info

### Number of papers

```{r descriptive info before collapsing conditions}
meta_overall1 %>% 
  mutate(authors = as.factor(authors)) %>% 
  dplyr::select(authors) %>% 
  unique() %>% 
  count()
```

### Number of studies per paper

```{r}
meta_overall1 %>% 
  dplyr::select(study_num_total) %>% 
  unique() %>% 
  count()
```

### Number of unique samples

```{r}
meta_overall1 %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```


### Unique effect sizes

```{r effect size overall mod}
meta_overall1 %>% 
  dplyr::select(effect_size_num) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell multi}
meta_overall1 %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

### Effect sizes per outcome category

```{r}
meta_overall1 %>% 
  dplyr::select(dunb, outcome_type) %>% 
  unique() %>% 
  group_by(outcome_type) %>% 
  count() %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `1` = "Stereotyping/Bias",
                               `2` = "Overlap/Merging",
                               `3` = "Interpersonal Feelings"))
```

### Effect sizes per comparison and outcome category

```{r effect sizes overall model per outcome}
meta_overall1 %>% 
  dplyr::select(dunb, pt_comparison, outcome_type) %>% 
  unique() %>% 
  group_by(pt_comparison, outcome_type) %>% 
  count() 
```

## Model results

**Contrasts:**

```{r multi contrasts}
contrasts(meta_overall1$outcome_type)
```

```{r overall meta interaction mod}
meta_multi2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_type,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_overall1)

meta_multi2
```

The intercept is when When pt is compared to the objective condition and interpersonal feels is the outcome. When looking at the main effect, this indicates that - in the interpersonal feeling category - the pt v control condition leads to significantly lower effect sizes the pt vs objective. When comparing pt v objective, there is no difference between the interpersonal feels and overlap conditions, but the stereotyping condition is significantly less than the interpersonal feels condition. The estimates representing the contrasts signify the "difference between the differences" (e.g., how large is the difference between the difference between pt comparison levels and outcome levels being compared in this contrast?). This model demonstrates this difference of differences is significant when comparing the stereotyping outcome and pt v control to the interpersonal feels outcome when pt v objective, but only marginal when comparing pt v control in the overlap outcome to the same reference group.

We then ran pairwise comparisons to know the following differences:

* pt_v_control:overlap vs pt_v_control_stereotyping

```{r pairwise comparisons overall meta}
anova(meta_multi2, btt=5:6)
```

The two contrasts are significantly different.

The interactions in the multivariate overall model tells us there are statistical differences between outcome context and pt condition, but does not tell us the effect sizes per cell (each outcome and pt condition) and if they differ from 0. These were obtained using emmeans.

## Marginal main effects

```{r emmeans multi}
qdrq1 <- qdrg(object = meta_multi2, data = meta_overall1)

emmeans_multi <- summary(emmeans(qdrq1, "pt_comparison","outcome_type"))
emmeans_multi

emmeans_multi %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  facet_grid(cols = vars(outcome_type)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Interaction effect on perspective taking comparison effects",
       subtitle = "Higher effect sizes = more positive outcomes",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs Objective','Vs Control'))
```

The interpersonal feelings outcome results are consistent with McAuliffe's findings for empathy/empathic concern (they found .68 for Imagine-other and .56 for Imagine-self vs objective, and we are collapsed across here and with other effects to a .54). However, they found a lower average effect for Imagine-other vs no-instructions (.08) than our comparison versus the control (.29), but again, we have more results included. We also obtained our results in the interactive MLM model, not individual meta-analysis models as McAuliffe et al did. MLM models employ shrinkage on the estimates that would not occur when subsetting data down to the individual comparisons, which also accounts for differences between our results and McAuliffe results. This also allows us to retain a larger k.

For the categories other than Interpersonal Feels, none of the other comparisons are statistically different from 0. Because of the significant heterogeneity that is moderated in our overall model, we are going to skip other moderators at the overall model level to explore models within each outcome more deeply. Clearly, at this higher-order level, there is too much variation between the studies for them to be compared as the same thing.

### I2 overall

```{r overall I2 multi}
get_I2_overall(meta_overall1, meta_multi2)
```

Overall heterogeneity is very high. 91.13% of the variance can be explained due to differences in the studies not explained by our predictors. 

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 multi}
get_I2_var_levels(meta_overall1, meta_multi2)
```

Most of this heterogeneity is explained by between-study variance, followed by between-outcome (within-study) variance. 

### Post-hoc Power

* The effect sizes are obtained from the marginal means for each of the cells
* Sample size is the average found earlier (n = 59) to the nearest number the r function would take
* k is using all effect sizes coded (375)

```{r power multi}
subgroup_power(n_groups = 6, 
               effect_sizes = c(.54, .30, .34, .30, -.07, -.01), 
               sample_size = 120,
               k = 375,
               es_type = "d")
```

# Stereotyping

```{r stereo model data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
## creating dataset collapsing across "day in the life" versus "other control"
meta_stereo1 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "5", "6"),
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
meta_stereo1 %<>% 
  filter(outcome_type == "1")

meta_stereo1$outcome_scale_group <- droplevels(meta_stereo1$outcome_scale_group)

## creating dataset collapsing across imagine-self and other to examine "day in the life" versus "other control"
meta_stereo2 <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison,
                                      pt_v_day_control = c("1", "5"),
                                      pt_v_other_control = c("2", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_day_control" |
           pt_comparison == "pt_v_other_control" |
           pt_comparison == "pt_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

meta_stereo2 %<>% 
  filter(outcome_type == "1")

meta_stereo2$outcome_scale_group <- droplevels(meta_stereo2$outcome_scale_group)
```

## Unique effect sizes

```{r stereo effect size num}
meta_stereo1 %>% 
  count()
```

### Number of unique samples

```{r}
meta_stereo1 %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell stereo}
meta_stereo1 %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

## Imagine-self vs imagine-other Model

**Contrasts:**

```{r stereo1 conrasts}
contrasts(meta_stereo1$pt_comparison) 
```

```{r stereo model imagine self vs other}
stereo_meta1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1
```

The intercept is not significantly different from zero, nor do any effects differ from it.

### Marginal main effects

```{r emmeans stereo1}
qdrq_stereo1 <- qdrg(object = stereo_meta1, data = meta_stereo1)

emmeans_stereo1 <- summary(emmeans(qdrq_stereo1, "pt_comparison"))
emmeans_stereo1

emmeans_stereo1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in stereotyping only",
       subtitle = "Lower effect sizes = more prejudice reduction (better)",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

None of the marginal main effects differ from zero. There is an interesting pattern in that self or other vs objective lead to higher effect sizes (less stereotype prejudice reduction). However, with nothing differing from zero, this does not mean anything statistically.

### I2 overall

```{r overall I2 stereo 1}
get_I2_overall(meta_stereo1, stereo_meta1)
```

Again, very high.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 1}
get_I2_var_levels(meta_stereo1, stereo_meta1)
```

Most of the heterogeneity is explained by between-studies variance, with about 16% also explained by between-conditions (within-study) variance. This differed from the heterogeneity pattern found in the overall interaction model.

### Post-hoc power

* k = 138 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power stereo 1}
subgroup_power(n_groups = 4, 
               effect_sizes = c(.08, -.08, .02, -.11), 
               sample_size = 100,
               k = 138,
               es_type = "d")
```

## Moderators for imagine self vs other model

### Scales of measurement

```{r stereo 1 mod1}
meta_stereo1 %>% 
  group_by(pt_comparison, outcome_scale_group) %>% 
  count() %>% 
  filter(n <2) 

#getting rid of cells with only 1
meta_stereo1_mod1_dat <- meta_stereo1 %>% 
  filter(!c(pt_comparison == "self_v_objective" & outcome_scale_group == "1" |
           pt_comparison == "self_v_control" & outcome_scale_group == "1"))

stereo_meta1_mod1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), data = meta_stereo1_mod1_dat)

stereo_meta1_mod1

```

I dropped cells with only 1 study and rma.mv dropped cells with 0. Nothing is significant from one another, and the intercept is not significantly different from 0. Any model with redudancies (0's in cells) cannot be modeled using emmeans to get marginal means.

#### I2 overall

```{r I2 overall stereo1 mod scales}
get_I2_overall(meta_stereo1_mod1_dat, stereo_meta1_mod1)
```

Still very high, but slightly lower than without the moderater.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod scales}
get_I2_var_levels(meta_stereo1_mod1_dat, stereo_meta1_mod1)
```

Without the crossed variance structure (because it is a moderator), more of that variance shifted to the between-conditions level. Most of the variance is still represented by the between-studies level.

### Target moderator - information amount

```{r stereo 1 mod2}
stereo_meta1_mod2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1_mod2
```

Nothing is significant.

```{r emmeans stereo1 mod2 }
qdrq_stereo1_mod2 <- qdrg(object = stereo_meta1_mod2, data = meta_stereo1)

emmeans_stereo1_mod2 <- summary(emmeans(qdrq_stereo1_mod2, "pt_comparison", "target_information"))
emmeans_stereo1_mod2
```

None of the effect sizes significantly differ from 0 when looking at the marginal main effects when broken down by target information.

#### I2 overall

```{r I2 overall stereo1 mod2}
get_I2_overall(meta_stereo1, stereo_meta1_mod2)
```

Compared to the original stereotyping model this is moderator, the overall heterogeneity is slightly higher.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod2}
get_I2_var_levels(meta_stereo1, stereo_meta1_mod2)
```

Most of the variance is between-studies, with a somewhat significant amount explained by between-outcome (within-study) variance.

### Target moderator - target out-group

```{r stereo 1 mod3}
stereo1_group_data <- meta_stereo1 %>% 
  select(dunb, var_dunb, pt_comparison, target_out_minor, sample_number_total,
         outcomes_within_sample_var, conditions_within_sample_var, outcome_scale_group) %>% 
  na.omit()

stereo_meta1_mod3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = stereo1_group_data)

stereo_meta1_mod3
```

Nothing is significant.

```{r emmeans stereo1 mod3}
qdrq_stereo1_mod3 <- qdrg(object = stereo_meta1_mod3, data = stereo1_group_data)

emmeans_stereo1_mod3 <- summary(emmeans(qdrq_stereo1_mod3, "pt_comparison", "target_out_minor"))
emmeans_stereo1_mod3
```

None of the effect sizes significantly differ from 0 when looking at the marginal main effects when broken down by by target group.

#### I2 overall

```{r I2 overall stereo1 mod3}
get_I2_overall(stereo1_group_data, stereo_meta1_mod3)
```

About the same as the original model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod3}
get_I2_var_levels(stereo1_group_data, stereo_meta1_mod3)
```

Most of the variance is explained by between-studies or between-conditions (within-study) variance. Interestingly, there is 0% variance explained by between-outcomes variance.

### Sample size

```{r stereo 1 mod4}
stereo_meta1_mod4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo1)

stereo_meta1_mod4
```

Sample size does not interact with pt comparison.

#### I2 overall

```{r I2 overall stereo1 mod4}
get_I2_overall(meta_stereo1, stereo_meta1_mod4)
```

Heterogeneity is about the same as the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere1 mod4}
get_I2_var_levels(meta_stereo1, stereo_meta1_mod4)
```

Most variance is explained by between-studies variance, followed by between-outcomes (within-study) variance.

## Model comparing "day in the life," other control, and objective comparisons

**Contrasts:**

```{r stereo2 contrasts}
contrasts(meta_stereo2$pt_comparison) 
```

```{r stereo model day in the life vs other control vs objective}
# OVerall model across outcome type
stereo_meta2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2
```

Pairwise comparisons to compare:

* pt_v_objective to pt_v_other control

```{r pairwise comparisons stereo 2}
anova(stereo_meta2, btt=2:3)
```

They do not significantly differ.

### Marginal main effects

```{r emmeans stereo2}
qdrq_stereo2 <- qdrg(object = stereo_meta2, data = meta_stereo2)

emmeans_stereo2 <- summary(emmeans(qdrq_stereo2, "pt_comparison"))
emmeans_stereo2

emmeans_stereo2 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "'Day in the life' control vs other comparison conditions in stereotyping only",
       subtitle = "Lower effect sizes = more prejudice reduction (better)",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs `Day in the life` control','Vs other control',
                              'Vs objective'))
```

The marginal main effects do not differ from 0.

### Overall I2

```{r overall I2 stereo 2}
get_I2_overall(meta_stereo2, stereo_meta2)
```

Heterogeneity is high.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 stereo 2}
get_I2_var_levels(meta_stereo2, stereo_meta2)
```

Most hetereogeneity is explained by between-study variance, followed by between-condition variance.

### Post-hoc power

* k = 138 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power stereo 2}
subgroup_power(n_groups = 3, 
               effect_sizes = c(.00, -.03, .10), 
               sample_size = 99,
               k = 138,
               es_type = "d")
```

## Moderators for "day in the life" model

### Scales of measurement

```{r stereo 2 mod1}
stereo_meta2_mod1 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), data = meta_stereo2)

stereo_meta2_mod1
```

Nothing is significant from one another, and the intercept is not significantly different from 0. Outcome scale group 5 (Essay stereotypicality) is marginally different from the intercept outcome scale group 1 (IAT). Rma.mv dropped cells with 0 due to redundancies, and therefore cannot be modeled in emmeans.

#### I2 overall

```{r I2 overall stereo2 mod scales}
get_I2_overall(meta_stereo2, stereo_meta2_mod1)
```

About the same as the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stereo2 mod scales}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod1)
```

Most hetereogeneity is explained by between-studies variance, followed by between-outcomes (within-study) variance.

### Target moderator - information amount

```{r stereo 2 mod2}
stereo_meta2_mod2 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2_mod2
```

Pt v other control with medium target information is marginally different from the intercept (p vs "day  in the life" control with impoverished information). Nothing else is significant from the intercept, nor is the intercept significant.

#### Marginal main effect

```{r emmeans stereo2 mod2 }
qdrq_stereo2_mod2 <- qdrg(object = stereo_meta2_mod2, data = meta_stereo2)

emmeans_stereo2_mod2 <- summary(emmeans(qdrq_stereo2_mod2, "pt_comparison", "target_information"))
emmeans_stereo2_mod2
```

None of the effect sizes significantly differ from 0 when looking at the marginal means when broken down by target information.

#### I2 overall

```{r I2 overall stereo2 mod2}
get_I2_overall(meta_stereo2, stereo_meta2_mod2)
```

Slightly higher than the original model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stereo2 mod2}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod2)
```

Most hetereogeneity is explained by between-studies variance, followed by between-outcomes (within-study) variance.

### Target moderator - target out-group

```{r stereo 2 mod3}
stereo2_group_data <- meta_stereo2 %>% 
  select(dunb, var_dunb, pt_comparison, target_out_minor, sample_number_total,
         outcomes_within_sample_var, conditions_within_sample_var, outcome_scale_group) %>% 
  na.omit()

stereo_meta2_mod3 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*target_out_minor,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = stereo2_group_data)

stereo_meta2_mod3
```

The intercept (pt v day in the life control with out-group target) is significantly different from 0. Pt v control is marginally different from the intercept, and pt v objective is significantly different from the intercept. The simple main effect of target out-group group is significant: in the "day in the life" control, there are higher positive effect sizes (less prejudice reduction) for non-out-group/minority targets than out-group/minority targets. The interactions are also significant, indicating there are differences between the differences for both pt_v_control with a non-out-group minority target and p_v_objective with a non-out-group/minority target in comparison to the intercept. The marginal main effects will show the estimates of each of these comparisons better, as well as if they differ from 0.

#### Marginal main effects

```{r emmeans stereo2 mod3}
qdrq_stereo2_mod3 <- qdrg(object = stereo_meta2_mod3, data = stereo2_group_data)

emmeans_stereo2_mod3 <- summary(emmeans(qdrq_stereo2_mod3, "pt_comparison", "target_out_minor"))
emmeans_stereo2_mod3

outgroup_label <- c("1" = "Outgroup/Minority", "2" = "Non-Outgroup/Minority")

emmeans_stereo2_mod3 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  facet_grid(cols = vars(target_out_minor),
             labeller = labeller(target_out_minor = outgroup_label)) +
  geom_col() +
  theme_minimal() +
  labs(title = "'Day in the life' control vs other comparison conditions in stereotyping only",
       subtitle = "Lower effect sizes = more prejudice reduction (better)",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Vs `Day in the life`','Vs other control',
                              'Vs objective'))
```

#### I2 overall

```{r I2 overall stereo2 mod3}
get_I2_overall(stereo2_group_data, stereo_meta2_mod3)
```

About the same as the model without moderaters.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere2 mod3}
get_I2_var_levels(stereo2_group_data, stereo_meta2_mod3)
```

Most heterogeneity explained by between-studies variance, or between-conditions variance.

### Sample size

```{r stereo 2 mod4}
stereo_meta2_mod4 <- rma.mv(dunb, 
                      var_dunb, 
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), data = meta_stereo2)

stereo_meta2_mod4
```

Sample size does not interact with pt comparison. None of the effects are significant.

#### I2 overall

```{r I2 overall stereo2 mod4}
get_I2_overall(meta_stereo2, stereo_meta2_mod4)
```

Heterogeneity is the same as in the model without moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels stere2 mod4}
get_I2_var_levels(meta_stereo2, stereo_meta2_mod4)
```

Heterogeneity is explained by between-studies variance the most, followed almost equally by between-conditions and between-outcomes variance.

# Overlap

```{r overlap meta data prep, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
#collapsing across conditions for data-prep
overlap_data <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      pt_v_control = c("1", "2", "5", "6"),
                                      pt_v_objective = c("3", "7"))) %>% 
  filter(pt_comparison == "pt_v_control" | pt_comparison == "pt_v_objective") %>% 
  mutate(outcome_type = dplyr::recode(outcome_type,
                               `3` = "Interpersonal Feels",
                               `2` = "Overlap",
                               `1` = "Stereotyping")) %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison)))

overlap_data %<>% mutate(pt_comparison = fct_relevel(pt_comparison, "pt_v_objective",
                                     "pt_v_control")) %>% 
  filter(outcome_type == "Overlap")
```

## Number of overall effects:

```{r effects overlap data}
overlap_data %>% 
  count()
```

## Effects per comparison group:

```{r effects pt overlap data}
overlap_data %>% 
  group_by(pt_comparison) %>% 
  count()
```

### Number of unique samples

```{r overlap samples}
overlap_data %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell overlap}
overlap_data %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

## Results

*Note: this comparison is the same as in the overall model interacting with outcome types, as the small k per cell in the overlap/merging category was the reason we had to collapse cross other perspective taking comparisons in that overall model.*

**Contrasts:**

```{r overlap contrasts}
contrasts(overlap_data$pt_comparison)
```

```{r overlap model pt}
overlap_data$outcome_scale_group <- droplevels(overlap_data$outcome_scale_group)
levels(overlap_data$outcome_scale_group)

overlap_pt <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = overlap_data)

overlap_pt
```

The intercept is marginally different from zero. The pt v control condition does not differ significantly from the intercept.

### Marginal main effect

```{r emmeans overlap}
qdrq_overlap <- qdrg(object = overlap_pt, data = overlap_data)

emmeans_overlap <- summary(emmeans(qdrq_overlap, "pt_comparison"))
emmeans_overlap 
```

This comparison is the same as the overall model. Without the interaction, the values change slightly, but the pattern and significance do not.

### I2 overall model

```{r I2 overall overlap}
get_I2_overall(overlap_data, overlap_pt)
```

This heterogeneity is still high, but lower than with the stereotyping outcome or the overall model with perspective taking interacting with outcome type.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels overlap}
get_I2_var_levels(overlap_data, overlap_pt)
```

Most of the heterogeneity is due to between-studies variance. However, a large amount is also due to scales of measurement in the crossed structure.

### Post-hoc power

* k = 48 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power overlap}
subgroup_power(n_groups = 2, 
               effect_sizes = c(.40, .33), 
               sample_size = 132,
               k = 48,
               es_type = "d")
```

## Moderators

### Scale of measurement

```{r overlap model scales mod}
overlap_data %>% 
  group_by(outcome_scale_group) %>% 
  count()

overlap_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = overlap_data)

overlap_model_mod1
```

Scale 7 (Self-report IOS merging/scales) while in the pt vs objective comparison is the reference group. These are simple effects in which the effect sizes become significantly lower for the pt vs objective comparison when merging is measured from overlap of traits/attributes or measures of social distancing. There are no interactions. 

#### Marginal main effects

```{r emmeans overlap scale}
qdrq_overlap2 <- qdrg(object = overlap_model_mod1, data = overlap_data)

emmeans_overlap2 <- summary(emmeans(qdrq_overlap2, "pt_comparison", "outcome_scale_group"))
emmeans_overlap2
```

When using a self-reported merging scale both pt vs objective and control lead to medium to large effect sizes that differed from 0. When using both the attribution and social distance scales, neither comparison significantly differed from 0.

#### I2 overall

```{r I2 overall overlap scale}
get_I2_overall(overlap_data, overlap_model_mod1)
```

With this moderator, the heterogeneity lessens a bit.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels overlap scale}
get_I2_var_levels(overlap_data, overlap_model_mod1)
```

Almost all of the heterogeneity can be explained by between-studies variance.

### Target - amount of information

```{r}
overlap_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = overlap_data)

overlap_model_mod2
```

None are significant. I2 lowered.

#### Marginal main effects

```{r emmeans overlap targ info mod}
qdrq_overlap3 <- qdrg(object = overlap_model_mod2, data = overlap_data)

emmeans_overlap3 <- summary(emmeans(qdrq_overlap3, "pt_comparison", "target_information"))
emmeans_overlap3
```

When target information is impoverished, tneither comparison significantly differs from 0. Regardless of comparison condition, the effect does not significantly differ from 0 when target information was rated a 2. When target information is rated as highly detailed, both pt comparisons differ from 0. The objective comparison condition led to a medium effect size, while the control comparison lead a medium to large effect size. 

#### Overall I2

```{r overall I2 overlap targ info}
get_I2_overall(overlap_data, overlap_model_mod2)
```

Heterogeneity is even lower when including target information in the model.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 overlap targ info}
get_I2_var_levels(overlap_data, overlap_model_mod2)
```

Most of this heterogeneity is attributed to between-studies variance, with variance due to scales of measurement (17%) and then between-outcomes attributing to similar amounts (13%).

With there being no redundancies in the model and lower I2, I wonder if we can model an interaction with scale?

### Both scale & target info

```{r}
#Modeled target information as a main effect since it had no interactions and, because, when I tried to model it with interactions it led to redundancies

overlap_data %>% 
  group_by(pt_comparison, outcome_scale_group, target_information) %>% 
  count() 

2*3*3

#Should be 18 but there are 13, so the redundancies come from the fact that there are cells with 0 in it, which rma.mv is dropping: https://stats.stackexchange.com/questions/223918/multilevel-metaregression-in-r-redundant-predictors-dropped-metafor

overlap_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = overlap_data)

overlap_model_mod3
```

The I2 decreases having all three predictors in the model, though it is concerning that some cells are 0 and that was causing redundancies. Cannot model with emmeans because of redundancies.

#### I2 overall

```{r I2 overall scale and targ mod}
get_I2_overall(overlap_data, overlap_model_mod3)
```

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels scale and targ mod}
get_I2_var_levels(overlap_data, overlap_model_mod3)
```

All variance is explained by between-studies variance. This is probably due to k being too small per cell.

### Target - In-group target grouping (relevant to outcome category)

```{r}
overlap_data <- overlap_data %>% 
  filter(target_ingroup_nonspecific == "1" | target_ingroup_nonspecific == "2")

overlap_data$target_ingroup_nonspecific <- droplevels(overlap_data$target_ingroup_nonspecific)

overlap_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_ingroup_nonspecific,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = overlap_data)

overlap_model_mod4
```

The intercept (pt_v_objective when the target is your in-group) is significantly different from 0. No effects differ from the intercept. Tried running an out-group moderator model as well, but it had a convergence issue.


```{r emmeans overlap in-group targ mod}
qdrq_overlap4 <- qdrg(object = overlap_model_mod4, data = overlap_data)

emmeans_overlap4 <- summary(emmeans(qdrq_overlap4, "pt_comparison", "target_ingroup_nonspecific"))
emmeans_overlap4
```

The marginal main effects indicate that the average effect size is highest when pt is compared to the objective condition and the target is an in-group target, followed by when the target is an in-group and pt is compared to the control. Both significantly differed from 0. The effect sizes when the target is not an in-group member border being non-significant and are about the same regardless of if pt is being compared to the objective (.31) or control condition (.34). Both are lower than the in-group effect sizes.

#### I2 overall

```{r overlap targ group mod I2 overall}
get_I2_overall(overlap_data, overlap_model_mod4)
```

Heterogeneity is not as low as other moderators.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r overlap targ group mod I2 var levels}
get_I2_var_levels(overlap_data, overlap_model_mod4)
```

Most heterogeneity is explained by between-studies variance, followed by between-outcomes variance.

### Sample size

```{r sample size mod overlap}
overlap_model_mod5 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = overlap_data)

overlap_model_mod5
```

Intercept different from 0. No interaction with sample size.

#### I2 overall

```{r overlap n mod I2 overall}
get_I2_overall(overlap_data, overlap_model_mod5)
```

Overall I2 is higher with this moderator.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r overlap n mod I2 var level}
get_I2_var_levels(overlap_data, overlap_model_mod5)
```

Most heterogeneity is explained by between-studies variance. 

# Interpersonal Feelings

```{r feels data prep}
feels <- meta_data %>% 
  mutate(pt_comparison = fct_collapse(pt_comparison, 
                                      self_v_control = c("1", "2"),
                                      self_v_objective = c("3"),
                                      other_v_control = c( "6"),# there are no day in the life controls in the interpersonal feels category
                                      other_v_objective = c("7"))) %>% 
  filter(pt_comparison == "self_v_control" | pt_comparison == "self_v_objective" | 
           pt_comparison == "other_v_control" | pt_comparison == "other_v_objective") %>% 
  mutate(pt_comparison = as.factor(droplevels(pt_comparison))) %>% 
  mutate(pt_comparison = fct_relevel(pt_comparison, 
                                     "self_v_objective",
                                     "self_v_control",
                                     "other_v_objective",
                                     "other_v_control"))
feels %<>% 
  filter(outcome_type == "3")

```

## Number of overall effects:

```{r number of effect sizes feels}
feels %>% 
  count()
```

## Effects per comparison group:

```{r effects pt feels data}
feels %>% 
  group_by(pt_comparison) %>% 
  count()
```

19 is one under our limit of 20, so this will need to be taken as exploratory for the purposes of this preliminary analysis for my prelims. I want to look at these comparisons because they speak to McAuliffe et al's meta-analytic results.

### Number of unique samples

```{r overlap samples}
feels %>% 
  dplyr::select(sample_number_total) %>% 
  unique() %>% 
  count()
```

### Average sample size per cell

**To be used in power analysis**

```{n per cell overlap}
feels %>% 
  dplyr::select(n_pt, n_comparison) %>% 
  unique() %>% 
  summarise(mean(n_pt),
            mean(n_comparison))
```

## Model results

**Contrasts:**

```{r feels pt contrasts}
contrasts(feels$pt_comparison)
```

```{r r feels model pt}
feels$outcome_scale_group <- droplevels(feels$outcome_scale_group)

feels_model <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var, 
                                    ~ 1 | outcome_scale_group), data = feels)

feels_model
```

The intercept (self vs objective) is significantly different from 0. The simple effects indicate that self v control and other v control lead to significantly lower effect sizes than self v objective. Other v objective did not differ from the intercept.

### Marginal main effects

```{r emmeans feels breakdown}
qdrq_feels1 <- qdrg(object = feels_model, data = feels)

emmeans_feels1 <- summary(emmeans(qdrq_feels1, "pt_comparison"))
emmeans_feels1

emmeans_feels1 %>% ggplot(aes(x = pt_comparison, y = emmean)) + 
  geom_col() +
  theme_minimal() +
  labs(title = "Imagine-self vs other & objective vs control instructions in interpersonal only",
       subtitle = "higher effect sizes = better outcome",
       x = "Perpsective taking comparison",
       y = "Average effect size (d unbiased)") +
  scale_x_discrete(labels = c('Self vs Objective','Self vs Control',
                              'Other vs Objective', 'Other vs Control'))
```

The marginal main effects indicate that all effect sizes except self v control significantly differ from 0. However, both self and other vs objective have medium to large effect sizes, and other v control has a small effect size.

### Overall I2 for model

```{r overall I2 feels}
get_I2_overall(feels, feels_model)
```

High heterogeneity.

### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r levels I2 feels}
get_I2_var_levels(feels, feels_model)
```

Most heterogeneity is explained by between-studies variance. A little more than 11% is explained by between-outcomes variance.

### Post-hoc power

* k = 184 is the number of effect sizes
* Sample size is the average found earlier to the nearest number the r function would take
* The effect sizes are from the marginal effect size outcomes

```{r power feels}
subgroup_power(n_groups = 4, 
               effect_sizes = c(.51, .26, .55, .29), 
               sample_size = 124,
               k = 184,
               es_type = "d")
```

## Moderators

### Scales of measurement

```{r feels model scales mod}
feels %>% 
  group_by(outcome_scale_group) %>% 
  count()

feels_mod <- feels %>% 
  filter(outcome_scale_group != 16) %>% 
  unique() #removing this level since there is only one study in it

feels_mod$outcome_scale_group <- droplevels(feels_mod$outcome_scale_group)

#model with interaction
feels_model_mod1 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*outcome_scale_group,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var), 
                      data = feels_mod)

feels_model_mod1
```

Scale group 10 (Empathy) is the reference group. Dropped scale group 16 since there was only one study in it. The analysis dropped redundant comparisons that have 0 studies per cell. There were no significant interactions (so one instruction did not work better with one scale than another in comparison to intercept). 

#### I2 overall

```{r I2 overall scales}
get_I2_overall(feels_mod, feels_model_mod1)
```

Scale of measurement only lowered the I2 by 2%. 

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 var levels scale mods}
get_I2_var_levels(feels_mod, feels_model_mod1)
```

Most heterogeneity is explained by between-study variance, followed by between-outcomes variance.

### Target - amount of information

```{r feels targ info mod}
feels_model_mod2 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_information,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod2
```

None are significant. Emmeans won't work because redundant predictors were dropped from ram.mv model.

#### I2 overall

```{r I2 overall feels targ info}
get_I2_overall(feels_mod, feels_model_mod2)
```

High heterogeneity - moderator did not help.

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r I2 levels feels targ info}
get_I2_var_levels(feels_mod, feels_model_mod2)
```

Most variance explained by between-study variance, with almost 12% explained by scales of measurement.

### Target - In need/In distress target grouping (relevant to outcome category)

```{r feels targ need mod}
feels_model_mod3 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*target_emapthetic_need,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod3
```

The intercept is significantly different from 0. The difference between other v control and intercept is marginal. There are no interactions.

#### Marginal main effects

```{r emmeans feels empathetic need targ mod}
qdrq_feels3 <- qdrg(object = feels_model_mod3, data = feels_mod)

emmeans_feels3 <- summary(emmeans(qdrq_feels3, "pt_comparison", "target_emapthetic_need"))
emmeans_feels3
```

Self and other vs objective still are different from 0 when the target is empathetic. Self and other vs control are not different from 0 when the target is empathetic. When the target is not empathetic, the only condition that does not differ from 0 is self vs control.

#### I2 overall

```{r feels need targ I2 overall}
get_I2_overall(feels_mod, feels_model_mod3)
```

High heterogeneity

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r feels need targ I2 levels}
get_I2_var_levels(feels_mod, feels_model_mod3)
```

Most heterogeneity explained by between-studies variance, with over 13% by scales of measurement.

### Sample size

```{r sample size mod feels}
feels_model_mod4 <- rma.mv(dunb, 
                      var_dunb,
                      mods = ~ pt_comparison*n_overall,
                      random = list(~ 1 | sample_number_total/outcomes_within_sample_var/conditions_within_sample_var,
                                    ~ 1 | outcome_scale_group), 
                      data = feels_mod)

feels_model_mod4
````

Intercept different from 0. No interaction with sample size.

#### I2 overall

```{r n feels I2 overall}
get_I2_overall(feels_mod, feels_model_mod4)
```

#### I2 for levels of variance

*Left value corresponds to top sigma in Variance components output in model results (Sample_number_total). Right-most value corresponds to bottom sigma (Outcome_scale group).*

```{r n feels I2 var levels}
get_I2_var_levels(feels_mod, feels_model_mod4)
```
